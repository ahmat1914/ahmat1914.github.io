<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>自相关图</title>
      <link href="2021/03/19/zi-xiang-guan-tu/"/>
      <url>2021/03/19/zi-xiang-guan-tu/</url>
      
        <content type="html"><![CDATA[<h3 id="自相关系数"><a href="#自相关系数" class="headerlink" title="自相关系数"></a>自相关系数</h3><p>自相关系数可以测量时间序列 <code>滞后值</code> 之间的线性相关性，正如相关系数可以衡量两个变量之间的线性相关性。</p><a id="more"></a><p>如 $r_1$ 衡量 $y_t$ 和 $y_{t-1}$ 之间的关系；$r_2$ 衡量 $y_t$ 和 $y_{t-2}$ 之间的关系。</p><p>$$<br>r_k=\frac{\sum_{t=k+1}^{T}(y_t-\hat{y})(y_{t-k}-\hat{y})}{\sum_{t=1}^{T}(y_t-\hat{y})^2}<br>$$</p><p>其中，$\hat{y}$ 是均值，$T$ 是序列长度。</p><p>通过绘制自相关系数图可以描绘 $自相关函数$ 或者是 $ACF$。</p><p><img src="/images/autocorrelation-logo.png" alt=""></p><p>在该图中：</p><ul><li>$r_4$ 值最大。这是由于数据的季节性形态：顶峰往往出现在第四季度，谷底往往出现在第二季度。</li><li>$r_2$ 值最小。这是由于谷底往往在高峰之后的两个季度出现。</li><li>蓝色虚线之内的区域自相关性可近似看做 0。</li></ul><h3 id="偏子相关"><a href="#偏子相关" class="headerlink" title="偏子相关"></a>偏子相关</h3><p>现在如果 $yt$ 和 $y_{t−1}$ 已经存在相关性，则 $y_{t−1}$ 和 $y_{t−2}$ 必然存在相关性。然而 $yt$ 和 $y_{t−2}$ 也同样必然相关，这是因为它们都与 $y_{t−1}$ 相关而不是因为 $y_{t−2}$ 包含新的信息可以用来预测 $y_t$。</p><p>为了解决这个问题，我们可以使用偏自相关, partial autocorrelations 简称 PACF。</p><p>偏自相关衡量的是在移除延迟 $1,2,3,…,k−1$ 对 $y_t$ 的影响的情况下, $y_t$ 和 $y_{t−k}$ 的关系。</p><p>因此延迟一阶偏自相关系数和延迟一阶自相关系数是相同的，因为没有延迟需要移除。</p><p>每个偏自相关系数都可以被估计为一个自回归模型中的末项系数。<br>特别地，$\alpha_k$ 作为第 k 个偏自相关系数，等于在一个 $AR(k)$ 模型中 $w_k$ 的估计值。</p><p><img src="/images/2-acf-pacf.png" alt=""></p><h3 id="ACF-图中的趋势性和季节性"><a href="#ACF-图中的趋势性和季节性" class="headerlink" title="ACF 图中的趋势性和季节性"></a>ACF 图中的趋势性和季节性</h3><ul><li><p>当数据具有趋势性时，短期滞后的自相关值较大，因为观测点附近的值波动不会很大。</p></li><li><p>当数据具有季节性时，自相关值在滞后阶数与季节周期相同时（或者在季节周期的倍数）较大。</p></li></ul><p>当数据同时具有趋势和季节性时，我们会观察到组合效应。如下图</p><p><img src="/images/autocorrelation-ts-data.png" alt=""></p><p><img src="/images/autocorrelation-acf.png" alt=""></p><ul><li>因为原时间序列中具有趋势变化，自相关系数值随着滞后阶数增加而缓慢降低。</li><li>原时间序列中的季节性变化，图中出现“圆齿状”形状。</li></ul><h3 id="白噪声"><a href="#白噪声" class="headerlink" title="白噪声"></a>白噪声</h3><p>白噪声是一个对所有时间其自相关系数为零的随机过程。</p><p><img src="/images/white-noise-ts-data.png" alt=""><br><img src="/images/white-noise-acf.png" alt=""></p><p>对于白噪声而言，我们期望它的自相关值接近0。</p><ul><li>但是由于随机扰动的存在，自相关值并不会精确地等于0。</li><li>对于一个长度为 $T$ 的白噪声序列而言，我们期望在0.95 的置信度下，它的自相关值处于 $\frac{\pm2}{\sqrt{T}}$ 之间。</li><li>我们可以很容易的画出ACF的边界值（图中蓝色虚线）。</li><li>如果一个序列中有较多的自相关值处于边界之外，那么该序列很可能不是白噪声序列。</li></ul><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="https://otexts.com/fpp2/" target="_blank" rel="noopener">Forecasting: Principles &amp; Practice</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 时间序列 </tag>
            
            <tag> acf </tag>
            
            <tag> pacf </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ARIMA 系列模型</title>
      <link href="2021/03/18/arima-xi-lie-mo-xing/"/>
      <url>2021/03/18/arima-xi-lie-mo-xing/</url>
      
        <content type="html"><![CDATA[<h3 id="AR-模型"><a href="#AR-模型" class="headerlink" title="AR 模型"></a>AR 模型</h3><p>自回归模型基于目标变量历史数据的组合对目标变量进行预测。自回归一词中的自字即表明其是对变量自身进行的回归。</p><p>一个 $p$ 阶的自回归模型 $AR(p)$可以表示如下：<br>$$<br>y_t=c+w_1y_{t-1}+w_2y_{t-2}+ … +w_py_{t-p}+ \epsilon_t<br>$$<br>$$<br>=c+ \sum_{i=1}^pw_iy_{t-i} + \epsilon_t<br>$$</p><p>其中，$c$是常数，$\epsilon_t$ 是白噪声序列。</p><p>$AR(p)$ 相当于$y$ 自己历史值的多元回归。</p><p>如 $AR(1)$ 模型：<br>$$y_t=c+w_1y_{t-1} + \epsilon_t$$</p><ul><li>当 $w_1=0$ 时，$y_t$相当于白噪声；</li><li>当 $w_1=1, c=0$ 时，$y_t$相当于随机游走模型（random walk）；</li><li>当 $w_1&lt;0$ 时，$y_t$相当于倾向于在正负值之间上下浮动。</li></ul><p>我们通常将自回归模型的应用限制在平稳数据上。</p><h3 id="AR-模型优点与限制"><a href="#AR-模型优点与限制" class="headerlink" title="AR 模型优点与限制"></a>AR 模型优点与限制</h3><p>自回归方法的优点是所需数据不多，可用自身历史数据来进行预测。</p><p>但也有如下限制：</p><ul><li>要求数据是平稳的。</li><li>历史数据必须具有自相关性，自相关系数$w_i$是关键。如果自相关系数(R)小于0.5，则不宜采用，否则预测结果极不准确。</li><li>无法采集其他变量信息。对于受社会因素影响较大的经济现象，不宜采用自回归，而应改采用可纳入其他变量的向量自回归模型（英语：Vector Autoregression model，简称VAR模型）。</li></ul><h3 id="MA-模型"><a href="#MA-模型" class="headerlink" title="MA 模型"></a>MA 模型</h3><p>移动平均模型（moving average model）使用历史预测误差来建立一个类似回归的模型。</p><p>$MA(q)$表示为：</p><p>$$<br>y_t=c+w_1\epsilon_{t-1}+w_2\epsilon_{t-2}+ … +w_q\epsilon_{t-q}+ \epsilon_t<br>$$<br>$$<br>=c+ \sum_{i=1}^qw_iy_{t-i} + \epsilon_t<br>$$</p><p>$y_t$ 的每一个值都可以被认为是一个历史预测误差的加权移动平均值。</p><p>MA模型和移动平均平滑法 的区别：</p><ul><li>移动平均模型是用于预测未来值的方法，</li><li>而移动平均平滑法（Moving average smoothing）则是用来估计历史值的循环趋势</li></ul><h3 id="移动平均模型的优缺点"><a href="#移动平均模型的优缺点" class="headerlink" title="移动平均模型的优缺点"></a>移动平均模型的优缺点</h3><p>使用移动平均法进行预测能平滑掉需求的突然波动对预测结果的影响。</p><p>但移动平均法运用时也存在着如下问题：</p><ul><li>要求数据是平稳的。</li><li>加大移动平均法的期数（即加大q值）会使平滑波动效果更好，但会使预测值对数据实际变动更不敏感；</li><li>移动平均值并不能总是很好地反映出趋势。由于是平均值，预测值总是停留在过去的水平上而无法预计会导致将来更高或更低的波动；</li></ul><h3 id="非季节性ARIMA模型"><a href="#非季节性ARIMA模型" class="headerlink" title="非季节性ARIMA模型"></a>非季节性ARIMA模型</h3><p>将差分和自回归模型以及移动平均模型结合起来的时候，我们可以得到一个非季节性 ARIMA 模型。</p><p>ARIMA 全称自回归移动平均模型 (Auto Regressive Integrated Moving Average)，是统计模型(statistic model)中最常见的一种用于时间序列预测的模型。（在这里Integrated指的是差分的逆过程) 。</p><p>ARIMA 模型要求数据是稳定序列。</p><blockquote><p>指数平滑模型（exponential smoothing）和ARIMA模型是应用最为广泛的两种。基于对这两种预测方法的拓展,很多其他的预测方法得以诞生。<br>与指数平滑模型针对于数据中的趋势（trend）和季节性（seasonality）不同，ARIMA模型旨在描绘数据的自回归性（autocorrelations）。</p></blockquote><p>ARIMA(q,d,q) 模型的特例:<br><img src="/images/special-arima.png" alt=""></p><h3 id="ARIMA-建模流程："><a href="#ARIMA-建模流程：" class="headerlink" title="ARIMA 建模流程："></a>ARIMA 建模流程：</h3><ol><li>画出数据时序图并检查有无异常观测。</li><li>如果必要的话，对数据进行变换（如 Box-Cox 变换）来稳定方差。</li><li>如果数据非平稳，对数据进s行差分直到数据平稳。</li><li>选择合适的 $p和q$ 的取值。</li><li>估计模型参数。<ol><li>一旦我们确定了模型的阶数(p,d,q的取值)，我们就需要估计参数 $c、w_1,…,w_p,、\beta_1,…,\beta_q$ 了。</li><li>可以使用极大似然估计 (maximum likelihood estimation) ，最大化观测到的数据出现的概率来确定参数；或使用最小二乘估计，通过最小化方差确定参数。</li></ol></li><li>假设检验，判断残差序列是否为白噪声序列。<ol><li>通过画出残差自相关图来检查你选择模型的残差，并且对残差进行一元混成检验（portmanteau test）或Box-Pierce方法检验。</li><li>如果它们看起来不像白噪声，那么模型就需要修正。（我们知道一个好的模型fitting出来的结果应该要是 stationary。）</li></ol></li><li>如果残差是白噪声，利用已通过检验的模型进行预测。</li></ol><p><img src="/images/process.png" alt=""></p><p>其中第4步，介绍两种常用的方法：</p><ol><li>观察 acf, pacf 图，人工确定可能的 p,q取值</li><li>遍历p，q的取值，选择最小化 AIC/BIC(信息注册函数) 的p,q 取值。如 Hyndman-Khandakar 算法。</li></ol><p>实际应用中，这两种方法可以混用，如</p><ul><li>先通过 Hyndman-Khandakar算法自动调参，然后通过 ACF/PACF图检验、修正 p、q取值。</li><li>或先观察 ACF，PACF图，选出几个可能的 p，q取值，然后计算 AIC，选择误差最小的。<br><img src="/images/aic-acf-difference.png" alt=""></li></ul><h3 id="自相关图和偏自相关图"><a href="#自相关图和偏自相关图" class="headerlink" title="自相关图和偏自相关图"></a>自相关图和偏自相关图</h3><p>仅从时序图中我们通常无法判断 $p和q$ 的合适取值。然而我们有时从自相关图和与其紧密相关的偏自相关图中，可以判断 $p和q$ 的合适取值。</p><p>自相关图（ACF Plot）反映了自回归中 $y_t$ 和 $y_{t−k}$ 在不同 k 取值之下的关系。</p><p>现在如果 $yt$ 和 $y_{t−1}$ 已经存在相关性，则 $y_{t−1}$ 和 $y_{t−2}$ 必然存在相关性。然而 $yt$ 和 $y_{t−2}$ 也同样必然相关，这是因为它们都与 $y_{t−1}$ 相关而不是因为 $y_{t−2}$ 包含新的信息可以用来预测 $y_t$。</p><p>为了解决这个问题，我们可以使用偏自相关, partial autocorrelations 简称 PACF。</p><p>偏自相关衡量的是在移除延迟 $1,2,3,…,k−1$ 对 $y_t$ 的影响的情况下, $y_t$ 和 $y_{t−k}$ 的关系。</p><p>因此延迟一阶偏自相关系数和延迟一阶自相关系数是相同的，因为没有延迟需要移除。</p><p>每个偏自相关系数都可以被估计为一个自回归模型中的末项系数。<br>特别地，$\alpha_k$ 作为第 k 个偏自相关系数，等于在一个 $AR(k)$ 模型中 $w_k$ 的估计值。</p><p><img src="/images/2-acf-pacf.png" alt=""></p><ul><li>如果数据来自于$ARIMA(p,d,0)$或者 $ARIMA(0,d,q)$ 模型，则自相关图和偏自相关图在判定$p$或者$q$的取值时非常有用;</li><li>如果 $p 和 q$ 都为正，则这些图在寻找最合适的 $p和q$ 值时不再有用;</li></ul><p>如果差分数据的自相关图和偏自相关图显示出如下特征，则他们可能来自于$ARIMA(p,d,0)$模型：</p><ul><li>自相关系数呈现指数下降或者类似正弦型的波动；（拖尾，Tail Off）</li><li>偏自相关图中的延迟 $p$ 中有明显突起，但延迟更大时不存在类似的突起。（截尾，Cut Off）</li></ul><p>如果差分数据的自相关图和偏自相关图数据显示出如下特征,则他们可能来自于$ARIMA(0,d,q)$ 模型：</p><ul><li>偏自相关系数呈现指数下降或者类似正弦型的波动；（拖尾，Tail Off）</li><li>在自相关图中的延迟q 中有明显突起,但延迟更大时不存在类似的突起。（截尾，Cut Off）</li></ul><h3 id="ACF-PACF-图确定-p-q取值"><a href="#ACF-PACF-图确定-p-q取值" class="headerlink" title="ACF,PACF 图确定 p, q取值"></a>ACF,PACF 图确定 p, q取值</h3><ol><li>观察哪个图截尾，acf 图或pacf 图。<ol><li>若 PACF 在P阶截尾的，而 ACF 是拖尾的，则建立 $AR(P)$ 模型，即 $ARIMA(P,d,0)$；</li><li>若 ACF 在Q阶截尾的， PACF 是拖尾的，则建立 $MA(Q)$ 模型，即 $ARIMA(0,d,Q)$；</li></ol></li><li>若 ACF 和 PACF 都拖尾，则序列适合ARIMA 模型，即 $ARIMA(P,d,Q)$，其中 P,Q 的值需要进一步确定，如 $(1,d,1), (2,d,1), (2,d,2)$</li></ol><p>例1：时序图如下：</p><p><img src="/images/1-ts-data.png" alt=""></p><p>时序图看起来不平稳，做一阶差分后如下图</p><p><img src="/images/1-diff1.png" alt=""></p><p>一阶差分后的数据，自相关图以及偏自相关图如下</p><p><img src="/images/1-acf-pacf.png" alt=""></p><p>pacf 在3阶截尾，acf拖尾，因此适合AR 模型，即 $ARIMA(3, 1, 0)$。</p><p>我们使用 $ARIMA(3,1,0)$以及衍生模型 $ARIMA(4,1,0),ARIMA(2,1,0),ARIMA(3,1,1)$ 等等进行拟合，在这些模型中，$ARIMA(3,1,1)$ 是 $AIC_c$ 值最小的模型。</p><p>$ARIMA(3,1,1)$ 模型的残差自相关图（如下）显示出所有的自相关系数都在置信域之内，这反映出残差类似于白噪声；</p><p>一元混成检验(另外一篇文章简介)得到了一个较大的p值，同样意味着残差类似白噪声。</p><p><img src="/images/1-arima3-1-1.png" alt=""></p><h3 id="最小化AIC，确定p-q-取值"><a href="#最小化AIC，确定p-q-取值" class="headerlink" title="最小化AIC，确定p,q 取值"></a>最小化AIC，确定p,q 取值</h3><p>Hyndman-Khandakar算法流程：<br><img src="/images/Hyndman-Khandakar.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="https://otexts.com/fpp2/" target="_blank" rel="noopener">Forecasting: Principles &amp; Practice</a></li><li><a href="http://people.duke.edu/~rnau/411arim3.htm" target="_blank" rel="noopener">ARIMA models for time series forecasting</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> arima </tag>
            
            <tag> ma </tag>
            
            <tag> ar </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>延迟算子</title>
      <link href="2021/03/18/yan-chi-suan-zi/"/>
      <url>2021/03/18/yan-chi-suan-zi/</url>
      
        <content type="html"><![CDATA[<h3 id="延迟算子"><a href="#延迟算子" class="headerlink" title="延迟算子"></a>延迟算子</h3><p>时间序列的延迟使用延迟算子（backshift operator）描述， 用$B$ 表示。 (有的文献使用滞后算子，Lag operator，用$L$表示）</p><p>$$<br>By_t=y_{t-1}<br>$$</p><p>$$<br>B(By_t)=B^2y_t=y_{t-2}<br>$$</p><p>$$<br>B^{12}y_t=y_{t-12}<br>$$</p><h3 id="一阶差分"><a href="#一阶差分" class="headerlink" title="一阶差分"></a>一阶差分</h3><p>$$y_t’=y_t-y_{t-1}=y_t-By_t=(1-B)y_t$$</p><h3 id="二阶差分"><a href="#二阶差分" class="headerlink" title="二阶差分"></a>二阶差分</h3><p>$$<br>y_t’’=y_t’ - y_{t-1}’=(y_t - y_{t-1}) - (y_{t-1} - y_{t-2})=y_t-2y_{t-1} + y_{t-2}=y_t-2By_t-B^2y_t=(1-2B+B^2)y_t=(1-B)^2y_t<br>$$</p><p>$$<br>y_t’’=(1-B)y_t’=(1-B)(1-B)y_t=(1-B)^2y_t<br>$$</p><h3 id="d-阶差分"><a href="#d-阶差分" class="headerlink" title="d 阶差分"></a>d 阶差分</h3><p>$$<br>(1-B)^dy_t<br>$$</p><h3 id="延迟算子特性"><a href="#延迟算子特性" class="headerlink" title="延迟算子特性"></a>延迟算子特性</h3><p>延迟算子符合代数变换的规则。如一阶差分乘以同期（季节性）差分可表示为：<br>$$<br>(1-B)(1-B^m)y_t=(1-B^m-B+B^{m+1})y_t=y_t-B^my_t-By_t+B^{m+1}y_t=y_t-y_{t-m}-y_{t-1}+y_{t-m-1} \tag{1}<br>$$</p><p>延迟算子的其他特性</p><p>$$B^0=1 \tag{2}$$</p><p>$$B(aY_t + bX_t + c) = aBY_t + bBX_t + c \tag{3}$$</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="https://otexts.com/fpp2/" target="_blank" rel="noopener">Forecasting: Principles &amp; Practice</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 时间序列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>时间序列的平稳性</title>
      <link href="2021/03/17/shi-jian-xu-lie-de-ping-wen-xing/"/>
      <url>2021/03/17/shi-jian-xu-lie-de-ping-wen-xing/</url>
      
        <content type="html"><![CDATA[<h3 id="平稳性"><a href="#平稳性" class="headerlink" title="平稳性"></a>平稳性</h3><p>如果时间序列的性质不随时间变化而变化（如均值0，方差趋于稳定值），我们称该序列是平稳的。典型的平稳序列是白噪声（white noise series）。</p><p>因此具有趋势、周期性、季节性的序列不是平稳序列。</p><p>ARIMA模型要求序列是平稳的，或可以通过处理（如差分）平稳化。</p><h3 id="判断平稳性"><a href="#判断平稳性" class="headerlink" title="判断平稳性"></a>判断平稳性</h3><p>判断平稳性主要有两种，观察法和单位根检验法。</p><ul><li>观察可视化的数据，从中推断数据是否稳定。如观察时间曲线图或自相关图</li><li>单位根检验的方法有很多种，如 ADF检验。</li></ul><h5 id="观察法"><a href="#观察法" class="headerlink" title="观察法"></a>观察法</h5><p>观察曲线图，判断哪些序列平稳</p><p><img src="/images/which-stationary-nonstationary.png" alt=""></p><ul><li>存在季节性的序列：d, h, i</li><li>存在趋势的序列：a, c, e, f, i</li><li>方差明显变化的序列: i</li></ul><p>剩下的 b, g 是平稳序列。</p><p>和时间曲线图一样，自相关图（ACF图）也能帮助我们识别非平稳时间序列。 对于一个平稳时间序列,自相关系数（ACF）会快速的下降到接近 0 的水平，然而非平稳时间序列的自相关系数会下降的比较缓慢。</p><p><img src="/images/acf-google.png" alt=""></p><p>差分后的谷歌股价的自相关图看起来像白噪声序列。所有自回归系数都在 95% 的置信度以内。</p><ul><li><input disabled="" type="checkbox"> 待更新</li></ul><h5 id="单位根检验"><a href="#单位根检验" class="headerlink" title="单位根检验"></a>单位根检验</h5><p>单位根检验是一种更客观的判定是否需要差分的方法。这个针对平稳性的统计假设检验被用于判断是否需要差分方法来让数据更平稳。<br>单位根检验的方法有很多种，ADF是一种常用的单位根检验方法，他的原假设为序列具有单位根，即非平稳，对于一个平稳的时序数据，就需要在给定的置信水平上显著，拒绝原假设。以下为检验结果，其p值大于0.99，说明并不能拒绝原假设。</p><p><img src="/images/p-value.png" alt=""></p><ul><li><input disabled="" type="checkbox"> 待更新<h3 id="平稳化处理方法"><a href="#平稳化处理方法" class="headerlink" title="平稳化处理方法"></a>平稳化处理方法</h3>平稳性是时间序列分析的前提条件，故我们需要对不平稳的序列进行处理将其转换成平稳的序列。常用的方法有：</li></ul><ul><li>对数变换</li><li>平滑法</li><li>差分</li><li>分解</li></ul><h3 id="差分"><a href="#差分" class="headerlink" title="差分"></a>差分</h3><p>差分可以通过去除时间序列中的一些变化特征来平稳化它的均值，并因此消除（或减小）时间序列的趋势和季节性。</p><p>差分序列是指原序列的连续观测值之间的变化值组成的时间序列，它可以被表示为：<br>$$<br>y_t’ = y_t - y_{t-1}<br>$$</p><h5 id="一阶差分、白噪声、random-walk"><a href="#一阶差分、白噪声、random-walk" class="headerlink" title="一阶差分、白噪声、random walk"></a>一阶差分、白噪声、random walk</h5><p>$$<br>y_t’ = y_t - y_{t-1}<br>$$</p><p>当差分序列$y’_t$是白噪声时，即可得到“随机游走”模型：</p><p>$$<br>y_t = y_{t-1} + \epsilon_t<br>$$<br>其中 $\epsilon_t$ 是白噪声。</p><h5 id="二阶差分"><a href="#二阶差分" class="headerlink" title="二阶差分"></a>二阶差分</h5><p>有时差分后的数据仍然不平稳，所以可能需要再一次对数据进行差分来得到一个平稳的序列：</p><p>$$<br>y_t’’=y_t’ - y_{t-1}’=(y_t - y_{t-1}) - (y_{t-1} - y_{t-2})=y_t-2y_{t-1} + y_{t-2}<br>$$</p><h5 id="季节性差分"><a href="#季节性差分" class="headerlink" title="季节性差分"></a>季节性差分</h5><p><img src="/images/seasonal-differ.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="https://otexts.com/fpp2/" target="_blank" rel="noopener">Forecasting: Principles &amp; Practice</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> arima </tag>
            
            <tag> 时间序列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Exploratory Data Analysis for Time Series</title>
      <link href="2021/03/08/exploratory-data-analysis-for-time-series/"/>
      <url>2021/03/08/exploratory-data-analysis-for-time-series/</url>
      
        <content type="html"><![CDATA[<p>分两个部分，第一部分，讨论时间序列数据上常用的方法，如柱状图（histograms）、曲线图（plotting），和一些能用于时间序列数据的聚合操作（group-by operations）。</p><p>第二部分，重点讨论时间序列分析相关的方法。</p><h2 id="Familiar-Methods"><a href="#Familiar-Methods" class="headerlink" title="Familiar Methods"></a>Familiar Methods</h2><p>先从常用于时间序列数据的探索性方法。如有哪些列、取值范围。</p><ul><li>某一列是否和另一列强相关？</li><li>某一特征均值、variance 多少？</li></ul><p>回答这些问题，可以做一些常用的技术分析，画图、计算统计值、画柱状图、using targeted scatter plots。</p><p>同时你也需要回答面向时间的问题：</p><ul><li>What is the range of values you see, and do they vary by time period or some other logical unit of analysis?</li><li>Does the data look consistent and uniformly measured, or does it suggest changes in either measurement or behavior over time?。</li></ul><p>我们使用一些时间序列数据，做一些数据分析，并探索时间序列特有的方法和一些非时间序列方法。</p><h3 id="Plotting"><a href="#Plotting" class="headerlink" title="Plotting"></a>Plotting</h3><p><img src="/images/simple-plot-of-timeseries-data.png" alt=""></p><h3 id="Histograms"><a href="#Histograms" class="headerlink" title="Histograms"></a>Histograms</h3><p>take a histogram of the data, also taking a histogram of the differenced data.</p><p><img src="/images/histogram-of-timeseries-and-difference.png" alt=""></p><p>A hist() of the difference of the data is often more interesting than a hist() of the untransformed data.<br>After all, in a time series context, often (and particularly in finance) what is most interesting is how a value changes from one measurement to the next rather than the value’s actual measurement. </p><p>The histogram of the difference tells us that the value of the time series has gone both up (positive difference values) and down (negative difference values) about the same amount over time. </p><p>we can see from this histogram that just a slight bias in favor of positive over negative differences is what accounts for that trend.</p><h3 id="Scatter-Plots"><a href="#Scatter-Plots" class="headerlink" title="Scatter Plots"></a>Scatter Plots</h3><p>We can use scatter plots to determine both how two stocks are linked at a specific time and how their price shifts are related over time.</p><p>In this example, we plot both cases (see Figure 3-3):</p><ul><li>The values of two different stocks over time</li><li>The values of the daily changes in these two stocks over time (via differencing) with R’s diff() function</li></ul><p><img src="/images/sotcks-price-and-difference-over-time.png" alt=""></p><p>As we’ve already seen, the actual values are less informative than the differences between adjacent time points, so we have plotted the diffs in a second scatter plot. These look like very strong correlations, but the relationships are not as strong as they appear.</p><p>What we need to do is find out whether the change in one stock earlier in time can predict the change in another stock later in time. To do this, we shift one of the differences of the stocks back by 1 before looking at the scatter plot.</p><p><img src="/images/correlation-between-the-stocks-disappears-as-soon-as-we-put-in-a-time-lag.png" alt=""></p><h2 id="Time-Series–Specific-Exploratory-Methods"><a href="#Time-Series–Specific-Exploratory-Methods" class="headerlink" title="Time Series–Specific Exploratory Methods"></a>Time Series–Specific Exploratory Methods</h2><p>focus on relations of values at different times in the same series.</p><p>We walk through a few concepts and related techniques used to classify time series.</p><p>We will cover the concepts and their resulting methods in order from stationarity to self-correlations to spurious correlations. </p><p>The concepts we will explore are:</p><ul><li><p>Stationarity: What it means for a time series to be stationary and a statistical test for stationarity</p></li><li><p>Self-correlation: What it means to say that a time series correlates with itself and what such a correlation indicates about the underlying dynamics of the time series</p></li><li><p>Spurious correlations: What it means for a correlation to be spurious and when you should expect to run into spurious correlations</p></li></ul><p>The methods we will learn to apply are:</p><ul><li><p>Rolling and expanding window functions</p></li><li><p>Self-correlation functions</p><ul><li>The autocorrelation function</li><li>The partial autocorrelation function</li></ul></li></ul><p>The first question you will likely ask about a time series is whether it appears to reflect a system that is <strong>“stable”</strong> or one that is constantly changing.<br>The level of stability, or <strong>stationarity</strong>, is important to assess because we need to know how much we should expect the system’s long-term past behavior to reflect its long-term future behavior.</p><p>Once we have assessed the “stability” (this word is not used in a technical sense here) of a time series, we try to determine whether there are <strong>internal dynamics</strong> in that series (seasonal changes, for example).<br>That is, we are looking for <strong>self-correlations</strong>, answering the fundamental question of how tightly past data, distant or recent, predicts future data.</p><p>Finally, when we think we have found certain behavioral dynamics within the system, we need to <strong>make sure we are not identifying relationships based on dynamics</strong> that do not in any way imply the causal relationships we wish to discover; hence, we must look for <strong>spurious correlations</strong>.</p><h3 id="Understanding-Stationarity"><a href="#Understanding-Stationarity" class="headerlink" title="Understanding Stationarity"></a>Understanding Stationarity</h3><p>Many traditional statistical time series models rely on a time series being stationary.</p><p>Generally speaking, a stationary time series is one that has fairly stable statistical properties over time, particularly with respect to mean and variance.</p><p>We will walk through the concept both intuitively and with a somewhat formal definition before discussing a common test for stationarity.</p><h4 id="INTUITION"><a href="#INTUITION" class="headerlink" title="INTUITION"></a>INTUITION</h4><p>A stationary time series is one in which a time series measurement reflects a system in a steady state. (it can be easier to rule things out as not being stationary rather than saying something is stationary. )</p><p><img src="/images/clear-example-of-a-timeseries-not-stationary.png" alt=""></p><p>There are several traits that show this process(airline passengers data) is not stationary.</p><ul><li>First, the mean value is increasing over time, rather than remaining steady.</li><li>Second, the distance between peak and trough on a yearly basis is growing, so the variance of the process is increasing over time.</li><li>Third, the process displays strong seasonal behavior, the antithesis of stationarity.</li></ul><h4 id="STATIONARY-DEFINITION-AND-THE-AUGMENTED-DICKEY–FULLER-TEST"><a href="#STATIONARY-DEFINITION-AND-THE-AUGMENTED-DICKEY–FULLER-TEST" class="headerlink" title="STATIONARY DEFINITION AND THE AUGMENTED DICKEY–FULLER TEST"></a>STATIONARY DEFINITION AND THE AUGMENTED DICKEY–FULLER TEST</h4><p><img src="/images/definition-of-a-stationary-process.png" alt=""></p><p>Statistical tests for stationarity often come down to the question of whether there is a unit root—that is, whether 1 is a solution of the process’s characteristic equation.1 A linear time series is nonstationary if there is a unit root, although lack of a unit root does not prove stationarity.</p><p>!(a simple intuition for what a unit root is)[/images/a-simple-intuition-for-what-a-unit-root-is.png]</p><p>Tests for determining whether a process is stationary are called $hypothesis tests$.</p><p>The $Augmented Dickey–Fuller (ADF)$ test is the most commonly used metric to assess a time series for stationarity problems. </p><blockquote><p>This test posits a null hypothesis that a unit root is present in a time series. Depending on the results of the test, this null hypothesis can be rejected for a specified significance level, meaning the presence of a unit root test can be rejected at a given significance level.</p></blockquote><p>待续。。。</p><h3 id="Applying-Window-Functions"><a href="#Applying-Window-Functions" class="headerlink" title="Applying Window Functions"></a>Applying Window Functions</h3><h3 id="Understanding-and-Identifying-Self-Correlation"><a href="#Understanding-and-Identifying-Self-Correlation" class="headerlink" title="Understanding and Identifying Self-Correlation"></a>Understanding and Identifying Self-Correlation</h3><h3 id="Spurious-Correlations"><a href="#Spurious-Correlations" class="headerlink" title="Spurious Correlations"></a>Spurious Correlations</h3><h2 id="Some-Useful-Visualizations"><a href="#Some-Useful-Visualizations" class="headerlink" title="Some Useful Visualizations"></a>Some Useful Visualizations</h2><h3 id="1D-Visualizations"><a href="#1D-Visualizations" class="headerlink" title="1D Visualizations"></a>1D Visualizations</h3><h3 id="2D-Visualizations"><a href="#2D-Visualizations" class="headerlink" title="2D Visualizations"></a>2D Visualizations</h3><h3 id="3D-Visualizations"><a href="#3D-Visualizations" class="headerlink" title="3D Visualizations"></a>3D Visualizations</h3><h3 id="More-Resources"><a href="#More-Resources" class="headerlink" title="More Resources"></a>More Resources</h3><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul><li>Practical Time Series Analysis By <a href="https://learning.oreilly.com/search/?query=author%3A%22Aileen%20Nielsen%22&sort=relevance&highlight=true" target="_blank" rel="noopener">Aileen Nielsen</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 时间序列 </tag>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>离线安装snap 包</title>
      <link href="2021/03/05/chi-xian-an-zhuang-snap-bao/"/>
      <url>2021/03/05/chi-xian-an-zhuang-snap-bao/</url>
      
        <content type="html"><![CDATA[<blockquote><p>国内有没有 Snap 源？<br>国内就没有一个能用的 snap 镜像源！<br>如题，snap 部署很方便，但是下载速度也太慢了吧?<br>问下 ubuntu 如何安装本地的 snap 包？</p></blockquote><a id="more"></a><p>demo with nodejs</p><pre class=" language-bash"><code class="language-bash">curl -H <span class="token string">'Snap-Device-Series: 16'</span> http://api.snapcraft.io/v2/snaps/info/node <span class="token operator">>></span> node.json</code></pre><p><code>node.json</code> 如下，ubuntu 对应找到 amd64 stable 对应的 download url<br><img src="/images/snap-node-info.png" alt=""></p><p>在有网络的机器上下载 <code>*.snap</code> 包</p><pre class=" language-bash"><code class="language-bash"><span class="token function">wget</span> https://api.snapcraft.io/api/v1/snaps/download/MEd4V4HHFkCXBSz6UzVmKF2D2PmWcVwR_3292.snap<span class="token comment" spellcheck="true"># 下载了 MEd4V4HHFkCXBSz6UzVmKF2D2PmWcVwR_3292.snap</span></code></pre><p>到目标机器上安装 <code>*.snap</code> 包</p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> snap <span class="token function">install</span> --dangerous MEd4V4HHFkCXBSz6UzVmKF2D2PmWcVwR_3292.snap --classic</code></pre><p><img src="/images/snap-node-list.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Tech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> snap </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DeepAR: 使用自回归递归网络实现概率预测</title>
      <link href="2021/03/03/deepar-shi-yong-zi-hui-gui-di-gui-wang-luo-shi-xian-gai-lu-yu-ce/"/>
      <url>2021/03/03/deepar-shi-yong-zi-hui-gui-di-gui-wang-luo-shi-xian-gai-lu-yu-ce/</url>
      
        <content type="html"><![CDATA[<h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><p>以商品销量举例，</p><ul><li>第 $i$ 个商品在 $t$ 时刻的销量 $Z$ 表示为 $Z_{i, t}$；<a id="more"></a></li><li>我们预测条件分布 $P(Z_{i,t}|Z_{i, t-1}, Z_{i, t-2}, …, Z_{i, 1})$</li><li>进一步预测更多(截止 T)时间上的条件分布 $P(Z_{i,t}, Z_{i,t+1}, Z_{i,t+2}, …, Z_{i,T}|Z_{i, t-1}, Z_{i, t-2}, …, Z_{i, 1})$</li></ul><p>我们已知，商品历史销量 $$Z_{i,1:t-1}:=Z_{i, 1}, Z_{i, 2}, …, Z_{i, t-1}$$ 和特征 $$X_{i, 1:T}:=X_{i,1}, X_{i,2}, …, X_{i, t-1}, X_{i, t}, X_{i, t+1},…, X_{i, T}$$，预测未来销量。</p><p>模型的分布为条件概率似然函数的乘积：</p><p><img src="/images/distribution-product-of-likehood-factors.png" alt=""></p><p>其中， 模型参数θ; </p><p>也就是说，已知历史 $conditional\ range:=[1, t-1]$，预测未来 $prediction\ range:=[t, T]$</p><p>$\mathbf{h}_{i,t}$ 是自回归网络：<br><img src="/images/autoregressive-recurrent-net.png" alt=""></p><p>其中 $\Theta$ 是参数。</p><p>对 $h_{i,0}$ 和 $Z{_{i,0}}$ 的初始化值都设置了0。</p><p>… … 待续</p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepAR </tag>
            
            <tag> 时间序列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>时间序列的数学表示</title>
      <link href="2021/03/03/shi-jian-xu-lie-de-shu-xue-biao-shi/"/>
      <url>2021/03/03/shi-jian-xu-lie-de-shu-xue-biao-shi/</url>
      
        <content type="html"><![CDATA[<p>任何按照时间顺序观察的事物都是时间序列。<a id="more"></a></p><p>以商品销量举例，</p><ul><li>第 $i$ 个商品在 $t$ 时刻的销量 $Z$ 表示为 $Z_{i, t}$；<!--more--></li><li>我们预测条件分布 $P(Z_{i,t}|Z_{i, t-1}, Z_{i, t-2}, …, Z_{i, 1})$</li><li>进一步预测更多(截止 T)时间上的条件分布 $P(Z_{i,t}, Z_{i,t+1}, Z_{i,t+2}, …, Z_{i,T}|Z_{i, t-1}, Z_{i, t-2}, …, Z_{i, 1})$</li></ul><p>我们已知，</p><ul><li>商品历史销量 $Z_{i,1:t-1}:=Z_{i, 1}, Z_{i, 2}, …, Z_{i, t-1}$</li><li>特征 $X_{i, 1:T}:=X_{i,1}, X_{i,2}, …, X_{i, t-1}, X_{i, t}, X_{i, t+1},…, X_{i, T}$</li></ul><p>也就是说，已知历史 $conditional\ range:=[1, t-1]$，预测未来 $prediction\ range:=[t, T]$</p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 时间序列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>逻辑回归</title>
      <link href="2021/02/28/luo-ji-hui-gui/"/>
      <url>2021/02/28/luo-ji-hui-gui/</url>
      
        <content type="html"><![CDATA[<p>逻辑回归（Logistic Regression）是一种常用于二分类问题的模型，又称对数几率回归（logit）。</p><a id="more"></a><h3 id="为什么使用逻辑回归？"><a href="#为什么使用逻辑回归？" class="headerlink" title="为什么使用逻辑回归？"></a>为什么使用逻辑回归？</h3><p>我们使用线性回归，解决连续变量的预测问题。<br>$$<br>f(x) = WX<br>$$</p><p>其中 $W \in \R^n$是参数，$X \in R^n$ 是输入，$Y \in R$。<br>我们如何使用线性回归解决二元分类问题？即 $Y \in {0, 1}$<br>有很多种做法，我们使用逻辑函数缩放$Y$，让结果缩放在 (0, 1)。逻辑函数和改造后的模型分别如下：<br>$$<br>f(z) = \frac{1}{1 + e^{-z}}<br>$$</p><p>$$<br>f(x) = \frac{1}{1 + e^{WX}}<br>$$</p><p>$$<br>f(x)={\begin{cases}<br>  \displaystyle \frac{1}{1 + e^{WX}} &amp;  Y_i=1, i=1,2,…N \\<br>  1- \frac{1}{1 + e^{WX}} = \frac{e^{WX}}{1 + e^{WX}} &amp; Y_i=0, i=1,2,…N<br>  \end{cases}}<br>  \\<br>  = (\frac{1}{1+e^{WX}})^{y_i} * (1- \frac{1}{1+e^{WX}})^{1- y_i}, i=1,2,…N<br>$$</p><p><img src="/images/Logistic-curve.png" alt=""><br>$f(x) \in (0, 1)$，输出的概率值越接近1，表示分类为1的可能性越高；输出的概率值越接近0，表示分类为0的可能性越高；</p><h3 id="逻辑回归模型参数的估计"><a href="#逻辑回归模型参数的估计" class="headerlink" title="逻辑回归模型参数的估计"></a>逻辑回归模型参数的估计</h3><p>可以使用最大似然估计法估计模型的参数。<br><img src="/images/mle-estimate-logistic-Regression-paramaters.png" alt=""></p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul><li>《统计学习方法》- 李航</li></ul>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>指示函数</title>
      <link href="2021/02/25/zhi-shi-han-shu/"/>
      <url>2021/02/25/zhi-shi-han-shu/</url>
      
        <content type="html"><![CDATA[<p>在集合论中，指示函数(Indicator function)是定义在某集合X上的函数，表示其中有哪些元素属于某一子集A。</p><a id="more"></a><p>集X的子集A的指示函数是函数${1_{A}:X\to \lbrace 0,1\rbrace }$ ，定义为:</p><p>$$<br>1_{A}(x)={\begin{cases}<br>  \displaystyle 1 &amp; 若 {x \in A} \\<br>  0 &amp; 若 {x \notin A}<br>  \end{cases}}<br>$$</p><p>A的指示函数也记作 $\chi_{A}(x)$ 或 $I_{A}(x)$。</p><blockquote><p>mathjax语法提示：<em>公式中用<code>\\</code>表示回车到下一行。为了和 <code>marked.js</code> 兼容，公式内改为4个反斜线<code>\\\\</code>换行。</em></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>朴素贝叶斯</title>
      <link href="2021/02/24/po-su-bei-xie-si/"/>
      <url>2021/02/24/po-su-bei-xie-si/</url>
      
        <content type="html"><![CDATA[<p>朴素贝叶斯方法是基于贝叶斯定于和特征条件独立假设的分类方法。</p><a id="more"></a><p>对于给定的训练数据集</p><ol><li>首先基于条件独立假设学习输入输出的联合概率分布；</li><li>然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出 y。</li></ol><p>朴素贝叶斯法优点</p><ul><li>实现简单</li><li>学习与预测效率都高</li></ul><h3 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h3><p>朴素贝叶斯法国训练数据学习联合概率分布$P(X = x, Y = y)$。</p><p>$$P(X=x, Y=c_k)=P(X=x) * P(Y=c_k|X=x)=P(Y=c_k) * P(=X=x|Y=c_k)$$</p><p>具体的计算先验概率分布$P(Y=c_k)$ 和 条件概率分布$P(X=x|Y=c_k)$。</p><p>计算条件概率分布 $P(X=x|Y=c_k)$ 是有困难（有指数级数量的参数），如：</p><ul><li>假设 X 是[m, n]维数据，即训练集有n个特征，每一个是离散型变量，特征$X^{(n)}$ 有$S_n$ 种取值；</li><li>Y是[m, 1]维数据，Y 是离散型变量，取值为c_k，有 K 种取值；</li><li>那么条件概率分布 $P(X^{(1)}=x^{(1)}, X^{(2)}=x^{(2)},… X^{(n)}=x^{(n)}|Y=c_k)$ 中参数个数为 $K * \prod_{i=0}^n{S_n}$</li></ul><p>于是，朴素贝叶斯对条件概率分布做了条件独立性假设，使用离散型独立变量的联合分布计算公式<br>$$P(A, B)=P(A) * P(B)$$<br>$$P(A, B|C)=P(A|C) * P(B|C)$$<br>具体的条件独立性假设如下：</p><p>$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)}, X^{(2)}=x^{(2)},… X^{(n)}=x^{(n)}|Y=c_k)=\prod_{j=1}^jP(X^{(j)}=x^{(j)}|Y=c_k)$</p><p>特征之间相互独立，意味着每个特征独立的对结果产生影响。条件独立性假设在实际应用中是不现实的，因此得名“朴素“贝叶斯。</p><p>对于给定的 x，使用学习到的模型计算后验概率分布 $P(Y=c_k|X=x)$，将后验概率最大的类 C作为x的分类结果输出。后验概率使用贝叶斯公式计算：<br>$$P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k) * P(Y=c_k)}{P(X=x)}$$</p><p>其中，转化一下$P(X=x)$<br>$$P(X=x) = \sum_{k=1}^{k}P(X=x, Y=c_k)=\sum_{k=1}^{k}P(X=x|Y=c_k) * P(Y=c_k)$$<br>于是后验概率计算公式变为：<br>$$P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k) * P(Y=c_k)}{\sum_{k=1}^{k}P(X=x|Y=c_k) * P(Y=c_k)}$$</p><p>其中，已计算得先验概率$P(Y=c_k)$、 条件概率分布 $P(X=x|Y=c_k)=\prod_{j=1}^jP(X^{(j)}=x^{(j)}|Y=c_k)$</p><p>于是朴素贝叶斯分类器可表示为：<br>$$y=f(x)=\underset{c_k}{\operatorname{argmax}}\frac{P(Y=c_k) * P(X=x|Y=c_k)}{\sum_{k=1}^{k}P(Y=c_k) * P(X=x|Y=c_k)}$$<br>$$=\underset{c_k}{\operatorname{argmax}}\frac{P(Y=c_k) * \prod_{j=1}^jP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_{k=1}^{k}P(Y=c_k) * \prod_{j=1}^jP(X^{(j)}=x^{(j)}|Y=c_k)}$$</p><p>因为，对于所有$Y=c_k$分母都是相同的，所以可简写为：</p><p>$$y=f(x)=\underset{c_k}{\operatorname{argmax}}P(Y=c_k) * \prod_{j=1}^jP(X^{(j)}=x^{(j)}|Y=c_k)$$</p><h3 id="学习与分类算法"><a href="#学习与分类算法" class="headerlink" title="学习与分类算法"></a>学习与分类算法</h3><p>输入数据 $T={(x_1, y_1), (x_2, y_2), (x_3, y_3), … (x_N, y_N) }$，其中 $x_i=(x_i^{(1)}, x_i^{(2)}, x_i^{(3)}, … x_i^{(n)})$，$x_i^{(j)}$ 是第i个样本第j个特征，$x_i^{(j)}\in {a_{j1}, a_{j2}, a_{j3}, … a_{jS_j} }$，a_{jl} 是第j个特征可能取的第l个值，$j=1,2,3,…,n$，$l=1,2,3,…,S_j$，$y_i \in {c_1, c_2, c_3, …, c_K}$；实例 $x$</p><p>问题：求实例 $x$ 所属分类</p><p>答题思路：</p><ol><li>计算先验概率和条件概率<br>$$P(Y=c_k)= \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}, k=1,2, …, K$$</li></ol><p>$$P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl}, y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)}, \\<br>j=1, 2, …, n;l=1,2,…,Sj;k=1,2,…, K$$<br>2. 对于给定的实例 $x_i=(x_i^{(1)}, x_i^{(2)}, x_i^{(3)}, … x_i^{(n)})$，计算朴素贝叶斯分类器：<br>$$y=f(x)=\underset{c_k}{\operatorname{argmax}}P(Y=c_k) * \prod_{j=1}^JP(X^{(j)}=x^{(j)}|Y=c_k)$$</p><p>习题1<br><img src="/images/exercise-bayes-metho-1.png" alt=""></p><h3 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h3><p>用极大似然估计可能会出现所要估计的概率值为0的情况。这会影响后验概率的计算结果，使之产生偏差。解决这一问题的方法是采用贝叶斯估计。</p><p>条件概率的贝叶斯估计如下：<br>$$<br>P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl}, y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k) + S_j*\lambda}<br>$$<br>其中，$\lambda \ge 0$。当$\lambda = 0$时就是极大似然估计。常取$\lambda=1$，这时称为拉普拉斯平滑（Laplacian smoothing）</p><p>类似，先验概率的贝叶斯估计如下：<br>$$<br>P_\lambda(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k) + \lambda}{N + K\lambda}<br>$$</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>朴素贝叶斯法是典型的生成学习方法。</p><ol><li>由训练数据学习联合概率分布$P(X, Y)$<ol><li>由训练数据学习 $P(X|Y)$ 和 $P(Y)$的<strong>估计</strong>，从而得到联合概率分布。<ol><li>估计$P(X|Y)$时假设条件独立以降低计算困难。因为这个假设，模型需要计算的条件概率的量大大减少，朴素贝叶斯法的学习和训练大为简化。当然同时牺牲了一些分类准确率。</li></ol></li><li>概率估计可以用极大似然估计或贝叶斯估计。</li></ol></li><li>然后使用贝叶斯公式求得后验概率分布 $P(Y|X)=\frac{P(X, Y)}{P(X)}$。</li><li>将输入 x 分到后验概率最大的类 y。后验概率最大相当于 0-1 损失函数时的期望风险最小化。<br>$$<br>y= \underset{c_k}{\operatorname{argmax}}P(Y=c_k) * \prod_{j=1}^JP(X^{(j)}=a_{jl}|Y=c_k)<br>$$</li></ol><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul><li>《统计学习方法》-李航</li></ul>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯 </tag>
            
            <tag> 概率 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>联合概率分布</title>
      <link href="2021/02/24/lian-he-gai-lu-fen-bu/"/>
      <url>2021/02/24/lian-he-gai-lu-fen-bu/</url>
      
        <content type="html"><![CDATA[<p>在概率论中, 对两个随机变量X和Y，其联合分布 （Joint probability distribution） 是同时对于X和Y的概率分布。事件A 和B 同时发生概率记为 $P(A, B)$。</p><a id="more"></a><h3 id="离散随机变量的联合分布"><a href="#离散随机变量的联合分布" class="headerlink" title="离散随机变量的联合分布"></a>离散随机变量的联合分布</h3><p>对离散随机变量而言，联合分布概率质量函数为 $P(X = x &amp; Y = y)$，即<br>$$P(X=x\ and\ Y=y)=P(X=x) * P(Y=y|X=x)=P(Y=y) * P(=X=x|Y=y)$$</p><p>有<br>$$\sum _{x}\sum _{y}P(X=x\ and\ Y=y)=1$$</p><h3 id="离散型独立变量的联合分布"><a href="#离散型独立变量的联合分布" class="headerlink" title="离散型独立变量的联合分布"></a>离散型独立变量的联合分布</h3><p>对于两相互独立的事件$P(X)$ 和 $P(Y)$，任意x和y而言有离散随机变量<br>$$P(X=x\ and\ Y=y)=P(X=x) * P(Y=y)$$</p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯 </tag>
            
            <tag> 概率 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>极大似然估计</title>
      <link href="2021/02/23/ji-da-si-ran-gu-ji/"/>
      <url>2021/02/23/ji-da-si-ran-gu-ji/</url>
      
        <content type="html"><![CDATA[<p>在统计学中，最大似然估计（英语：Maximum Likelihood Estimation，简作MLE），也称极大似然估计，是用来估计一个概率模型的参数的一种方法。</p><a id="more"></a><h3 id="频率学派-vs-贝叶斯学派"><a href="#频率学派-vs-贝叶斯学派" class="headerlink" title="频率学派 vs 贝叶斯学派"></a>频率学派 vs 贝叶斯学派</h3><p>频率学派与贝叶斯学派探讨「不确定性」这件事时的出发点与立足点不同</p><ul><li>频率学派认为世界是确定的，有一个本体，这个本体的真值是不变的，我们的目标就是要找到这个真值或真值所在的范围；</li><li>贝叶斯学派认为世界是不确定的，人们对世界先有一个预判，而后通过观测数据对这个预判做调整，我们的目标是要找到最优的描述这个世界的概率分布。</li></ul><p>频率学派和贝叶斯学派解决问题的角度不同</p><ul><li>频率学派从「自然」角度出发，试图直接为「事件」本身建模，即事件在独立重复试验中发生的频率趋于极限，那么这个极限就是该事件的概率。</li><li>贝叶斯学派并不从试图刻画「事件」本身，而从「观察者」角度出发。贝叶斯学派并不试图说「事件本身是随机的」，或者「世界的本体带有某种随机性」，这套理论根本不言说关于「世界本体」的东西，而只是从「观察者知识不完备」这一出发点开始，构造一套在贝叶斯概率论的框架下可以对不确定知识做出推断的方法。</li></ul><h3 id="概率函数"><a href="#概率函数" class="headerlink" title="概率函数"></a>概率函数</h3><p>概率研究的是，已经知道了模型和参数后，给出一个事件发生的概率（频率）。<br>$$<br>概率函数：参数 + 观测 –&gt; 结果<br>$$</p><p>如果$θ$ 是已知确定的，$X$ 是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点$X$，其出现概率是多少（表示不同$X$出现的概率）。</p><p>概率函数用于在已知一些参数的情况下，预测接下来的观测所得到的结果。</p><h3 id="似然函数"><a href="#似然函数" class="headerlink" title="似然函数"></a>似然函数</h3><p>统计是根据给出的观测数据，利用这些数据进行建模和参数的预测。（例如推测是一个高斯模型，以及得到该模型的具体的参数 $σ,μ$ 等）。</p><p>$$<br>似然函数: 观测 + 结果 –&gt; 参数<br>$$</p><p>如果$X$ 是已知确定的，$θ$ 是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数$θ$，出现 $x$ 这个样本点的概率是多少(表示不同$θ$下，$X$出现的概率）。此时的函数也记作 $L(θ|x)$ 或 $L(x;θ)$ 或 $f(x;θ)$</p><p>似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性(可能性)。就是给定一组观测数据，对有关事物的性质的参数进行估计，即已知具体样本数据，对于模型的参数进行分析预测。</p><p><strong>最大似然</strong>就是模型参数的最大可能性。</p><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>最大似然估计是一种“模型已定，参数未知”的方法。即利用已知的样本的结果，在使用某个模型的基础上，反推最有可能导致这样结果的模型参数值。</p><p>最大似然估计的思想: 使得观测数据（样本）发生概率最大的参数就是最好的参数。</p><p>极大似然估计是典型的频率学派观点，它的基本思想是：待估计参数 $θ$ 是客观存在的，只是未知而已，当 $θ-mle$ 满足 $θ = θ-mle$ 时，该组观测样本 $(X1,X2,…,Xn) = (x1, x2,…,xn)$ 更容易被观测到，我们就说 $[θ-mle]$ 是 $[θ]$ 的极大似然估计值。也即，估计值 $[θ-mle]$ 使得事件发生的可能性最大。</p><p>假设分布率为 $P=p(x;θ)$，$x$ 是发生的样本，$θ$ 是代估计的参数，$p(x;θ)$ 表示估计参数为$θ$时，发生$x$的的概率。<br>那么当我们的样本值为：$x1,x2,…,xn$ 时，</p><p>$$L(θ) = L(x1,x2,…,xn;θ) = p(x1|θ) * p(x2|θ) * …p(xn|θ)$$</p><p>其中$L(θ)$ 成为样本的似然函数。假设有 $θ^$ 使得 $L(θ)$ 的取值最大，那么 $θ^$ 就叫做参数 $θ$ 的极大似然估计值。</p><h3 id="求解过程"><a href="#求解过程" class="headerlink" title="求解过程"></a>求解过程</h3><p>求极大似然函数估计值的一般步骤：</p><ol><li>写出似然函数；</li><li>对似然函数取对数，并整理；</li><li>求导数 ；</li><li>求对数似然函数的最大值（求导，解似然方程）。如果似然函数可导，那么就可以通过求导数的方式得到驻点，从而算出极大值。</li></ol><p>对一个独立同分布的样本集来说，总体的似然就是每个样本似然的乘积。总体的似然就是每个样本似然的乘积，但是连乘计算起来比较麻烦，为了求解方便，我们通常会将似然函数取对数，从而转成对数似然函数。</p><h3 id="贝叶斯方法中的应用"><a href="#贝叶斯方法中的应用" class="headerlink" title="贝叶斯方法中的应用"></a>贝叶斯方法中的应用</h3><p>贝叶斯方法思路如下：</p><ol><li>由训练数据学习联合概率分布$P(X, Y)$<ol><li>由训练数据学习 $P(X|Y)$ 和 $P(Y)$的<strong>估计</strong>，从而得到联合概率分布。<ol><li>估计$P(X|Y)$时假设条件独立以降低计算困难。因为这个假设，模型需要计算的条件概率的量大大减少，朴素贝叶斯法的学习和训练大为简化。当然同时牺牲了一些分类准确率。</li></ol></li><li>概率估计可以用<strong>极大似然估计</strong>或贝叶斯估计。</li></ol></li><li>然后使用贝叶斯公式求得后验概率分布 $P(Y|X)=\frac{P(X, Y)}{P(X)}$。</li><li>将输入 x 分到后验概率最大的类 y。后验概率最大相当于 0-1 损失函数时的期望风险最小化。</li></ol>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯 </tag>
            
            <tag> 概率 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贝叶斯分类器</title>
      <link href="2021/02/22/bei-xie-si-fen-lei-qi/"/>
      <url>2021/02/22/bei-xie-si-fen-lei-qi/</url>
      
        <content type="html"><![CDATA[<h3 id="贝叶斯决策论"><a href="#贝叶斯决策论" class="headerlink" title="贝叶斯决策论"></a>贝叶斯决策论</h3><p>贝叶斯决策论（Bayesian decision theory）是概率框架下实施决策的基本方法。</p><a id="more"></a><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>估计类条件概率的一种常用策略是先假定其具有某种确定的概率分布，再基于训练样本对概率分布的参数进行估计。</p><h3 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h3><p>不难发现，基于贝叶斯公式来估计后验概率 P(c|x) 的主要困难在于：类条件概率P(x|c) 是所有属性上的联合概率，难以从有限的训练样本直接估计而得。为避开这个问题，朴素贝叶斯分类器(naive Bayes classifier)采用了”属性条件独立性假设“(attribute conditional independence assumption)：对已知类别，假设所有属性相互独立。换言之，假设每个属性独立地对分类结果产生影响。</p><h3 id="半朴素贝叶斯分类器"><a href="#半朴素贝叶斯分类器" class="headerlink" title="半朴素贝叶斯分类器"></a>半朴素贝叶斯分类器</h3><p>为了降低贝叶斯公式中估计后验概率的困难，朴素贝叶斯分类器采用了属性条件独立性假设，但在现实任务中这个假设往往很难成立。于是，人们尝试对属性条件独立性假设进行一定程度的放松，由此产生了一类称为”半朴素贝叶斯分类器“(semi-naive Bayes classifiers) 的学习方法。</p><h3 id="贝叶斯网"><a href="#贝叶斯网" class="headerlink" title="贝叶斯网"></a>贝叶斯网</h3><p>贝叶斯网(Bayesian Network)也称信念网 (Belief Network)，它借助有向无环图（Directed Acyclic Graph， 简称 DAG）来刻画属性之间的依赖关系，并使用条件概率表（Conditional Probability Table，简称 CPT）来描述属性的联合概率分布。</p><h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>现实应用中往往会遇到”不完整“的训练样本。在这种数据不完整情况下，是否仍能对模型参数进行估计呢？<br>数据缺失的变量称”隐变量“（latent variable）。<br>EM（Expectation-Maximization）算法是常用的估计参数隐变量的利器。它是一种迭代式方法。</p><h3 id="阅读材料"><a href="#阅读材料" class="headerlink" title="阅读材料"></a>阅读材料</h3><ul><li>《机器学习》-周志华</li></ul>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>朴素贝叶斯方法概览</title>
      <link href="2021/02/21/po-su-bei-xie-si-fang-fa-gai-lan/"/>
      <url>2021/02/21/po-su-bei-xie-si-fang-fa-gai-lan/</url>
      
        <content type="html"><![CDATA[<p>源于托马斯·贝叶斯（Thomas Bayes）生前为解决一个“逆概”问题写的一篇文章，而这篇文章是在他死后才由他的一位朋友发表出来的。</p><a id="more"></a><h3 id="什么叫逆概问题"><a href="#什么叫逆概问题" class="headerlink" title="什么叫逆概问题"></a>什么叫逆概问题</h3><p>在贝叶斯写这篇文章之前，人们已经能够计算“正向概率”，如“假设袋子里面有N个白球，M个黑球，你伸手进去摸一把，摸出黑球的概率是多大”。<br>而一个自然而然的问题是反过来：“如果我们事先并不知道袋子里面黑白球的比例，而是闭着眼睛摸出一个（或好几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例作出什么样的推测”。这个问题，就是所谓的逆概问题。</p><h3 id="机器学习与贝叶斯方法"><a href="#机器学习与贝叶斯方法" class="headerlink" title="机器学习与贝叶斯方法"></a>机器学习与贝叶斯方法</h3><p>后来，贝叶斯方法席卷了概率论，并将应用延伸到各个问题领域，所有需要作出概率预测的地方都可以见到贝叶斯方法的影子，特别地，贝叶斯是机器学习的核心方法之一。<br>这背后的深刻原因在于，现实世界本身就是不确定的，人类的观察能力是有局限性的。</p><h3 id="贝叶斯方法思路"><a href="#贝叶斯方法思路" class="headerlink" title="贝叶斯方法思路"></a>贝叶斯方法思路</h3><ol><li>首先提出猜测（hypothesis，更为严格的说法是“假设”，这里用“猜测”更通俗易懂一点）</li><li>算出各种不同猜测的可能性大小。就是计算特定猜测的后验概率，对于连续的猜测空间则是计算猜测的概率密度函数。</li><li>算出最靠谱的猜测是什么。所谓的模型比较，模型比较如果不考虑先验概率的话就是最大似然方法。</li></ol>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯 </tag>
            
            <tag> 概率论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>先验概率和后验概率</title>
      <link href="2021/02/21/xian-yan-gai-lu-he-hou-yan-gai-lu/"/>
      <url>2021/02/21/xian-yan-gai-lu-he-hou-yan-gai-lu/</url>
      
        <content type="html"><![CDATA[<p>讲解先验概率（Prior probability）和后验概率（Posterior probability）最好的方法是举例子。</p><a id="more"></a><h3 id="例子-1"><a href="#例子-1" class="headerlink" title="例子 1"></a>例子 1</h3><p>一般情况下发生了交通事故，更容易堵车。</p><ul><li><p>先验概率：今天出门堵车的可能性 $P(堵车)$</p></li><li><p>条件概率：新闻上说发生了交通事故，今天出门堵车的概率 $P(堵车|交通事故)$</p></li><li><p>后验概率：已经堵车了，有发生交通事故的概率 $P(交通事故|堵车)$</p></li></ul><h3 id="例子-2"><a href="#例子-2" class="headerlink" title="例子 2"></a>例子 2</h3><p>玩lol占总人口60%，不玩lol的人占40%；</p><p>先验概率：$P(X=玩lol)=0.6；P(X=不玩lol)=0.4$</p><p>玩 lol 人中80%是男性，20%是女性；不玩 lol 人中20%是男性，80%是女性</p><p>条件概率分布：</p><ul><li><p>$P(Y=男性|X=玩lol)=0.8；P(Y=女性|X=玩lol)=0.2$</p></li><li><p>$P(Y=男性|X=不玩lol)=0.2；P(Y=女性|X=不玩lol)=0.8$</p></li></ul><p>那么已知玩家为男性的情况下，他是lol玩家的概率是多少？</p><p>依据贝叶斯准则可得：</p><p>$P(X=玩lol|Y=男性) = \frac{P(Y=男性|X=玩lol) * P(X=玩lol)}{P(Y=男性|X=玩lol) * P(X=玩lol) + P(Y=男性|X=不玩lol) * P(X=不玩lol)}$</p><p>最后算出的 $P(X=玩lol|Y=男性)$ 称之为X的后验概率，即它获得是在观察到事件Y发生后得到的。</p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯 </tag>
            
            <tag> 概率论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bias-Variance Tradeoff</title>
      <link href="2021/02/19/bias-variance-tradeoff/"/>
      <url>2021/02/19/bias-variance-tradeoff/</url>
      
        <content type="html"><![CDATA[<p>While training our model Bias and Variance plays a key role in achieving the required accuracy of the model.There need to be a balance or we can say compromise between Bias and Variance, so to avoid overfitting and underfitting of the model. This compromise is called Tradeoff.</p><a id="more"></a><h3 id="Bias（偏差）"><a href="#Bias（偏差）" class="headerlink" title="Bias（偏差）"></a>Bias（偏差）</h3><p>In the simplest terms, Bias is the difference between the Predicted Value and the Expected Value.<br>简而言之，偏差是预测值和实际值之间的误差。</p><p>μ = mean(Y’)<br>Bias = μ - Y</p><p><img src="/images/bias.png" alt=""></p><p>Model with high bias pays very little attention to the training data and oversimplifies the model causing leading to underfitting.</p><p>高偏差模型无法捕捉数据规律，导致欠拟合（underfitting）。一般情况下过于简单的模型导致高偏差欠拟合。</p><h3 id="Variance（方差）"><a href="#Variance（方差）" class="headerlink" title="Variance（方差）"></a>Variance（方差）</h3><p>简而言之，方差衡量预测结果的离散程度。<br>μ = mean(Y’)<br>variance(Y’) = (Y’ - μ)^2</p><p><img src="/images/variance.png" alt=""></p><p>Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.</p><p>方差高的模型过于捕捉数据规律（甚至捕捉了噪声数据规律），导致无法泛化到新数据上。高方差的模型一般情况下训练数据上表现很好、测试数据上表现很差。</p><h3 id="Bias-Variance-Tradeoff"><a href="#Bias-Variance-Tradeoff" class="headerlink" title="Bias-Variance Tradeoff"></a>Bias-Variance Tradeoff</h3><p><img src="/images/bias-variance-tradeoff.png" alt=""></p><p>If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias.</p><p>So we need to find the right/good balance without overfitting and underfitting the data.</p><h3 id="Total-Error"><a href="#Total-Error" class="headerlink" title="Total Error"></a>Total Error</h3><p><img src="images/total-error.png" alt=""></p><p>To build a good model, we need to find a good balance between bias and variance such that it minimizes the total error.</p><p><img src="/images/optimal-balance-of-bias-and-variance.png" alt=""></p><p>An optimal balance of bias and variance would never overfit or underfit the model.</p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 偏差 </tag>
            
            <tag> 方差 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ensemble Learning</title>
      <link href="2021/02/19/ensemble-learning/"/>
      <url>2021/02/19/ensemble-learning/</url>
      
        <content type="html"><![CDATA[<p>集成多个算法结果得到比任何单个算法更好的结果，称为集成学习。</p><a id="more"></a><p>常见的集成学习方法有：</p><ul><li>贝叶斯最优分类器</li><li>Bootstrap aggregating（又称 Bagging）。代表算法随机森林。</li><li>Boosting。代表算法 AdaBoost、XGBoost。</li><li>贝叶斯参数平均（Bayesian Parameter Averaging， BPA）</li><li>贝叶斯模型组合（BMC）</li><li>桶模型（Bucket of models）</li><li>Stacking（堆叠泛化）</li></ul><h3 id="Bootstrap-Aggregating"><a href="#Bootstrap-Aggregating" class="headerlink" title="Bootstrap Aggregating"></a>Bootstrap Aggregating</h3><p>又称 Bagging，代表算法随机森林。集成模型中的每个模型在投票时具有相同的权重。为了减小模型方差，Baging使用随机抽取的子训练集训练集成中的每个模型。例如，随机森林算法将随机决策树与Bagging相结合，以实现更高的分类准确度。</p><ul><li>每个基评估器相互独立。</li><li>基评估器准确率得高于0.5。</li></ul><p>随机森林的效果为什么比单个模型好？<br>假设有三个分类器，每一个准确率是0.8。少数服从多数原则下投票时，任意两个以上分类正确，集成结果也会正确。任意两个以上分类正确的概率参考如下公式: 3* (0.8)^2<em>0.2+1</em>(0.8)^3*1=0.896<br>    <img src="/images/error-of-rf.png" alt=""></p><h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>代表算法 AdaBoost、XGBoost。Boosting通过在训练新模型实例时更注重先前模型错误分类的实例来增量构建集成模型。在某些情况下，Boosting已被证明比Bagging可以得到更好的准确率，不过它也更倾向于对训练数据过拟合。</p><ul><li>基评估器是相关的、按顺序构建的。</li></ul><h3 id="Stacking（堆叠泛化）"><a href="#Stacking（堆叠泛化）" class="headerlink" title="Stacking（堆叠泛化）"></a>Stacking（堆叠泛化）</h3><p>首先，使用可用数据训练所有其他算法，然后训练组合器算法以使用其他算法的所有预测作为附加输入进行最终预测。通常用逻辑回归模型作为组合器。</p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 集成学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>随机森林</title>
      <link href="2021/02/19/sui-ji-sen-lin/"/>
      <url>2021/02/19/sui-ji-sen-lin/</url>
      
        <content type="html"><![CDATA[<p>随机森林是一种具有代表性的 Bagging（装袋法）集成算法。</p><a id="more"></a><h3 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h3><p>把 n 个基评估器的结果，汇总后得到集成算法的结果，以此得到比单个评估器更好的模型表现。</p><ul><li>其中每个基评估器都是决策树，所以称呼森林；</li><li>随机挑选特征和数据，生成 n 个决策树，所以称随机。</li></ul><h3 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h3><ul><li>Bootsrap：随机森林通过有放回的随机抽样技术来形成不同的训练数据，bootsrap等于 TRUE，表示使用这种抽样技术。<ul><li>含有n个样本的数据集中，进行有放回随机采样n 次，组成大小为n的自助集。</li><li>由于有放回随机采样，每个自助集都不同，训练的决策树也不同。</li><li>由于有放回采样，有些数据样本会被多个自助集采到、有些数据集始终不会被采到。</li></ul></li><li>Out of  Bag：一般来说，自助集平均包含 63%的原始数据。每个样本被抽到某个自助集中的概率为 1 - (1 - 1/n)^n 。当 n 足够大时，这个概率收敛于 1 - 1/e, 约等于 0.632。因此会有 37% 的训练数据被浪费掉，没有参与建模，这些数据被称为袋外数据(out of bag data, 简写 OOB)。<ul><li>除了最初划分为测试集的数据之外，这部分OOB数据也可以加入到测试集中。</li><li>当然，使用随机森林时，我们可以不划分测试集和训练集，只需要用袋外数据作为测试集即可。（当n 和 n_estimators 都不够大时，很可能就没有oob数据）</li></ul></li></ul><h3 id="相关问题"><a href="#相关问题" class="headerlink" title="相关问题"></a>相关问题</h3><ol><li>常用的集成算法有哪些？<ol><li>Bagging （装袋）法：基评估器互相独立、平行。代表算法是随机森林</li><li>Boosting （提升）法：基评估器有顺序，逐渐提升。代表算法有 XGboost</li><li>Stacking 法</li></ol></li><li>随机森林用了什么方法，来保证集成的效果一定好于单个分类器?<ol><li>例如随机森林中有25棵树，每棵树分类错误的概率是 0.2</li><li>当且仅当13及以上的树都分类错误时随机森林集成结果才分类错误</li><li>13都分类错误的概率：<br><img src="/images/error-of-rf.png" alt=""></li></ol></li><li>随机森林如何保证，每一个决策树都不一样？（随机性）<ol><li>有放回随机抽样训练数据，训练决策树</li><li>随机选择部分特征，构造决策树</li></ol></li><li>使用随机森林有什么特殊要求？<ol><li>要求每个基评估器都相互独立；</li><li>基分类器的分类准确率要超过随机分类器（即，随机森林使用的基分类器的准确率至少要超过50%）。当基分类器的误差率小于0.5，即准确率大于0.5 时，集成的效果是比基分类器要好的。相反， 当基分类器的误差率大于0.5，袋装的集成算法就失效了。</li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 集成学习 </tag>
            
            <tag> 树模型 </tag>
            
            <tag> 随机森林 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用交叉验证选择决策树预剪枝参数 max_depth</title>
      <link href="2021/02/15/shi-yong-jiao-cha-yan-zheng-xuan-ze-jue-ce-shu-yu-jian-zhi-can-shu-max-depth/"/>
      <url>2021/02/15/shi-yong-jiao-cha-yan-zheng-xuan-ze-jue-ce-shu-yu-jian-zhi-can-shu-max-depth/</url>
      
        <content type="html"><![CDATA[<p>max_depth  限制树的最大深度，超过设定深度的树枝全部剪掉。</p><a id="more"></a><h3 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> tree<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_wine<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_splitwine <span class="token operator">=</span> load_wine<span class="token punctuation">(</span><span class="token punctuation">)</span>Xtrain<span class="token punctuation">,</span> Xtest<span class="token punctuation">,</span> Ytrain<span class="token punctuation">,</span> Ytest <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>wine<span class="token punctuation">.</span>data<span class="token punctuation">,</span>wine<span class="token punctuation">.</span>target<span class="token punctuation">,</span>test_size<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>feature_name <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'酒精'</span><span class="token punctuation">,</span><span class="token string">'苹果酸'</span><span class="token punctuation">,</span><span class="token string">'灰'</span><span class="token punctuation">,</span><span class="token string">'灰的碱性'</span><span class="token punctuation">,</span><span class="token string">'镁'</span><span class="token punctuation">,</span><span class="token string">'总酚'</span><span class="token punctuation">,</span><span class="token string">'类黄酮'</span><span class="token punctuation">,</span><span class="token string">'非黄烷类酚类'</span><span class="token punctuation">,</span><span class="token string">'花青素'</span><span class="token punctuation">,</span><span class="token string">'颜色强度'</span><span class="token punctuation">,</span><span class="token string">'色调'</span><span class="token punctuation">,</span><span class="token string">'od280/od315稀释葡萄酒'</span><span class="token punctuation">,</span>'脯氨酸’<span class="token punctuation">]</span></code></pre><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>默认参数模型准确率</p><pre class=" language-python"><code class="language-python">clf <span class="token operator">=</span> tree<span class="token punctuation">.</span>DecisionTreeClassifier<span class="token punctuation">(</span>criterion<span class="token operator">=</span><span class="token string">"entropy"</span>                                    <span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token number">30</span>                                    <span class="token punctuation">,</span>splitter<span class="token operator">=</span><span class="token string">"random"</span>                                    <span class="token punctuation">)</span>clf <span class="token operator">=</span> clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>Xtrain<span class="token punctuation">,</span> Ytrain<span class="token punctuation">)</span>score <span class="token operator">=</span> clf<span class="token punctuation">.</span>score<span class="token punctuation">(</span>Xtest<span class="token punctuation">,</span> Ytest<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>score<span class="token punctuation">)</span></code></pre><p><code>0.88</code></p><h3 id="默认-max-depth-生成决策树"><a href="#默认-max-depth-生成决策树" class="headerlink" title="默认 max_depth 生成决策树"></a>默认 max_depth 生成决策树</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> graphvizdot_data <span class="token operator">=</span> tree<span class="token punctuation">.</span>export_graphviz<span class="token punctuation">(</span>clf                                <span class="token punctuation">,</span>feature_names<span class="token operator">=</span> feature_name                                <span class="token punctuation">,</span>class_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"琴酒"</span><span class="token punctuation">,</span><span class="token string">"雪莉"</span><span class="token punctuation">,</span><span class="token string">"贝尔摩德"</span><span class="token punctuation">]</span>                                <span class="token punctuation">,</span>filled<span class="token operator">=</span><span class="token boolean">True</span>                                <span class="token punctuation">,</span>rounded<span class="token operator">=</span><span class="token boolean">True</span>                                <span class="token punctuation">)</span>graph <span class="token operator">=</span> graphviz<span class="token punctuation">.</span>Source<span class="token punctuation">(</span>dot_data<span class="token punctuation">)</span>graph</code></pre><p><img src="/images/max_depth/11.png" alt=""></p><h3 id="max-depth-3-生成决策树"><a href="#max-depth-3-生成决策树" class="headerlink" title="max_depth=3 生成决策树"></a>max_depth=3 生成决策树</h3><pre class=" language-python"><code class="language-python">clf <span class="token operator">=</span> tree<span class="token punctuation">.</span>DecisionTreeClassifier<span class="token punctuation">(</span>criterion<span class="token operator">=</span><span class="token string">"entropy"</span>                                    <span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token number">30</span>                                    <span class="token punctuation">,</span>splitter<span class="token operator">=</span><span class="token string">"random"</span>                                    <span class="token punctuation">,</span>max_depth<span class="token operator">=</span><span class="token number">3</span>                                <span class="token comment" spellcheck="true">#    ,min_samples_leaf=10</span>                                <span class="token comment" spellcheck="true">#    ,min_samples_split=25</span>                                    <span class="token punctuation">)</span>clf <span class="token operator">=</span> clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>Xtrain<span class="token punctuation">,</span> Ytrain<span class="token punctuation">)</span>dot_data <span class="token operator">=</span> tree<span class="token punctuation">.</span>export_graphviz<span class="token punctuation">(</span>clf                                <span class="token punctuation">,</span>feature_names<span class="token operator">=</span> feature_name                                <span class="token punctuation">,</span>class_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"琴酒"</span><span class="token punctuation">,</span><span class="token string">"雪莉"</span><span class="token punctuation">,</span><span class="token string">"贝尔摩德"</span><span class="token punctuation">]</span>                                <span class="token punctuation">,</span>filled<span class="token operator">=</span><span class="token boolean">True</span>                                <span class="token punctuation">,</span>rounded<span class="token operator">=</span><span class="token boolean">True</span>                                <span class="token punctuation">)</span>graph <span class="token operator">=</span> graphviz<span class="token punctuation">.</span>Source<span class="token punctuation">(</span>dot_data<span class="token punctuation">)</span>graph</code></pre><p><img src="/images/max_depth/2.png" alt=""></p><h3 id="交叉验证-学习曲线"><a href="#交叉验证-学习曲线" class="headerlink" title="交叉验证 学习曲线"></a>交叉验证 学习曲线</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plttest <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    clf <span class="token operator">=</span> tree<span class="token punctuation">.</span>DecisionTreeClassifier<span class="token punctuation">(</span>max_depth<span class="token operator">=</span>i<span class="token operator">+</span><span class="token number">1</span>                                    <span class="token punctuation">,</span>criterion<span class="token operator">=</span><span class="token string">"entropy"</span>                                    <span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token number">30</span>                                    <span class="token punctuation">,</span>splitter<span class="token operator">=</span><span class="token string">"random"</span>                                    <span class="token punctuation">)</span>    clf <span class="token operator">=</span> clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>Xtrain<span class="token punctuation">,</span> Ytrain<span class="token punctuation">)</span>    score <span class="token operator">=</span> clf<span class="token punctuation">.</span>score<span class="token punctuation">(</span>Xtest<span class="token punctuation">,</span> Ytest<span class="token punctuation">)</span>    test<span class="token punctuation">.</span>append<span class="token punctuation">(</span>score<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">,</span>test<span class="token punctuation">,</span>color<span class="token operator">=</span><span class="token string">"red"</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"max_depth"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><img src="/images/max_depth/2.png" alt=""></p><h3 id="交叉验证-学习曲线-1"><a href="#交叉验证-学习曲线-1" class="headerlink" title="交叉验证 学习曲线"></a>交叉验证 学习曲线</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plttest <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    clf <span class="token operator">=</span> tree<span class="token punctuation">.</span>DecisionTreeClassifier<span class="token punctuation">(</span>max_depth<span class="token operator">=</span>i<span class="token operator">+</span><span class="token number">1</span>                                    <span class="token punctuation">,</span>criterion<span class="token operator">=</span><span class="token string">"entropy"</span>                                    <span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token number">30</span>                                    <span class="token punctuation">,</span>splitter<span class="token operator">=</span><span class="token string">"random"</span>                                    <span class="token punctuation">)</span>    clf <span class="token operator">=</span> clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>Xtrain<span class="token punctuation">,</span> Ytrain<span class="token punctuation">)</span>    score <span class="token operator">=</span> clf<span class="token punctuation">.</span>score<span class="token punctuation">(</span>Xtest<span class="token punctuation">,</span> Ytest<span class="token punctuation">)</span>    test<span class="token punctuation">.</span>append<span class="token punctuation">(</span>score<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">,</span>test<span class="token punctuation">,</span>color<span class="token operator">=</span><span class="token string">"red"</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"max_depth"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><img src="/images/max_depth/3.png" alt=""></p><h3 id="max-depth-3-时模型准确率"><a href="#max-depth-3-时模型准确率" class="headerlink" title="max_depth=3 时模型准确率"></a>max_depth=3 时模型准确率</h3><pre class=" language-python"><code class="language-python">clf <span class="token operator">=</span> tree<span class="token punctuation">.</span>DecisionTreeClassifier<span class="token punctuation">(</span>criterion<span class="token operator">=</span><span class="token string">"entropy"</span><span class="token punctuation">,</span>                                    max_depth<span class="token operator">=</span><span class="token number">3</span>                                    <span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token number">30</span>                                    <span class="token punctuation">,</span>splitter<span class="token operator">=</span><span class="token string">"random"</span>                                    <span class="token punctuation">)</span>clf <span class="token operator">=</span> clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>Xtrain<span class="token punctuation">,</span> Ytrain<span class="token punctuation">)</span>score <span class="token operator">=</span> clf<span class="token punctuation">.</span>score<span class="token punctuation">(</span>Xtest<span class="token punctuation">,</span> Ytest<span class="token punctuation">)</span>score</code></pre><p><code>0.94</code></p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
          <category> exercises </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
            <tag> 树模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>支持向量机相关习题</title>
      <link href="2021/01/22/zhi-chi-xiang-liang-ji-xiang-guan-xi-ti/"/>
      <url>2021/01/22/zhi-chi-xiang-liang-ji-xiang-guan-xi-ti/</url>
      
        <content type="html"><![CDATA[<ol><li>比较感知机的对偶形式与线性可分支持向量机的对偶形式</li><li><strong>已知正例点$x_1=(1,2)^T$，$x_2=(2,3)^T$，$x_3=(3,3)^T$,负例点$x_4=(2,1)^T$，$x_5=(3,2)^T$，试求最大间隔分离超平面和分类决策函数，并在图上画出分离超平面、间隔边界及支持向量</strong></li><li>**线性支持向量机还可以定义为以下形式，试求其对偶形式：</li></ol><p>$$\min_{w,b,\xi}{\frac{1}{2}|w|^2}+C\sum^N_{i=1}\xi_i^2\s.t.{\quad}y_i(w{\cdot}x_i+b)\ge1-\xi_i,,i=1,2,\cdots,N\\xi_i\ge0,,i=1,2,\cdots,N$$</p><ol start="4"><li><strong>证明内积的正整数幂函数$K(x,z)=(x{\cdot}z)^p$是正定核函数，这里$p$是正整数，$x,z{\in}R^n$</strong></li><li>自己编程实现SVM，并在西瓜数据集 3.0αα 测试。</li><li>试证明样本空间中任意点 $x$ 到超平面 $(w,b)$ 的的距离为式 (6.2)。</li><li>试使用 LIBSVM，在西瓜数据集 3.0α 上分别用线性核和高斯核训练一个 SVM，并比较其支持向量的差别。</li><li>选择两个 UCI 数据集，分别用线性核和高斯核训练一个 SVM，并与BP 神经网络和 C4.5 决策树进行实验比较</li><li>试讨论线性判别分析与线性核支持向量机在何种条件下等价。</li><li>试述高斯核 SVM 与 RBF 神经网络之间的联系。</li><li>试析 SVM 对噪声敏感的原因。</li><li>试给出式 (6.52) 的完整 KKT 条件。</li><li>以西瓜数据集 3.0α 的”密度”为输入”含糖率”为输出，试使用LIBSVM 训练一个 SVR。</li><li>试使用核技巧推广对率回归，产生”核对率回归”。</li><li>试设计一个能显著减少 SVM 中支持向量的数目而不显著降低泛化性能的方法。</li></ol>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
          <category> exercises </category>
          
      </categories>
      
      
        <tags>
            
            <tag> svm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>支持向量机内容概要</title>
      <link href="2021/01/20/zhi-chi-xiang-liang-ji-nei-rong-gai-yao/"/>
      <url>2021/01/20/zhi-chi-xiang-liang-ji-nei-rong-gai-yao/</url>
      
        <content type="html"><![CDATA[<h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p>支持向量机（Support Vector Machine）是一种二类分类模型。基本原理是，特征空间上几何间隔最大化。</p><blockquote><p>In its most simple type, SVM doesn’t support multiclass classification natively. It supports binary classification and separating data points into two classes. For multiclass classification, the same principle is utilized after breaking down the multiclassification problem into multiple binary classification problems. – <a href="https://www.baeldung.com/cs/svm-multiclass-classification#:~:text=Multiclass%20Classification%20Using%20SVM,data%20points%20into%20two%20classes.&text=The%20idea%20is%20to%20map,separation%20between%20every%20two%20classes" target="_blank" rel="noopener">Multiclass Classification Using Support Vector Machines</a></p></blockquote><ul><li>针对线性可分数据集，使用线性可分支持向量机即可。通过硬（几何）间隔最大化学习得到分离超平面 $w^* \cdot x + b^*=0$ 和相应的分类决策函数 $f(x)=sign(w^* \cdot x + b^*)$ .<ul><li>几何间隔最大化：求解相应的凸二次规划问题；从而得到让几何间隔最大的 $w^*, b^*$。</li><li>求解凸二次规划问题：构建相应的拉格朗日函数；用拉格朗日对偶性，求解对偶问题；从而得到凸二次规划问题的解</li></ul></li><li>针对近似线性可分数据集，引入松弛变量作为惩罚项（称软间隔最大化），然后再使用线性可分支持向量机。</li><li>针对非线性问题，通过非线性变换（核技巧）把非线性问题转换成线性问题。求解线性问题，然后得到对应的非线性问题的解。</li></ul><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li>《统计学习方法》–李航</li></ul>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> svm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k-means 习题</title>
      <link href="2021/01/12/k-means-xi-ti/"/>
      <url>2021/01/12/k-means-xi-ti/</url>
      
        <content type="html"><![CDATA[<ol><li>简述一下K-means算法的原理和工作流程<ol><li>输入k，表示数据聚类到k簇</li><li>随机选出 k个质心</li><li>将每个点归到距离最近的质心所属簇</li><li>求簇中每个点均值，求出该质心新的位置</li><li>循环3、4</li><li>当质心不再发生变化停止迭代</li></ol></li><li>K-means中常用的到中心距离的度量有哪些？<ol><li>欧几里得距离</li><li>曼哈顿距离</li><li>余弦相似度</li></ol></li><li>K-means 中的k值如何选取?<ol><li>随机选取，画出k和轮廓系数（Silhouette Coefficient）的学习曲线，选择最佳k值</li><li>手肘法：</li></ol></li><li>K-means 算法中初始点的选择对最终结果有影响吗？<ol><li>不同的初始值结果可能不一样</li><li>对计算速度有影响。更好的选取初始点，可以加快聚类速度</li></ol></li><li>K-means 聚类中每个类别中心的初始点如何选择？<ol><li>随机选择初始质心</li><li>不停更新质心，求出更佳中心</li></ol></li><li>K-means 中空聚类的处理<ol><li>选择一个距离当前任何质心最远的点。这将消除当前对总平方误差影响最大的点。</li><li>从具有最大SSE的簇中选择一个替补的质心，这将分裂簇并降低聚类的总SSE。如果有多个空簇，则该过程重复多次。</li><li>如果噪点或者孤立点过多，考虑更换算法，如密度聚类</li></ol></li><li>K-means 是否会一直陷入选择质心的循环停不下来？<ol><li>不会，有数学证明Kmeans一定会收敛，大概思路是利用SSE的概念（也就是误差平方和），即每个点到自身所归属质心的距离的平方和，这个平方和是一个凸函数，通过迭代一定可以到达它的局部最优解。（不一定是全局最优解）</li><li>但同时为了让模型更快，可以设置：<ol><li>迭代次数设置</li><li>设定收敛判断距离</li></ol></li></ol></li><li>如何快速收敛数据量超大的 K-means？<ol><li>方法1：洗牌后的数据，先选区部分数据，求出质心，确定局部最佳 k 值；使用上述求出的质心，作为初始质心，聚类全量数据</li><li>方法2：Mini Batch Kmeans使用了一种叫做Mini Batch（分批处理）的方法对数据点之间的距离进行计算。Mini Batch的好处是计算过程中不必使用所有的数据样本，而是从不同类别的样本中抽取一部分样本来代表各自类型进行计算。由于计算样本数量少，所以会相应的减少运行时间，但另一方面抽样页必然会带来准确度的下降。</li></ol></li><li>K-means算法的优点和缺点是什么？<ol><li>缺点：<ol><li>需要人为输入 k值，且很难确定；</li><li>局部最优；</li><li>对噪音和异常点敏感；</li><li>需样本存在均值（限定数据种类）；</li><li>聚类效果依赖于聚类中心的初始化；</li><li>对于非凸数据集或类别规模差异太大的数据效果不好</li></ol></li><li>优点：</li><li>简单、易于理解、实现容易；</li><li>可解释性强；</li><li>基于少量数据也可达到较好效果。</li></ol></li><li>如何对K-means聚类效果进行评估？<ol><li>簇内距离最小化，簇外距离最大化</li><li>计算每个点轮廓系数；观察每个类中各个点轮廓系数分布<ol><li>a是Xi与同簇的其他样本的平均距离，称为凝聚度</li><li>b是Xi与最近簇中所有样本的平均距离，称分离度</li><li>1 - a/max(a,b)<ol><li>当 a==b，轮廓系数=0，说明这两个簇中数据应该属于同一个簇</li><li>a &gt; b, 轮廓系数负数，说明聚类效果不好</li><li>a &lt; b ，轮廓系数越接近1，说明聚类效果越好</li></ol></li></ol></li></ol></li><li>K-Means与KNN有什么区别<ol><li>KNN是分类算法，K-means是聚类算法；</li><li>KNN是监督学习，K-means是非监督学习</li><li>KNN喂给它的数据集是带Label的数据，已经是完全正确的数据，K-means喂给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序。</li><li>KNN没有明显的前期训练过程，K-means有明显的前期训练过程</li><li>K的含义KNN来了一个样本x,要给它分类，即求出它的y,就从数据集中，在X附近找距离它最近的K个数据点，这K个数据点，类别C占的个数最多，就把x的label设为c.</li><li>K-means中K是人工固定好的数字，假设数据集合可以分为k个簇，由于是依靠人工定好，需要一些先验知识。</li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
          <category> exercises </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k-means </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k-means 算法知识点梳理</title>
      <link href="2021/01/12/k-means-suan-fa-zhi-shi-dian-shu-li/"/>
      <url>2021/01/12/k-means-suan-fa-zhi-shi-dian-shu-li/</url>
      
        <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>k-means 算法是最常用的聚类算法，属于无监督学习算法。</p><h2 id="工作原理和流程"><a href="#工作原理和流程" class="headerlink" title="工作原理和流程"></a>工作原理和流程</h2><ol><li>输入k，表示数据聚类到k簇</li><li>随机选出 k个质心（Centroid）</li><li>将每个点归到距离最近的质心所属簇</li><li>求簇中每个点均值，求出该质心新的位置</li><li>循环3、4</li><li>当质心不再发生变化停止迭代</li></ol><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ol><li>最大的特点是简单、好理解</li><li>只能应用于连续型的数据，因此对一些数据集需要先处理</li><li>要在聚类前需要手工指定要分成几类</li></ol><h2 id="如何评估？"><a href="#如何评估？" class="headerlink" title="如何评估？"></a>如何评估？</h2><p>KMeans的目标是确保“簇内差异小，簇外差异大。</p><p>使用轮廓系数同时衡量：</p><ol><li>样本与其自身所在的簇中的其他样本的相似度 a，等于样本与同一簇中所有其他点之间的平均距离</li><li>样本与其他簇中的样本的相似度b，等于样本与下一个最近的簇中的所有点之间的平均距离<br><img src="/images/%E8%BD%AE%E5%BB%93%E7%B3%BB%E6%95%B0.png" alt=""></li></ol><ol><li>值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似</li><li>当样本点与簇外的样本更相似的时候，轮廓系数就为负。</li><li>当轮廓系数为0时，则代表两个簇中的样本相似度一致，两个簇本应该是一个簇。</li><li>sklearn中，我们使用模块metrics中的类 silhouette_score 来计算轮廓系数，它返回的是一个数据集中，所有样本的轮廓系数的均值。但我们还有同在metrics模块中silhouette_sample，它的参数与轮廓系数一致，但返回的是数据集中每个样本自己的轮廓系数。</li></ol><h2 id="相关公式"><a href="#相关公式" class="headerlink" title="相关公式"></a>相关公式</h2><p>每个点分配到距离最近的质心所属簇。几种距离计算方式：</p><ol><li>欧几里得距离<ol><li><img src="/images/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E8%B7%9D%E7%A6%BB.png" alt=""></li><li><img src="/images/inertial.png" alt=""></li><li><img src="/images/total_inertial.png" alt=""></li></ol></li><li>曼哈顿距离<ol><li><img src="/images/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E8%B7%9D%E7%A6%BB.png" alt=""></li></ol></li><li>余弦相似度：用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小<ol><li><img src="/images/CosineSimilarity.png" alt=""></li></ol></li></ol><h2 id="画图分析"><a href="#画图分析" class="headerlink" title="画图分析"></a>画图分析</h2><ol><li>scatter 聚类图</li><li>每个点的轮廓系数柱状图<br><img src="/images/Silhouette.png" alt=""></li></ol>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k-means </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ID3 算法</title>
      <link href="2021/01/07/id3-suan-fa/"/>
      <url>2021/01/07/id3-suan-fa/</url>
      
        <content type="html"><![CDATA[<p>决策树是一个非常常见并且优秀的机器学习算法，它易于理解、可解释性强，其可作为分类算法，也可用于回归模型。基本算法有 ID3、C4.5、CART。</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><img src="/images/ID3.C4.5.CART.png" alt=""></p><p>ID3算法（Iterative Dichotomiser 3 迭代二叉树3代）是一个由 Ross Quinlan 发明的用于决策树的算法。</p><p>这个算法是建立在奥卡姆剃刀的基础上：越是小型的决策树越优于大的决策树（简单理论）。(尽管如此，该算法也不是总是生成最小的树形结构。)</p><blockquote><p>奥卡姆剃刀（英语：Occam’s Razor, Ockham’s Razor），又称“奥坎的剃刀”，拉丁文为lex parsimoniae，意思是简约之法则，是由14世纪逻辑学家、圣方济各会修士奥卡姆的威廉（William of Occam，约1287年至1347年，奥卡姆（Ockham）位于英格兰的萨里郡）提出的一个解决问题的法则，他在《箴言书注》2卷15题说“切勿浪费较多东西，去做‘用较少的东西，同样可以做好的事情’。</p></blockquote><p>信息熵越大，从而样本纯度越低。</p><p>ID3 算法的核心思想就是以信息增益来度量特征选择，选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间（C4.5 也是贪婪搜索）。</p><blockquote><p>贪心算法（英语：greedy algorithm），又称贪婪算法，是一种在每一步选择中都采取在当前状态下最好或最优（即最有利）的选择，从而希望导致结果是最好或最优的算法。比如在旅行推销员问题中，如果旅行员每次都选择最近的城市，那这就是一种贪心算法。</p></blockquote><ul><li><input checked="" disabled="" type="checkbox"> 贪婪算法</li></ul>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
            <tag> 树模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Advantages of the different impurity metrics?</title>
      <link href="2021/01/07/advantages-of-the-different-impurity-metrics/"/>
      <url>2021/01/07/advantages-of-the-different-impurity-metrics/</url>
      
        <content type="html"><![CDATA[<p>分类决策树中，我们的目标函数是最大化每次切分数据带来的信息增益：</p><p><img src="/images/information-gain-2021010701.png" alt=""></p><p>其中 $f$ 是决策问题基于的特征；$D_p$ 和 $D_j$ 分别是父节点数据集和第 $j$ 个子节点数据集； $I$ 不纯度衡量指标； $N$ 是数据集总大小，$N_j$ 是第 $j$ 个子节点上数据集大小。</p><p>分类树衡量切分数据集方案好坏的的标尺，最常用的是基尼不纯度（gini impurity）和信息熵（entropy）。另外，有些地方也会用 Classification Error ($I_E$)。<br><img src="/images/information-gain-20210107.png" alt=""></p><p>信息熵的定义如下：</p><p><img src="/images/entropy.202101071.png" alt=""></p><p>for all “non-empty” classes<br>共有 C 种分类，且每个类型对于的数据集不全为空的数据集中</p><p><img src="/images/entropy.202101072.png" alt=""></p><p>$p(i|t)$ 是 $t$ 节点上属于分类 $i$ 的数据集所占的比例；如果一个节点上所有数据属于单个类型，该节点上数据集的信息熵是 0；如果一个节点上的所有数据均匀分布于不同类型，该节点上数据集的信息熵是最大；</p><p>基尼不纯度可被当做衡量<strong>最小化数据切分错误的概率</strong>的指标。</p><p><img src="/images/gini-impurity.202001071.png" alt=""></p><p>实际应用中，使用基尼系数还是信息熵作为衡量指标，生成的决策树差别不会太大，因此时间应该更多的花在不同剪枝策略的实验上。</p><p><img src="/images/error.202001071.png" alt=""></p><p><img src="/images/overview-plot.20200107.png" alt=""></p><p>上图可见，Classification Error 对节点上某个类型所占比例的变化不是很敏感，因此不太适合作为目标函数，用来衡量剪枝效果倒是可以。</p><p><img src="/images/split.20200107.png" alt=""></p><p>假设数据集 $D_p$ 中 40条数据属于类型1，1条数据属于类型2，对比A、B 两种切分数据集的方案。</p><p>可发现，使用 Classification Error 作为目标函数时，A()、B 两种方案的信息增益一样，都是 $IG_E = 0$，无法衡量出哪个方案更好。</p><p><img src="/images/calc_1.png" alt=""></p><p><img src="/images/calc_2.png" alt=""></p><p>使用 Gini Impurity 作为目标函数时，正确衡量（和不纯度变化一致）出 A(0.125)方案信息增益比B(0.1666)方案小。</p><p><img src="/images/calc_3.png" alt=""></p><p>使用 entropy 作为目标函数时，也正确衡量（和不纯度变化一致）出 B(0.31)方案信息增益比 A(0.19)方案大。</p><p><img src="/images/calc_5.png" alt=""></p><p>因此，</p><ul><li>信息熵对数据集的不纯度更加敏感（原因见上图 ），因此决策树的生长会更加“精细”，对于高维数据或者噪音很多的数据，很容易过拟合；</li><li>信息熵的计算比基尼系数缓慢一些，因为基尼系数的计算不涉及对数。</li></ul>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
            <tag> 评估指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>entropy VS gini</title>
      <link href="2021/01/07/entropy-vs-gini/"/>
      <url>2021/01/07/entropy-vs-gini/</url>
      
        <content type="html"><![CDATA[<h3 id="Entropy-vs-gini"><a href="#Entropy-vs-gini" class="headerlink" title="Entropy vs gini"></a>Entropy vs gini</h3><p><code>sklearn.tree.DecisionTreeClassifier(criterion=&quot;gini&quot;)</code> 中 <code>criterion</code> 参数用于指定节点上决策问题评估指标（不纯度降低量最大或信息增益最大），有两种选择: </p><ul><li><code>entropy</code>，使用信息熵(Entropy)衡量数据集包含的信息量；父节点信息熵减去子节点（左右子节点求和）信息熵作为信息增益；挑出带来信息增益最大的切分数据问题作为父节点决策问题。</li><li><code>gini</code>，使用基尼不纯度(Gini Impurity)衡量数据的不纯度；父节点不纯度减去子节点（左右子节点求和）不纯度作为不纯度降低量；挑出带来不纯度降低量最大的切分数据问题作为父节点决策问题。</li></ul><p>对比</p><p><img src="/images/overview-plot.20200107.png" alt=""></p><ul><li>信息熵对数据集的不纯度更加敏感（原因见上图 ），因此决策树的生长会更加“精细”，对于高维数据或者噪音很多的数据，很容易过拟合；</li><li>信息熵的计算比基尼系数缓慢一些，因为基尼系数的计算不涉及对数。</li></ul>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
            <tag> 评估指标 </tag>
            
            <tag> 熵 </tag>
            
            <tag> 基尼不纯度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>画决策树</title>
      <link href="2021/01/07/hua-jue-ce-shu/"/>
      <url>2021/01/07/hua-jue-ce-shu/</url>
      
        <content type="html"><![CDATA[<h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>本示例我们使用红酒数据集。</p><pre class=" language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_wine<span class="token operator">>></span><span class="token operator">></span> wine_dataset <span class="token operator">=</span> load_wine<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"数据集特征 shape："</span><span class="token punctuation">,</span> wine_dataset<span class="token punctuation">.</span>data<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>数据集特征 shape： <span class="token punctuation">(</span><span class="token number">178</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"数据集标签 shape:"</span><span class="token punctuation">,</span> wine_dataset<span class="token punctuation">.</span>target<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>数据集标签 shape<span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token number">178</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"特征名称："</span><span class="token punctuation">,</span> wine_dataset<span class="token punctuation">.</span>feature_names<span class="token punctuation">)</span>特征名称： <span class="token punctuation">[</span><span class="token string">'alcohol'</span><span class="token punctuation">,</span> <span class="token string">'malic_acid'</span><span class="token punctuation">,</span> <span class="token string">'ash'</span><span class="token punctuation">,</span> <span class="token string">'alcalinity_of_ash'</span><span class="token punctuation">,</span> <span class="token string">'magnesium'</span><span class="token punctuation">,</span> <span class="token string">'total_phenols'</span><span class="token punctuation">,</span> <span class="token string">'flavanoids'</span><span class="token punctuation">,</span> <span class="token string">'nonflavanoid_phenols'</span><span class="token punctuation">,</span> <span class="token string">'proanthocyanins'</span><span class="token punctuation">,</span> <span class="token string">'color_intensity'</span><span class="token punctuation">,</span> <span class="token string">'hue'</span><span class="token punctuation">,</span> <span class="token string">'od280/od315_of_diluted_wines'</span><span class="token punctuation">,</span> <span class="token string">'proline'</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"标签对应名称:"</span><span class="token punctuation">,</span> wine_dataset<span class="token punctuation">.</span>target_names<span class="token punctuation">)</span>标签对应名称<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'class_0'</span> <span class="token string">'class_1'</span> <span class="token string">'class_2'</span><span class="token punctuation">]</span></code></pre><p>数据切分使用 train_test_split。</p><blockquote><p>train_test_split:Split arrays or matrices into random train and test subsets</p></blockquote><pre class=" language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token comment" spellcheck="true"># 数据切分</span><span class="token operator">>></span><span class="token operator">></span> X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>wine_dataset<span class="token punctuation">.</span>data<span class="token punctuation">,</span> wine_dataset<span class="token punctuation">.</span>target<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>X_train<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> X_test<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> y_train<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> y_test<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token number">124</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">54</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">124</span><span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">54</span><span class="token punctuation">,</span><span class="token punctuation">)</span></code></pre><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><pre class=" language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>tree <span class="token keyword">import</span> DecisionTreeClassifier<span class="token operator">>></span><span class="token operator">></span> clf <span class="token operator">=</span> DecisionTreeClassifier<span class="token punctuation">(</span>random_state<span class="token operator">=</span><span class="token number">110</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>DecisionTreeClassifier<span class="token punctuation">(</span>random_state<span class="token operator">=</span><span class="token number">110</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"score on train dataset:"</span><span class="token punctuation">,</span> clf<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">)</span>score on train dataset<span class="token punctuation">:</span> <span class="token number">1.0</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"score on test dataset:"</span><span class="token punctuation">,</span> clf<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span><span class="token punctuation">)</span>score on test dataset<span class="token punctuation">:</span> <span class="token number">0.8333333333333334</span></code></pre><h3 id="画决策树"><a href="#画决策树" class="headerlink" title="画决策树"></a>画决策树</h3><p>我们使用 <code>export_graphviz</code> 生成决策树的图像到 <code>DOT</code> 格式</p><blockquote><p>export_graphviz: Export a decision tree in DOT format.</p></blockquote><pre class=" language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>tree <span class="token keyword">import</span> export_graphviz<span class="token operator">>></span><span class="token operator">></span> dot_data <span class="token operator">=</span> export_graphviz<span class="token punctuation">(</span>clf<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>dot_data<span class="token punctuation">)</span>digraph Tree <span class="token punctuation">{</span>node <span class="token punctuation">[</span>shape<span class="token operator">=</span>box<span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">0</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"X[12] &lt;= 755.0\ngini = 0.645\nsamples = 124\nvalue = [45, 52, 27]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">1</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"X[6] &lt;= 1.275\ngini = 0.454\nsamples = 74\nvalue = [2, 50, 22]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">1</span> <span class="token punctuation">[</span>labeldistance<span class="token operator">=</span><span class="token number">2.5</span><span class="token punctuation">,</span> labelangle<span class="token operator">=</span><span class="token number">45</span><span class="token punctuation">,</span> headlabel<span class="token operator">=</span><span class="token string">"True"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">2</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"X[10] &lt;= 1.005\ngini = 0.153\nsamples = 24\nvalue = [0, 2, 22]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">2</span> <span class="token punctuation">;</span><span class="token number">3</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"gini = 0.0\nsamples = 22\nvalue = [0, 0, 22]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">3</span> <span class="token punctuation">;</span><span class="token number">4</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"gini = 0.0\nsamples = 2\nvalue = [0, 2, 0]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">4</span> <span class="token punctuation">;</span><span class="token number">5</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"X[0] &lt;= 13.175\ngini = 0.077\nsamples = 50\nvalue = [2, 48, 0]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">5</span> <span class="token punctuation">;</span><span class="token number">6</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"gini = 0.0\nsamples = 44\nvalue = [0, 44, 0]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">5</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">6</span> <span class="token punctuation">;</span><span class="token number">7</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"X[1] &lt;= 2.125\ngini = 0.444\nsamples = 6\nvalue = [2, 4, 0]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">5</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">7</span> <span class="token punctuation">;</span><span class="token number">8</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"gini = 0.0\nsamples = 4\nvalue = [0, 4, 0]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">7</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">8</span> <span class="token punctuation">;</span><span class="token number">9</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"gini = 0.0\nsamples = 2\nvalue = [2, 0, 0]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">7</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">9</span> <span class="token punctuation">;</span><span class="token number">10</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"X[5] &lt;= 2.125\ngini = 0.249\nsamples = 50\nvalue = [43, 2, 5]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">10</span> <span class="token punctuation">[</span>labeldistance<span class="token operator">=</span><span class="token number">2.5</span><span class="token punctuation">,</span> labelangle<span class="token operator">=</span><span class="token operator">-</span><span class="token number">45</span><span class="token punctuation">,</span> headlabel<span class="token operator">=</span><span class="token string">"False"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">11</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"X[10] &lt;= 0.803\ngini = 0.278\nsamples = 6\nvalue = [0, 1, 5]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">10</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">11</span> <span class="token punctuation">;</span><span class="token number">12</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"gini = 0.0\nsamples = 5\nvalue = [0, 0, 5]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">11</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">12</span> <span class="token punctuation">;</span><span class="token number">13</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"gini = 0.0\nsamples = 1\nvalue = [0, 1, 0]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">11</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">13</span> <span class="token punctuation">;</span><span class="token number">14</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"X[3] &lt;= 27.5\ngini = 0.044\nsamples = 44\nvalue = [43, 1, 0]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">10</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">14</span> <span class="token punctuation">;</span><span class="token number">15</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"gini = 0.0\nsamples = 43\nvalue = [43, 0, 0]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">14</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">15</span> <span class="token punctuation">;</span><span class="token number">16</span> <span class="token punctuation">[</span>label<span class="token operator">=</span><span class="token string">"gini = 0.0\nsamples = 1\nvalue = [0, 1, 0]"</span><span class="token punctuation">]</span> <span class="token punctuation">;</span><span class="token number">14</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token number">16</span> <span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><p>使用 <code>graphviz</code> 画出决策树</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> graphvizgraph <span class="token operator">=</span> graphviz<span class="token punctuation">.</span>Source<span class="token punctuation">(</span>dot_data<span class="token punctuation">)</span>graph</code></pre><p><img src="/images/%E7%94%BB%E5%87%BA%E5%86%B3%E7%AD%96%E6%A0%91/decisionTree.default.png" alt=""></p><h3 id="调整画图参数"><a href="#调整画图参数" class="headerlink" title="调整画图参数"></a>调整画图参数</h3><p>我们调整画图参数，让决策树更易阅读</p><pre class=" language-python"><code class="language-python">feature_name <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">"酒精"</span><span class="token punctuation">,</span>    <span class="token string">"苹果酸"</span><span class="token punctuation">,</span>    <span class="token string">"灰"</span><span class="token punctuation">,</span>    <span class="token string">"灰的碱性"</span><span class="token punctuation">,</span>    <span class="token string">"镁"</span><span class="token punctuation">,</span>    <span class="token string">"总酚"</span><span class="token punctuation">,</span>    <span class="token string">"类黄酮"</span><span class="token punctuation">,</span>    <span class="token string">"非黄烷类酚类"</span><span class="token punctuation">,</span>    <span class="token string">"花青素"</span><span class="token punctuation">,</span>    <span class="token string">"颜色强度"</span><span class="token punctuation">,</span>    <span class="token string">"色调"</span><span class="token punctuation">,</span>    <span class="token string">"od280/od315稀释葡萄酒"</span><span class="token punctuation">,</span>    <span class="token string">"脯氨酸"</span><span class="token punctuation">,</span><span class="token punctuation">]</span>class_names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"琴酒"</span><span class="token punctuation">,</span> <span class="token string">"雪莉"</span><span class="token punctuation">,</span> <span class="token string">"贝尔摩德"</span><span class="token punctuation">]</span>dot_data <span class="token operator">=</span> export_graphviz<span class="token punctuation">(</span>    clf<span class="token punctuation">,</span>    feature_names<span class="token operator">=</span>feature_name<span class="token punctuation">,</span>    class_names<span class="token operator">=</span>class_names<span class="token punctuation">,</span>    filled<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    rounded<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token keyword">import</span> graphvizgraph <span class="token operator">=</span> graphviz<span class="token punctuation">.</span>Source<span class="token punctuation">(</span>dot_data<span class="token punctuation">)</span>graph</code></pre><p><img src="/images/%E7%94%BB%E5%87%BA%E5%86%B3%E7%AD%96%E6%A0%91/decisionTree.custom.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
          <category> exercises </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
            <tag> 画图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树相关习题</title>
      <link href="2021/01/07/jue-ce-shu-xiang-guan-xi-ti/"/>
      <url>2021/01/07/jue-ce-shu-xiang-guan-xi-ti/</url>
      
        <content type="html"><![CDATA[<h3 id="《机器学习》课后习题–周志华"><a href="#《机器学习》课后习题–周志华" class="headerlink" title="《机器学习》课后习题–周志华"></a>《机器学习》课后习题–周志华</h3><h4 id="4-1-试证明对于不含冲突数据-即特征向量完全相同但标记不同-的训练集，必存在与训练集一致-即训练误差为-0-的决策树。"><a href="#4-1-试证明对于不含冲突数据-即特征向量完全相同但标记不同-的训练集，必存在与训练集一致-即训练误差为-0-的决策树。" class="headerlink" title="4.1 试证明对于不含冲突数据(即特征向量完全相同但标记不同)的训练集，必存在与训练集一致(即训练误差为 0) 的决策树。"></a>4.1 试证明对于不含冲突数据(即特征向量完全相同但标记不同)的训练集，必存在与训练集一致(即训练误差为 0) 的决策树。</h4><p><strong>答：</strong>   </p><p>从原书p74的图4.2的决策树学习的基本算法可以看出，生成一个叶节点有三种情况：</p><ol><li>节点下样本 $D$ 全属于同一类样本 $C$，则将当前节点作为 $C$类叶节点。</li><li>属性集 $A=\oslash$，或者样本在当前属性集上取值相同。即特征用完了（当只剩最后一个特征时，进一步分裂，只能将各取值设立叶节点，标记为样本最多的类别。），或者的样本在 $A$ 上取值都相同（感觉这里貌似和第一条重复了）。这时取 $D$ 中最多的类作为此节点的类别标记。</li><li>在某一节点上的属性值 $a_{\ast}^{v}$ ，样本为空，即没有样本在属性 $a_{\ast}$ 上取值为 $a_{\ast}^{v}$ 。同样取 $D$ 中最多的类作为此节点的类别标记。</li></ol><p>在这道题中，目标是找出和训练集一致的决策树，所以不必考虑第3点，从1、2情况来看出决策树中树枝停止“生长”生成叶节点只会在样本属于同一类或者所有特征值都用完的时候，那么可能导致叶节点标记与实际训练集不同时只会发生在特征值都用完的情况（同一节点中的样本，其路径上的特征值都是完全相同的），而由于训练集中没有冲突数据，那每个节点上训练误差都为 0。</p><ul><li><input disabled="" type="checkbox"> 贪婪学习</li><li><input disabled="" type="checkbox"> 决策树停止生长的条件</li><li><input disabled="" type="checkbox"> 实验：对任意数据集，是否一定能生成训练集上误差为 0 的决策树？</li></ul><hr><h4 id="4-2-试析使用”最小训练误差”作为决策树划分选择准则的缺陷。"><a href="#4-2-试析使用”最小训练误差”作为决策树划分选择准则的缺陷。" class="headerlink" title="4.2 试析使用”最小训练误差”作为决策树划分选择准则的缺陷。"></a>4.2 试析使用”最小训练误差”作为决策树划分选择准则的缺陷。</h4><p><strong>答：</strong>   </p><p>这道题暂时没想出答案。在网上找了其他的答案，都是认为会造成过拟合，没给出具体证明。<br>而我的理解决策树本身就是容易过拟合的，就算使用信息增益或者基尼指数等，依旧容易过拟合，<br>至于使用“最小训练误差”会不会“更容易”过拟合暂时没理解明白。</p><ul><li><input disabled="" type="checkbox"> 决策树划分选择准则，有哪些，有什么区别及优缺点？</li><li><input disabled="" type="checkbox"> sklearn.tree.DecisionTreeRegressor 中 criterion 参数取值 <code>mse, friedman_mse, mae</code>。原理及区别</li></ul><hr><h4 id="4-3-试编程实现基于信息熵进行划分选择的决策树算法，并为表-4-3-中数据生成一棵决策树。"><a href="#4-3-试编程实现基于信息熵进行划分选择的决策树算法，并为表-4-3-中数据生成一棵决策树。" class="headerlink" title="4.3 试编程实现基于信息熵进行划分选择的决策树算法，并为表 4.3 中数据生成一棵决策树。"></a>4.3 试编程实现基于信息熵进行划分选择的决策树算法，并为表 4.3 中数据生成一棵决策树。</h4><p><strong>答：</strong>   </p><p>因为数据集的原因，数据量比较小，在选择划分属性的时候会出现特征的信息增益或者信息增益率相同的情况。<br>所有生成的决策树和书中可能不一致。并且在生成叶节点时，会出现两类数量一致的情况，这时候叶节点就随机设置一个分类了。</p><p>代码实现了以信息增益、增益率、基尼指数划分准则。下面一道题（4.4）也是用相同的代码。<br>另外画图的代码是主要参考《机器学习实战》决策树那一章画图源码。</p><p>有些地方代码有点乱，比如进行剪枝的部分就有大量重复代码；并且预剪枝部分可以在生成决策树的时候实现，减少计算量。以后有机会再优化一下。</p><p>代码在：<a href="https://github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch4--%E5%86%B3%E7%AD%96%E6%A0%91/4.3-4.4" target="_blank" rel="noopener">4.3-4.4</a></p><p>生成决策树如下：<br><img src="/images/%E5%86%B3%E7%AD%96%E6%A0%91%E7%9B%B8%E5%85%B3%E4%B9%A0%E9%A2%98/1.jpg" alt="1"></p><hr><h4 id="4-4-试编程实现基于基尼指数进行划分选择的决策树算法，为表-4-2-中数据生成预剪枝、后剪枝决策树并与未剪枝决策树进行比较"><a href="#4-4-试编程实现基于基尼指数进行划分选择的决策树算法，为表-4-2-中数据生成预剪枝、后剪枝决策树并与未剪枝决策树进行比较" class="headerlink" title="4.4 试编程实现基于基尼指数进行划分选择的决策树算法，为表 4.2 中数据生成预剪枝、后剪枝决策树并与未剪枝决策树进行比较."></a>4.4 试编程实现基于基尼指数进行划分选择的决策树算法，为表 4.2 中数据生成预剪枝、后剪枝决策树并与未剪枝决策树进行比较.</h4><p><strong>答：</strong> </p><p>代码在：<a href="https://github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch4--%E5%86%B3%E7%AD%96%E6%A0%91/4.3-4.4" target="_blank" rel="noopener">4.3-4.4</a></p><p>未剪枝、后剪枝、预剪枝生成决策树分别如下，总体来说后剪枝会相比于预剪枝保留更多的分支。</p><p>有两个需要注意的地方。一个是在4.3中说过的，因为划分属性的信息增益或者基尼指数相同的原因，<br>这个时候选择哪一个属性作为划分属性都是对的，生成决策树和书中不一致是正常的（书中第一个节点为“脐部”）。<br>另外数据量这么小的情况下，常常会出现剪枝前后准确率不变的情况，原书中也提到这种情况通常要进行剪枝的，<br>但是这道题中若进行剪枝，会出现只有一个叶节点的情况。为了画图好看点…所以都不无论在预剪枝还是后剪枝中，这种情况都会采取不剪枝策略。参考原书P82。</p><p>经过测试，在未剪枝的情况下，验证集上准确率为0.2857；后剪枝准确率为0.5714；预剪枝也为0.5714。</p><p>未剪枝：<br><img src="/images/%E5%86%B3%E7%AD%96%E6%A0%91%E7%9B%B8%E5%85%B3%E4%B9%A0%E9%A2%98/2.jpg" alt="2"><br>后剪枝：<br><img src="/images/%E5%86%B3%E7%AD%96%E6%A0%91%E7%9B%B8%E5%85%B3%E4%B9%A0%E9%A2%98/3.jpg" alt="3"><br>预剪枝：<br><img src="/images/%E5%86%B3%E7%AD%96%E6%A0%91%E7%9B%B8%E5%85%B3%E4%B9%A0%E9%A2%98/4.jpg" alt="4"></p><ul><li><input disabled="" type="checkbox"> 实验：预剪枝、后剪枝决策树并与未剪枝决策树进行比较</li></ul><hr><h4 id="4-5-试编程实现基于对率回归进行划分选择的决策树算法，并为表-4-3-中数据生成一棵决策树"><a href="#4-5-试编程实现基于对率回归进行划分选择的决策树算法，并为表-4-3-中数据生成一棵决策树" class="headerlink" title="4.5 试编程实现基于对率回归进行划分选择的决策树算法，并为表 4.3 中数据生成一棵决策树."></a>4.5 试编程实现基于对率回归进行划分选择的决策树算法，并为表 4.3 中数据生成一棵决策树.</h4><p><strong>答：</strong> </p><p><strong>这个没实现。</strong> 一种思路就是拟合对率回归后，从所有特征中选择一个 w 值最高的一个特征值，即权重最高的一个特征值作为划分选择，<br>但是没想好对于 One-hot 之后的特征权重怎么计算，比如“色泽”有三种取值“乌黑”、“青绿”、“浅白”，在One-hot之后会有三个特征，<br>那么最后“色泽”这个特征的权重应该是取平均值？以后有机会….也不填坑。</p><ul><li><input disabled="" type="checkbox"> 对率回归</li></ul><hr><h4 id="4-6-试选择-4-个-UCI-数据集，对上述-3-种算法所产生的未剪枝、预剪枝、后剪枝决策树进行实验比较，并进行适当的统计显著性检验"><a href="#4-6-试选择-4-个-UCI-数据集，对上述-3-种算法所产生的未剪枝、预剪枝、后剪枝决策树进行实验比较，并进行适当的统计显著性检验" class="headerlink" title="4.6 试选择 4 个 UCI 数据集，对上述 3 种算法所产生的未剪枝、预剪枝、后剪枝决策树进行实验比较，并进行适当的统计显著性检验."></a>4.6 试选择 4 个 UCI 数据集，对上述 3 种算法所产生的未剪枝、预剪枝、后剪枝决策树进行实验比较，并进行适当的统计显著性检验.</h4><p><strong>答：</strong> </p><p>只拿sklearn中自带的iris数据集试了一下剪枝后的准确率，发现不同随机数种子（使得数据集划分不同）导致最后验证集的准确率变化挺大。</p><p><strong>统计显著性检验没实现。</strong><br><a href="https://github.com/han1057578619/MachineLearning_Zhouzhihua_ProblemSets/tree/master/ch4--%E5%86%B3%E7%AD%96%E6%A0%91/4.6" target="_blank" rel="noopener">代码在这</a></p><ul><li><input disabled="" type="checkbox"> 统计显著性检验</li></ul><hr><h4 id="4-7-图-4-2-是一个递归算法，若面临巨量数据，则决策树的层数会很深，使用递归方法易导致”栈”溢出。试使用”队列”数据结构，以参数-MaxDepth-控制树的最大深度，写出与图-4-2-等价、但不使用递归的决策树生成算法"><a href="#4-7-图-4-2-是一个递归算法，若面临巨量数据，则决策树的层数会很深，使用递归方法易导致”栈”溢出。试使用”队列”数据结构，以参数-MaxDepth-控制树的最大深度，写出与图-4-2-等价、但不使用递归的决策树生成算法" class="headerlink" title="4.7 图 4.2 是一个递归算法，若面临巨量数据，则决策树的层数会很深，使用递归方法易导致”栈”溢出。试使用”队列”数据结构，以参数 MaxDepth 控制树的最大深度，写出与图 4.2 等价、但不使用递归的决策树生成算法."></a>4.7 图 4.2 是一个递归算法，若面临巨量数据，则决策树的层数会很深，使用递归方法易导致”栈”溢出。试使用”队列”数据结构，以参数 MaxDepth 控制树的最大深度，写出与图 4.2 等价、但不使用递归的决策树生成算法.</h4><p><strong>答：</strong> </p><p>主要思路每一次循环遍历一层下节点(除去叶节点)，为每一个节点生成子节点，将非叶节点入队；<br>用参数L保存每一层有多少个节点。下一次循环执行同样的步骤。直至所有的节点都叶节点，此时队列为空。具体如下：</p><pre class=" language-human"><code class="language-human">输入：训练集D = {(x1, y1), (x2, y2)...(xm, ym)};      属性集A = {a1, a2... ad};      最大深度MaxDepth = maxDepth过程：函数TreeDenerate(D, A, maxDepth)1:  生成三个队列，NodeQueue、DataQueue、AQueue分别保存节点、数据、和剩余属性集;2:  生成节点Node_root;3:  if A为空 OR D上样本都为同一类别:4:      将Node_root标记为叶节点，其标记类别为D中样本最多的类;5:      return Node_root;6:  end if 7:  将Node入队NodeQueue; 将D入队 DataQueue; 将A入队AQueue;8:  初始化深度depth=0;9:  初始化L = 1;    # L用于记录每一层有多少非叶节点。10: while NodeQueue 非空:11:     L* = 012:     for _ in range(L):            # 遍历当前L个非叶节点13:         NodeQueue 出队Node; DataQueue出队D; AQueue 出队A;14:         从A中选择一个最优划分属性a*;15:         for a* 的每一个值 a*v do:16:             新建一个node*，并将node*连接为Node的一个分支;17:             令 Dv表示为D中在a*上取值为a*v的样本子集;18:             if Dv为空:19:                 将node*标记为叶节点，其标记类别为D中样本最多的类;20:                 continue;21:             end if22:             if A\{a*}为空 OR Dv上样本都为同一类别 OR depth == maxDepth:23:                 将node*标记为叶节点，其标记类别为Dv中样本最多的类;24:                 continue;25:             end if             26:             将node*入队NodeQueue; 将Dv入队 DataQueue; 将A\{a*} 入队AQueue;27:             L* += 1;        # 用于计算在第depth+1 层有多少个非叶节点28:     L = L*;29:     depth += 1;30: return Node_root;输入以Node_root为根节点的一颗决策树</code></pre><ul><li><input disabled="" type="checkbox"> 使用”队列”数据结构, 不使用递归的决策树生成算法。</li></ul><hr><h4 id="4-8-试将决策树生成的深度优先搜索过程修改为广度优先搜索，以参数MaxNode-控制树的最大结点数，将题-4-7-中基于队列的决策树算法进行改写。对比题-4-7-中的算法，试析哪种方式更易于控制决策树所需存储不超出内存。"><a href="#4-8-试将决策树生成的深度优先搜索过程修改为广度优先搜索，以参数MaxNode-控制树的最大结点数，将题-4-7-中基于队列的决策树算法进行改写。对比题-4-7-中的算法，试析哪种方式更易于控制决策树所需存储不超出内存。" class="headerlink" title="4.8 试将决策树生成的深度优先搜索过程修改为广度优先搜索，以参数MaxNode 控制树的最大结点数，将题 4.7 中基于队列的决策树算法进行改写。对比题 4.7 中的算法，试析哪种方式更易于控制决策树所需存储不超出内存。"></a>4.8 试将决策树生成的深度优先搜索过程修改为广度优先搜索，以参数MaxNode 控制树的最大结点数，将题 4.7 中基于队列的决策树算法进行改写。对比题 4.7 中的算法，试析哪种方式更易于控制决策树所需存储不超出内存。</h4><p><strong>答：</strong> </p><p>4.7写的算法就是广度优先搜索的。这道题将MaxNode改为MaxDepth，只需要改几个地方。<br>有一点需要注意的地方，就是在给一个节点生成子节点时（19-32行），可能造成节点数大于最大值的情况，<br>比如某属性下有3种取值，那么至少要生成3个叶节点，这个时候节点总数可能会超过最大值，这时最终节点数可能会是MaxNode+2。</p><p>至于两种算法对比。个人理解当数据特征值，各属性的取值较多时，形成的决策树会趋于较宽类型的树，<br>这时使用广度优先搜索更容易控制内存。若属性取值较少时，深度优先搜索更容易控制内存。</p><p>对4.7中修改如下：</p><pre><code>输入：训练集D = {(x1, y1), (x2, y2)...(xm, ym)};      属性集A = {a1, a2... ad};      最大深度MaxNode = maxNode过程：函数TreeDenerate(D, A, maxNode)1:  生成三个队列，NodeQueue、DataQueue、AQueue分别保存节点、数据、和剩余属性集;2:  生成节点Node_root;3:  if A为空 OR D上样本都为同一类别:4:      将Node_root标记为叶节点，其标记类别为D中样本最多的类;5:      return Node_root;6:  end if 7:  将Node入队NodeQueue; 将D入队 DataQueue; 将A入队AQueue;8:  初始化深度numNode=1;9:  初始化L = 1;    # L用于记录每一层有多少非叶节点。10: while NodeQueue 非空:11:     L* = 012:     for _ in range(L):            # 遍历当前L个非叶节点13:         NodeQueue 出队Node; DataQueue出队D; AQueue 出队A;14:         if numNode &gt;= maxNode:15:             将Node标记为叶节点，其标记类别为D中样本最多的类;16:             continue;17:         end if;18:         从A中选择一个最优划分属性a*;19:         for a* 的每一个值 a*v do:20:             numNode+=121:             生成一个node*，并将node*连接为Node的一个分支;22:             令 Dv表示为D中在a*上取值为a*v的样本子集;23:             if Dv为空:24:                 将node*标记为叶节点，其标记类别为D中样本最多的类;25:                 continue;26:             end if27:             if A\{a*}为空 OR Dv上样本都为同一类别:28:                 将node*标记为叶节点，其标记类别为Dv中样本最多的类;29:                 continue;30:             end if             31:             将node*入队NodeQueue; 将Dv入队 DataQueue; 将A\{a*} 入队AQueue;32:             L* += 1;        # 用于计算在第depth+1 层有多少个非叶节点33:         end if;34:     L = L*;35: return Node_root;</code></pre><ul><li><input disabled="" type="checkbox"> 深度优先搜索和广度优先搜索决策树生成算法。对比分析</li></ul><hr><h4 id="4-9-试将-4-4-2-节对缺失值的处理机制推广到基尼指数的计算中去"><a href="#4-9-试将-4-4-2-节对缺失值的处理机制推广到基尼指数的计算中去" class="headerlink" title="4.9 试将 4.4.2 节对缺失值的处理机制推广到基尼指数的计算中去."></a>4.9 试将 4.4.2 节对缺失值的处理机制推广到基尼指数的计算中去.</h4><p><strong>答：</strong> </p><p>这道题相对简单。使用书中式4.9、4.10、4.11有，对于原书中4.5式可以推广为：</p><p>$Gini(D) =1-\sum_{k=1}^{\left| y \right|}\tilde{p_{k}}^{2}$ ，属性a的基尼指数可推广为：</p><p>$Gini_index(D, a)=p\times Gini_index(\tilde{D}, a) =p\times\sum_{v=1}^{V}\tilde{v}Gini(D^{v})$ 。</p><hr><h4 id="4-10-从网上下载或自己编程实现任意一种多变量决策树算法，并观察其在西瓜数据集-3-0-上产生的结果"><a href="#4-10-从网上下载或自己编程实现任意一种多变量决策树算法，并观察其在西瓜数据集-3-0-上产生的结果" class="headerlink" title="4.10 从网上下载或自己编程实现任意一种多变量决策树算法，并观察其在西瓜数据集 3.0 上产生的结果"></a>4.10 从网上下载或自己编程实现任意一种多变量决策树算法，并观察其在西瓜数据集 3.0 上产生的结果</h4><p><strong>答：</strong> </p><p>待补充。</p><ul><li><input disabled="" type="checkbox"> 多变量决策树算法</li></ul>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
          <category> exercises </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
            <tag> 树模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于决策树你该知道哪些?</title>
      <link href="2021/01/06/guan-yu-jue-ce-shu-ni-gai-zhi-dao-na-xie/"/>
      <url>2021/01/06/guan-yu-jue-ce-shu-ni-gai-zhi-dao-na-xie/</url>
      
        <content type="html"><![CDATA[<h3 id="工具和技巧"><a href="#工具和技巧" class="headerlink" title="工具和技巧"></a>工具和技巧</h3><ol><li>graphviz 画出决策树</li><li>超参数的学习曲线：以超参数的取值为横坐标，模型的度量指标为纵坐标的曲线，它是用来衡量不同超参数取值下模型的表现的线。</li></ol><h3 id="实验："><a href="#实验：" class="headerlink" title="实验："></a>实验：</h3><ol><li>各个超参数学习曲线：<ol><li>criterion: gini, entropy；</li><li>max_depth</li></ol></li></ol><p><img src="/images/decision.tree.max_depth.png" alt=""></p><h3 id="重点："><a href="#重点：" class="headerlink" title="重点："></a>重点：</h3><ol><li>原理<ol><li>是什么？</li><li>工作流程</li><li>公式：熵(entropy), gini, info_gain, mse, mae,r2</li></ol></li><li>减少过拟合</li><li>重要参数，以及调参经验</li><li>优缺点</li><li>习题</li></ol><h3 id="知识点："><a href="#知识点：" class="headerlink" title="知识点："></a>知识点：</h3><ol><li>决策树介绍</li><li>sklearn 中分类决策树重要参数<ol><li>不纯度指标：<ol><li>criterion： gini, entropy；怎样选取参数？<ol><li>维度低，数据比较清晰的时候，信息熵和基尼系数没区别；</li><li>当决策树的拟合程度不够的时候，使用信息熵；</li></ol></li></ol></li><li>随机性：<ol><li>random_state：用来设置分枝中的随机模式的参数。<ol><li>集成算法：一棵树不能保证最优，那就建更多的不同的树，然后从中取最好的。</li><li>怎样从一组数据集中建不同的树？在每次分枝时，不使用全部特征，而是随机选取一部分特征，从中选取不纯度相关指标最优的作为分枝用的节点。这样，每次生成的树也就不同了。</li></ol></li><li>splitter也是用来控制决策树中的随机选项的，有两种输入值，输入”best”，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看），输入“random”，决策树在分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合</li></ol></li><li>剪枝参数：不加限制的情况下，一棵决策树会生长到衡量不纯度的指标最优，或者没有更多的特征可用为止。这样的决策树往往会过拟合。<ol><li>max_depth: 限制树的最大深度，超过设定深度的树枝全部剪掉</li><li>min_samples_leaf 限定，一个节点在分枝后的每个子节点都必须包含至少 min_samples_leaf个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。</li><li>min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生。 </li></ol></li><li>精修：max_features &amp; min_impurity_decrease 一般max_depth使用，用作树的”精修“<ol><li>max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。</li><li>min_impurity_decrease 限制信息增益的大小，信息增益小于设定数值的分枝不会发生。</li></ol></li><li>目标权重参数：class_weight &amp; min_weight_fraction_leaf 完成样本标签平衡的参数。<ol><li>class_weight：参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。</li><li>min_weight_fraction_leaf：这个基于权重的剪枝参数来使用。</li></ol></li></ol></li><li>sklearn 中回归决策树重要参数<ol><li>criterion：回归树衡量分枝质量的指标，支持的标准有三种：mse, friedman_mse, mae<ol><li>mse：均方误差mean squared error<ol><li></li><li>父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准</li><li>这种方法通过使用叶子节点的均值来最小化L2损失</li></ol></li><li>friedman_mse：费尔德曼均方误差。<ol><li>使用弗里德曼针对潜在分枝中的问题改进后的均方误差</li></ol></li><li>mae：绝对平均误差 mean absolute error。<ol><li>这种指标使用叶节点的中值来最小化L1损失</li></ol></li><li>另外，回归树的接口 score 返回的是R平方，并不是MSE。<br> 1. </li></ol></li></ol></li><li>交叉验证：用来观察模型的稳定性的一种方法。<ol><li>我们将数据划分为n份，依次使用其中一份作为测试集，其他n-1份作为训练集，多次计算模型的精确性来评估模型的平均准确程度。</li><li>训练集和测试集的划分会干扰模型的结果，因此用交叉验证n次的结果求出的平均值，是对模型效果的一个更好的度量。</li></ol></li><li>决策树优点:<ol><li>易于理解和解释，因为树木可以画出来被看见</li><li>需要很少的数据准备。其他很多算法通常都需要数据规范化，需要创建虚拟变量并删除空值等。但请注意，sklearn中的决策树模块不支持对缺失值的处理。</li><li>使用树的成本（比如说，在预测数据的时候）是用于训练树的数据点的数量的对数，相比于其他算法，这是一个很低的成本。</li><li>能够同时处理数字和分类数据，既可以做回归又可以做分类。其他技术通常专门用于分析仅具有一种变量类型的数据集。</li><li>能够处理多输出问题，即含有多个标签的问题，注意与一个标签中含有多种标签分类的问题区别开</li><li>是一个白盒模型，结果很容易能够被解释。如果在模型中可以观察到给定的情况，则可以通过布尔逻辑轻松解释条件。相反，在黑盒模型中（例如，在人工神经网络中），结果可能更难以解释。</li><li>可以使用统计测试验证模型，这让我们可以考虑模型的可靠性。</li><li>即使其假设在某种程度上违反了生成数据的真实模型，也能够表现良好。</li></ol></li><li>决策树的缺点<ol><li>决策树学习者可能创建过于复杂的树，这些树不能很好地推广数据。这称为过度拟合。修剪，设置叶节点所需的最小样本数或设置树的最大深度等机制是避免此问题所必需的，而这些参数的整合和调整对初学者来说会比较晦涩</li><li>决策树可能不稳定，数据中微小的变化可能导致生成完全不同的树，这个问题需要通过集成算法来解决。</li><li>决策树的学习是基于贪婪算法，它靠优化局部最优（每个节点的最优）来试图达到整体的最优，但这种做法不能保证返回全局最优决策树。这个问题也可以由集成算法来解决，在随机森林中，特征和样本会在分枝过程中被随机采样。</li><li>有些概念很难学习，因为决策树不容易表达它们，例如 XOR，奇偶校验或多路复用器问题。</li><li>如果标签中的某些类占主导地位，决策树学习者会创建偏向主导类的树。因此，建议在拟合决策树之前平衡数据集。 </li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
            <tag> 树模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>add machines to MASS</title>
      <link href="2020/05/04/add-machines-to-mass/"/>
      <url>2020/05/04/add-machines-to-mass/</url>
      
        <content type="html"><![CDATA[<p>There are two ways to add a machine to MAAS.</p><ul><li>If you place the machine on a connected network, and the machine is configured to netboot, MAAS will enlist it from there. </li><li>If you add a machine manually, MAAS will automatically commission it. </li></ul><h3 id="How-enlistment-works"><a href="#How-enlistment-works" class="headerlink" title="How enlistment works"></a>How enlistment works</h3><p>When MAAS enlists a machine, the first step is to contact the DHCP server, so that the machine can be assigned an IP address, which is necessary to download a kernel and initrd via TFTP.</p><p>Next, initrd mounts a Squashfs image, ephemerally, via HTTP, so that cloud-init can execute.</p><p>Finally, cloud-init runs enlistment and setup scripts.</p><blockquote><p>The enlistment scripts send the region API server information about the machine, including the architecture, MAC address and other details. The API server, in turn, stores these details in the database. This information-gathering process is known as automatic discovery.</p></blockquote><p>After the enlistment process, the MAAS places the machine in the ‘Ready’ state.</p><p>Typically, the next step will be to commission the machine. </p><h3 id="Add-a-machine-manually"><a href="#Add-a-machine-manually" class="headerlink" title="Add a machine manually"></a>Add a machine manually</h3><p>Enlistment can be done manually if the hardware specifications of the underlying machine are known.</p><p>On the ‘Machines’ page of the web UI, click the ‘Add hardware’ button and then select ‘Machine’.</p><p>Fill in the form and hit ‘Save machine’.</p><p>Normally, when you add a machine manually, MAAS will immediately attempt to commission the machine.</p><blockquote><p> Note that you will need to configure the underlying machine to boot over the network, or commissioning will not happen. MAAS cannot handle this configuration for you. </p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> MASS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>concepts and terms of MASS</title>
      <link href="2020/05/04/concepts-and-terms-of-mass/"/>
      <url>2020/05/04/concepts-and-terms-of-mass/</url>
      
        <content type="html"><![CDATA[<p>Concepts and terms of MASS</p><h3 id="Nodes"><a href="#Nodes" class="headerlink" title="Nodes"></a>Nodes</h3><p>A node is a general term that refers to multiple, more specific objects. </p><p>Nodes include:</p><ul><li>Controllers</li><li>Machines</li><li>Devices</li></ul><p>node’s life cycle:</p><h4 id="Controllers"><a href="#Controllers" class="headerlink" title="Controllers"></a>Controllers</h4><h4 id="Machines"><a href="#Machines" class="headerlink" title="Machines"></a>Machines</h4><p>A machine is a node that can be deployed by MAAS.</p><h4 id="Devices"><a href="#Devices" class="headerlink" title="Devices"></a>Devices</h4><p>A device is a non-deployable node. </p><p>Static or dynamic IP addresses and DNS names can be assigned to any device or parent node. </p><h3 id="Pods"><a href="#Pods" class="headerlink" title="Pods"></a>Pods</h3><p>Pods, also called composable hardware, allow for the dynamic composition of machines from a pool of available hardware resources (e.g. disk space, memory, cores).</p><h3 id="Zones"><a href="#Zones" class="headerlink" title="Zones"></a>Zones</h3><p>A physical zone, or just zone, is an organisational unit that contains nodes where each node is in one, and only one, zone.</p><h3 id="Regions"><a href="#Regions" class="headerlink" title="Regions"></a>Regions</h3><p>A region is an organisational unit one level above a zone.</p><h3 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h3><p>A series is essentially an operating system version.</p><h3 id="Images"><a href="#Images" class="headerlink" title="Images"></a>Images</h3><p>An image is used to provision a machine. As soon as you install MAAS, images are imported based on what series you have selected. MAAS won’t work until it has imported the necessary images.</p><h3 id="Fabrics"><a href="#Fabrics" class="headerlink" title="Fabrics"></a>Fabrics</h3><p>A <strong>fabric</strong> connects VLANs.</p><p>Consequently, it would be impossible for two VLANs to communicate with each other. A fabric makes these VLAN-to-VLAN connections possible.</p><p>You could describe a fabric as a VLAN namespace. It’s a switch or a combination of switches that use trunking to provide access to specific VLANs. </p><p>The following conceptual diagram shows two fabrics in the same data centre or region, each using distinct VLAN ranges and their associated subnets:</p><p><img src="/images/46177305128bf7f3190f8a7bbd037c33e96f6a9e.png" alt=""></p><h3 id="Spaces"><a href="#Spaces" class="headerlink" title="Spaces"></a>Spaces</h3><p>A space is a logical grouping of subnets that can communicate with one another. </p><p>Spaces can be arranged to group subnets according to various parameters. </p><p>One of the most common examples is a DMZ space, which might group subnets presenting a web interface to the public Internet. Behind this DMZ would be specific applications that aren’t allowed to interact directly with the user, but instead must interact with a Web UI in the DMZ space.</p><h3 id="Tags"><a href="#Tags" class="headerlink" title="Tags"></a>Tags</h3><p>A tag (not to be confused with VLAN tags) is user-created and associated with nodes based on their physical properties.</p><h3 id="Subnets"><a href="#Subnets" class="headerlink" title="Subnets"></a>Subnets</h3><p>A subnet is a “layer 3” network, defined by a network address and a network mask length (in bits) and usually written in “CIDR” format. MAAS supports IPv4 and IPv6 subnets. Examples include:</p><pre class=" language-auto"><code class="language-auto">10.0.0.0/8172.16.0.0/12192.168.0.0/162001:db8:4d41:4153::/64</code></pre><h3 id="VLANs"><a href="#VLANs" class="headerlink" title="VLANs"></a>VLANs</h3><p>VLANs (Virtual LANs) are a common way to create logically separate networks using the same physical infrastructure.</p><h3 id="DHCP-relay"><a href="#DHCP-relay" class="headerlink" title="DHCP relay"></a>DHCP relay</h3><p>A DHCP relay, or relay agent, is a network device that forwards requests and replies between a DHCP client and a DHCP server when both are not on the same physical subnet.</p><h3 id="Interfaces"><a href="#Interfaces" class="headerlink" title="Interfaces"></a>Interfaces</h3><h4 id="Physical"><a href="#Physical" class="headerlink" title="Physical"></a>Physical</h4><p>After a node is commissioned, MAAS discovers its physical interfaces.</p><p>MAAS always creates a device with at least one physical interface.</p><p>Before deployment, a MAAS administrator can configure additional interfaces on the node, including one or more of the types mentioned below.</p><h4 id="Bond"><a href="#Bond" class="headerlink" title="Bond"></a>Bond</h4><p>A bond interface is capable of aggregating two or more physical interfaces into a single logical interface. </p><h4 id="VLAN"><a href="#VLAN" class="headerlink" title="VLAN"></a>VLAN</h4><p>A VLAN interface can be used to connect to a tagged VLAN, if the node is connected to an authorised port.</p><h3 id="Machine-actions"><a href="#Machine-actions" class="headerlink" title="Machine actions"></a>Machine actions</h3><p>Machine actions are essentially “things you can do with nodes”.</p><p>You can trigger them via the web UI or the MAAS CLI.</p><h4 id="Abort"><a href="#Abort" class="headerlink" title="Abort"></a>Abort</h4><p>You can abort any action that permits retries. Currently, only commissioning and deployment permit retries.</p><h4 id="Acquire"><a href="#Acquire" class="headerlink" title="Acquire"></a>Acquire</h4><p>Allocates (reserves) a node to the MAAS user performing the action (and currently logged in). </p><p>Changes a node’s status from ‘Ready’ to ‘Allocated’.</p><h4 id="Commission"><a href="#Commission" class="headerlink" title="Commission"></a>Commission</h4><p>This action commissions a node, changing a node’s status from ‘New’ to ‘Commissioning’ to ‘Ready’.</p><p>Commissioning enables MAAS to build a detailed inventory of RAM, CPU, storage, NICs and accelerators like GPUs. </p><h4 id="Delete"><a href="#Delete" class="headerlink" title="Delete"></a>Delete</h4><p>This action removes a node from MAAS</p><h4 id="Deploy"><a href="#Deploy" class="headerlink" title="Deploy"></a>Deploy</h4><p>This action, which includes ‘Power on,’ deploys a node, changing a node’s status from ‘Ready’ (or ‘Allocated’) to a deployed status.</p><h4 id="Exit-rescue-mode"><a href="#Exit-rescue-mode" class="headerlink" title="Exit rescue mode"></a>Exit rescue mode</h4><p>This action changes a node’s status from ‘Rescue mode’ to the ‘Exiting rescue mode’ transitory status and then back to its original status when the operation is complete.</p><h3 id="Node-statuses"><a href="#Node-statuses" class="headerlink" title="Node statuses"></a>Node statuses</h3><h4 id="Allocated"><a href="#Allocated" class="headerlink" title="Allocated"></a>Allocated</h4><p>The node is allocated (reserved) to a MAAS user. See node action ‘Acquire’.</p><h4 id="Broken"><a href="#Broken" class="headerlink" title="Broken"></a>Broken</h4><p>The node is broken. See node action ‘Mark broken’.</p><h4 id="Commissioning"><a href="#Commissioning" class="headerlink" title="Commissioning"></a>Commissioning</h4><p>The node is in the process of commissioning. See node action ‘Commission’.</p><h4 id="Deployed"><a href="#Deployed" class="headerlink" title="Deployed"></a>Deployed</h4><p>The node is deployed. See node action ‘Deploy’.</p><p>The visible status will be the name of the chosen OS (e.g. ‘Ubuntu 16.04 LTS’).</p><h4 id="Deploying"><a href="#Deploying" class="headerlink" title="Deploying"></a>Deploying</h4><p>The node is in the process of deploying. See node action ‘Deploy’.</p><p>The visible status will be Deploying to ‘OS’, where ‘OS’ is the name of the OS being deployed (e.g. ‘Deploying to Ubuntu 16.04 LTS’).</p><h4 id="Entering-rescue-mode"><a href="#Entering-rescue-mode" class="headerlink" title="Entering rescue mode"></a>Entering rescue mode</h4><p>The node is in the process of entering rescue mode. See node action ‘Rescue mode’.</p><h4 id="Exiting-rescue-mode"><a href="#Exiting-rescue-mode" class="headerlink" title="Exiting rescue mode"></a>Exiting rescue mode</h4><p>The node is in the process of exiting rescue mode. See node action ‘Exit rescue mode’.</p><h4 id="Failed-Commissioning"><a href="#Failed-Commissioning" class="headerlink" title="Failed Commissioning"></a>Failed Commissioning</h4><p>The node failed to commission.</p><h4 id="Failed-Deployment"><a href="#Failed-Deployment" class="headerlink" title="Failed Deployment"></a>Failed Deployment</h4><p>The node failed to deploy.</p><h4 id="Locked"><a href="#Locked" class="headerlink" title="Locked"></a>Locked</h4><p>It’s not strictly a status, but a machine showing a ‘padlock’ symbol adjacent to its name is in a locked state.</p><h4 id="New"><a href="#New" class="headerlink" title="New"></a>New</h4><p>This status represents the first stage of a node’s life in MAAS. Typically, a node with this status has just been added to MAAS.</p><h4 id="Ready"><a href="#Ready" class="headerlink" title="Ready"></a>Ready</h4><p>A node bearing this status has been commissioned and is ready for use, including the necessary BMC credentials. MAAS can start or stop this machine, and allocate or (re)deploy it with a fresh operating system.</p><h4 id="Rescue-mode"><a href="#Rescue-mode" class="headerlink" title="Rescue mode"></a>Rescue mode</h4><p>The node is in rescue mode and is ready to accept SSH connections. See node action ‘Rescue mode’.</p><h3 id="Package-repositories"><a href="#Package-repositories" class="headerlink" title="Package repositories"></a>Package repositories</h3><p>Package repositories managed within MAAS can be of two types:</p><ul><li>Ubuntu package repositories</li><li>Personal Package Archives (PPA)</li></ul><h4 id="Personal-Package-Archives-PPA"><a href="#Personal-Package-Archives-PPA" class="headerlink" title="Personal Package Archives (PPA)"></a>Personal Package Archives (PPA)</h4><p>A Personal Package Archive (PPA) is a <a href="https://launchpad.net/" target="_blank" rel="noopener">Launchpad</a>-based method for any individual (or team) to build and distribute packages for Ubuntu.</p><h3 id="Brief-network-tutorial"><a href="#Brief-network-tutorial" class="headerlink" title="Brief network tutorial"></a>Brief network tutorial</h3><p>The following is a brief network tutorial, provided as a tool to synchronize understanding.</p><h4 id="DHCP"><a href="#DHCP" class="headerlink" title="DHCP"></a>DHCP</h4><p>The Dynamic Host Control Protocol is a network management system in which a server (or group of servers) dynamically assigns IP addresses and other network parameters to a network device. </p><p>DHCP operates using the “DORA” model: Discovery, Offer, Request, and Acknowledge:</p><ol><li>Potential DHCP clients broadcast a DHCPDISCOVER message on its attached subnet using destination address 255.255.255.255.</li><li>connected DHCP server receives the DHCPDISCOVER message and sends a DHCPOFFER message, containing an IP address that the client may use.</li><li>The client replies with a DHCPREQUEST message, requesting the offered address.</li><li>The DHCP server responds with a DHCPACK (acknowledgement) which includes various important configuration parameters, such as the lease duration.</li></ol><p>(Of course, there is <a href="https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol" target="_blank" rel="noopener">much more to DHCP</a>, but what’s covered here should be sufficient understanding for using MAAS.)</p><h4 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h4><p>In the client/server age, the lines between client and server are blurred and sometimes reversible.</p><p> For the purposes of MAAS and general networking principles, we can define a “client” as a node that uses shared resources via a network. If that same client provides shared resources to other nodes, it could also be considered a server.</p><h3 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h3><p>A server is a node that provides shared resources to clients via a network. </p><p> If that same server uses shared resources from other nodes, it could also be considered a client, but only in that context.</p><h4 id="Network-interface"><a href="#Network-interface" class="headerlink" title="Network interface"></a>Network interface</h4><p>A network interface, often referred to as a “network interface card” or NIC, is either a separate physical card connected to a node, a set of circuits embedded on a node’s motherboard, or a radio transceiver attached to a node in some way.</p><p>All network connections require a NIC. The terms “port” and “adapter” are also used to refer to a network interface.</p><h4 id="MAC-address"><a href="#MAC-address" class="headerlink" title="MAC address"></a>MAC address</h4><p>A MAC or “media access control” address is a unique address or “physical address” associated with a network interface. </p><p>Every computer in the world theoretically has a unique MAC address. You can identify a node’s IP address with the command <code>ipconfig /all</code>.</p><h4 id="Network-cable"><a href="#Network-cable" class="headerlink" title="Network cable"></a>Network cable</h4><p>Network cables are special cables that connect non-wireless-based nodes.</p><h4 id="Packet"><a href="#Packet" class="headerlink" title="Packet"></a>Packet</h4><p>A packet is a unit of network traffic. It may or may not represent a complete message.</p><h4 id="Repeater"><a href="#Repeater" class="headerlink" title="Repeater"></a>Repeater</h4><p>Technically, a repeater is a network signal amplifier with two RJ45 connectors which adds one maximum length (for the cable type) to the network connection or “run.”</p><h4 id="Hub"><a href="#Hub" class="headerlink" title="Hub"></a>Hub</h4><p>Hubs essentially started as repeaters. While they may be able to connect more than two computers together</p><h4 id="Switch"><a href="#Switch" class="headerlink" title="Switch"></a>Switch</h4><p>A switch is a “smart” device that connects cables from nodes to make networks. </p><h4 id="Network-topology"><a href="#Network-topology" class="headerlink" title="Network topology"></a>Network topology</h4><p>Topology describes how nodes are connected to a network, specifically referring to the shapes made by the cables and the paths that packets can take. </p><h4 id="Patch-panel"><a href="#Patch-panel" class="headerlink" title="Patch panel"></a>Patch panel</h4><p>A patch panel is simply a 24- to 48-port panel of connectors that can link together three- to ten-foot cables</p><h4 id="LAN"><a href="#LAN" class="headerlink" title="LAN"></a>LAN</h4><p>Besides topology, networks can also be classified by their size, range, or “reach.” One such classification is the Local Area Network (LAN), which connects computers in close proximity (about 300 feet).</p><h4 id="WAN"><a href="#WAN" class="headerlink" title="WAN"></a>WAN</h4><p>A WAN (wide area network) is a network which connects LANs across large geographic distances, e.g., thousands of miles.</p><h4 id="MAN"><a href="#MAN" class="headerlink" title="MAN"></a>MAN</h4><p>A metro area network or MAN connects LANs over a smaller area, like a city or urban footprint. Basically, if it isn’t really a WAN, but you can’t connect it with cables, it’s usually considered a MAN.</p>]]></content>
      
      
      
        <tags>
            
            <tag> MASS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is Juju?</title>
      <link href="2020/05/03/what-is-juju/"/>
      <url>2020/05/03/what-is-juju/</url>
      
        <content type="html"><![CDATA[<p>From <a href="https://juju.is/docs" target="_blank" rel="noopener">https://juju.is/docs</a></p><hr><p>Juju is an open source application modeling tool. It allows you to deploy, configure, scale and operate your software on public and private clouds.</p><h3 id="Clouds"><a href="#Clouds" class="headerlink" title="Clouds"></a>Clouds</h3><blockquote><p>MAAS manages the machines and Juju manages the services running on those machines. – From <a href="https://maas.io/docs" target="_blank" rel="noopener">https://maas.io/docs</a></p></blockquote><p>Juju supports  a wide variety of clouds:(cloud manages the machines,Juju manages the services)</p><ul><li><a href="https://juju.is/docs/aws-cloud" target="_blank" rel="noopener">Amazon AWS</a> *</li><li><a href="https://juju.is/docs/azure-cloud" target="_blank" rel="noopener">Microsoft Azure</a> *</li><li><a href="https://juju.is/docs/gce-cloud" target="_blank" rel="noopener">Google GCE</a> *</li><li><a href="https://juju.is/docs/oci-cloud" target="_blank" rel="noopener">Oracle</a> *</li><li><a href="https://juju.is/docs/rackspace-cloud" target="_blank" rel="noopener">Rackspace</a> *</li><li><a href="https://juju.is/docs/lxd-cloud" target="_blank" rel="noopener">LXD</a> (local) *</li><li><a href="https://juju.is/docs/lxd-cloud" target="_blank" rel="noopener">LXD</a> (remote)</li><li><a href="https://juju.is/docs/k8s-cloud" target="_blank" rel="noopener">Kubernetes</a></li><li><a href="https://juju.is/docs/vsphere-cloud" target="_blank" rel="noopener">VMware vSphere</a></li><li><a href="https://juju.is/docs/openstack-cloud" target="_blank" rel="noopener">OpenStack</a></li><li><a href="https://juju.is/docs/maas-cloud" target="_blank" rel="noopener">MAAS</a></li><li><a href="https://juju.is/docs/manual-cloud" target="_blank" rel="noopener">Manual</a></li></ul><p>The exception is for a local LXD cloud; credentials are added automatically.</p><h3 id="Setting-up"><a href="#Setting-up" class="headerlink" title="Setting up"></a>Setting up</h3><p>You need to install snap, which will allow you to install other software.</p><p>Install Juju locally with snap:</p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> snap <span class="token function">install</span> juju --classic</code></pre><h3 id="Create-a-cloud-on-localhost"><a href="#Create-a-cloud-on-localhost" class="headerlink" title="Create a cloud on localhost"></a>Create a cloud on localhost</h3><p>LXD manages operating system containers and makes the the “localhost” cloud available.</p><blockquote><p>LXD is a next generation system container manager. It offers a user experience similar to virtual machines but using Linux containers instead.<br>It’s image based with pre-made images available for a wide number of Linux distributions and is built around a very powerful, yet pretty simple, REST API.<br>— <a href="https://linuxcontainers.org/lxd/" target="_blank" rel="noopener">LXD website</a></p></blockquote><p>Install LXD with snap:</p><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> snap <span class="token function">install</span> lxd</code></pre><p>LXD needs to be configured for its environment:</p><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># Use lxd help init for a list of all the options.</span>lxd init --auto</code></pre><p>Verify that the localhost cloud is available</p><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># Juju should have detected the presence of LXD and has added it as the localhost cloud.</span>juju clouds</code></pre><h3 id="Bootstrap-the-controller"><a href="#Bootstrap-the-controller" class="headerlink" title="Bootstrap the controller"></a>Bootstrap the controller</h3><p>Juju uses an active software agent, called the controller, to manage applications.</p><p>Install the controller through a bootstrap process.</p><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># During the bootstrap process, Juju connects with the cloud, then provision a machine to install the controller on, then install it.</span><span class="token comment" spellcheck="true"># juju help bootstrap</span>juju bootstrap localhost overlord</code></pre><h3 id="First-workload-Hello-Juju"><a href="#First-workload-Hello-Juju" class="headerlink" title="First workload: Hello Juju!"></a>First workload: Hello Juju!</h3><p>simple web application uses the Flask microframework to send “Hello Juju!” via HTTP.</p><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># The charm name hello-juju is resolved into an actual charm version by contacting the Charm Store.</span>juju deploy hello-juju</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> Juju </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is MASS?</title>
      <link href="2020/05/03/what-is-mass/"/>
      <url>2020/05/03/what-is-mass/</url>
      
        <content type="html"><![CDATA[<p>From <a href="https://maas.io/docs" target="_blank" rel="noopener">https://maas.io/docs</a></p><hr><h3 id="What-is-MASS"><a href="#What-is-MASS" class="headerlink" title="What is MASS ?"></a>What is MASS ?</h3><p>MAAS is <strong>Metal As A Service</strong>,  converts bare-metal servers into cloud instances of virtual machines, as if they are instances hosted in a public cloud like AWS. ( KVM guests can even act as MAAS machines if they boot from the network via PXE.)</p><p>You can discover, commission, deploy, and dynamically reconfigure a large network of individual units. </p><p>MAAS can act as a standalone PXE/preseed service or integrate with other technologies.It works exceptionally well with <a href="https://jaas.ai/docs/maas-cloud" target="_blank" rel="noopener">Juju</a>, the service and model management tool. MAAS manages the machines and Juju manages the services running on those machines.</p><h3 id="What-MAAS-offers"><a href="#What-MAAS-offers" class="headerlink" title="What MAAS offers?"></a>What MAAS offers?</h3><p>MAAS can manage a large number of physical machines by merging them into user-defined resource pools. </p><p>MAAS integrates all the tools you need into a smooth system-management experience. It includes:</p><ul><li>web UI (optimised for mobile devices)</li><li>Ubuntu, CentOS, Windows, and RHEL installation support</li><li>open-source IP address management (IPAM)</li><li>full API/CLI support</li><li>high availability (optional)</li><li>IPv6 support</li><li>inventory of components</li><li>DHCP and DNS for other devices on the network</li><li>DHCP relay integration</li><li>VLAN and fabric support</li><li>NTP for the entire infrastructure</li><li>hardware testing</li><li>composable hardware support</li></ul><p>These tools can be controlled from a responsive <a href="https://maas.io/docs/web-ui" target="_blank" rel="noopener">web UI</a> or a <a href="https://maas.io/docs/maas-cli" target="_blank" rel="noopener">CLI</a> driven by a REST API. </p><h3 id="Colocation-of-key-components"><a href="#Colocation-of-key-components" class="headerlink" title="Colocation of key components"></a>Colocation of key components</h3><p>MAAS relies on two key components: the <em>region controller</em> and the <em>rack controller</em>.(In essence, rack controllers manage racks, while the region controller manages the data centre. )</p><ul><li>The region controller handles operator requests; </li><li>the rack controller provides high-bandwidth services to multiple racks. </li></ul><blockquote><p>We generally recommended installing both controllers on the same system. The default MAAS install delivers this colocated configuration automatically. This all-in-one solution also provides <a href="https://maas.io/docs/dhcp" target="_blank" rel="noopener">DHCP</a>.</p></blockquote><h3 id="How-MAAS-works"><a href="#How-MAAS-works" class="headerlink" title="How MAAS works?"></a>How MAAS works?</h3><p>When you <a href="https://maas.io/docs/add-nodes#heading--add-a-node-manually" target="_blank" rel="noopener">add a new machine</a> to MAAS, or elect to add a machine that MAAS has <a href="https://maas.io/docs/add-nodes#heading--enlistment" target="_blank" rel="noopener">enlisted</a>, MAAS <a href="https://maas.io/docs/commission-nodes" target="_blank" rel="noopener">commissions</a> it for service and adds it to the pool. At that point, the machine is ready for use. MAAS keeps things simple, marking machines as “New,” “Commissioning,” “Ready,” and so on.</p><p>There are two ways to add a machine to MAAS. Assuming it’s on the network and capable of PXE-booting, you can add it explicitly – or MAAS can simply discover it when you turn it on.</p><p>Enlistment just means that MAAS discovers a machine when you turn it on, and presented to the MAAS administrator, so that they can choose whether or not to commission it.Machines that have only enlisted will show up in the machine list as “New.”</p><p>Commissioning means that MAAS has successfully booted the machine, scanned and recorded its resources, and prepared it for eventual deploymen.Machines that you explicitly add are automatically commissioned. MAAS marks a successfully-commissioned machine as “Ready” in the machine list.</p><p>MAAS controls machines through IPMI (or another BMC). </p><blockquote><p>MAAS overwrites the machine’s disk space with your chosen, pre-cached OS images.</p></blockquote><p>MAAS users allocate (“acquire”) machines for use when needed. The web UI also allows you to acquire machines manually, such as when you are reserving specific hardware for certain users. You can remotely access and customise the installed operating system via SSH.</p><p>Note that <a href="https://jaas.ai/docs/maas-cloud" target="_blank" rel="noopener">Juju</a> is designed to work with MAAS. MAAS becomes a backend Juju resource pool with all functionality fully available. For instance, if Juju removes a machine, then MAAS will release that machine to the pool. With Juju, MAAS can become an integral part of your data centre strategy and operations.</p>]]></content>
      
      
      <categories>
          
          <category> Tech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MASS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is LXD?</title>
      <link href="2020/05/03/what-is-lxd/"/>
      <url>2020/05/03/what-is-lxd/</url>
      
        <content type="html"><![CDATA[<p>From <a href="https://linuxcontainers.org/lxd/" target="_blank" rel="noopener">https://linuxcontainers.org/lxd/</a></p><hr><p>LXD is a next generation system container manager. It offers a user experience similar to virtual machines but using Linux containers instead.</p><p>LXD isn’t a rewrite of LXC, in fact it’s building on top of LXC to provide a new, better user experience. </p><p>LXD is written in Go, it’s free software and is developed under the Apache 2 license.</p><p>The LXD project was founded and is currently led by Canonical Ltd with contributions from a range of other companies and individual contributors.</p><p>LXD is image based.</p><p>Support for Cross-host container and image transfer</p><p>Advanced resource control (cpu, memory, network I/O, block I/O, disk usage and kernel resources)</p><p>Device passthrough (USB, GPU, unix character and block devices, NICs, disks and paths)</p><p>Network management (bridge creation and configuration, cross-host tunnels, …)</p><p>Storage management (support for multiple storage backends, storage pools and storage volumes)</p><h3 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h3><p>The core of LXD is a privileged daemon which exposes a REST API over a local unix socket as well as over the network (if enabled).</p><p>Clients, such as the command line tool provided with LXD itself then do everything through that REST API. It means that whether you’re talking to your local host or a remote server, everything works the same way.</p><h3 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h3><pre class=" language-bash"><code class="language-bash">snap <span class="token function">install</span> lxd<span class="token comment" spellcheck="true"># or apt install lxd lxd-client</span></code></pre><h3 id="Initial-configuration"><a href="#Initial-configuration" class="headerlink" title="Initial configuration"></a>Initial configuration</h3><p>Before you can create containers, you need to tell LXD a little bit about your storage and network needs.</p><pre class=" language-bash"><code class="language-bash">lxd init</code></pre><h3 id="Access-control"><a href="#Access-control" class="headerlink" title="Access control"></a>Access control</h3><p>Access control for LXD is based on group membership.</p><ul><li>The root user as well as members of the “lxd” group can interact with the local daemon.</li><li>Anyone with access to the LXD socket can fully control LXD, which includes the ability to attach host devices and filesystems, this should therefore only be given to users who would be trusted with root access to the host.</li></ul><h3 id="Creating-and-using-your-first-container"><a href="#Creating-and-using-your-first-container" class="headerlink" title="Creating and using your first container"></a>Creating and using your first container</h3><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># create and start a new Ubuntu 18.04 named first</span>lxc launch ubuntu:18.04 first<span class="token comment" spellcheck="true"># list all running containers</span>lxc list<span class="token comment" spellcheck="true"># you can get a shell inside it with:</span>lxc <span class="token function">exec</span> first -- /bin/bash<span class="token comment" spellcheck="true"># Or just run a command directly:</span>lxc <span class="token function">exec</span> first -- <span class="token function">apt-get</span> update<span class="token comment" spellcheck="true"># To pull/push a file from the container, use:</span>lxc <span class="token function">file</span> pull first/etc/hosts <span class="token keyword">.</span>lxc <span class="token function">file</span> push hosts first/tmp/<span class="token comment" spellcheck="true"># To stop the container, simply do:</span>lxc stop first<span class="token comment" spellcheck="true"># And to remove it entirely:</span>lxc delete first</code></pre><h3 id="Container-images"><a href="#Container-images" class="headerlink" title="Container images"></a>Container images</h3><p>LXD is image based. Containers must be created from an image and so the image store must get some images before you can do much with LXD.</p><p>There are three ways to feed that image store:</p><ol><li>Use one of the the built-in image remotes</li><li>Use a remote LXD as an image server</li><li>Manually import an image tarball</li></ol><p><strong>Using the built-in image remotes</strong></p><p>LXD comes with 3 default remotes providing images:</p><ol><li>ubuntu: (for stable Ubuntu images)</li><li>ubuntu-daily: (for daily Ubuntu images)</li><li>images: (for a <a href="https://images.linuxcontainers.org/" target="_blank" rel="noopener">bunch of other distros</a>)</li></ol><pre class=" language-bash"><code class="language-bash">lxc launch ubuntu:16.04 my-ubuntulxc launch ubuntu-daily:18.04 my-ubuntu-devlxc launch images:centos/6/amd64 my-centos</code></pre><p><strong>Using a remote LXD as an image server</strong></p><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># add a remote image server</span>lxc remote add my-images 1.2.3.4<span class="token comment" spellcheck="true"># use a remote image server</span>lxc launch my-images:image-name your-container<span class="token comment" spellcheck="true"># list obtained images:</span>lxc image list my-images:</code></pre><p><strong>Manually importing an image</strong></p><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># If you already have a lxd-compatible image file, you can import it with:</span>lxc image <span class="token function">import</span> <span class="token operator">&lt;</span>file<span class="token operator">></span> --alias my-alias<span class="token comment" spellcheck="true">#  start a container with it:</span>lxc launch my-alias my-container</code></pre><h3 id="Multiple-hosts"><a href="#Multiple-hosts" class="headerlink" title="Multiple hosts"></a>Multiple hosts</h3><p>The “lxc” command line tool can talk to multiple LXD servers and defaults to talking to the local one.</p><p>set a server work as remote server:</p><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#  tell LXD to bind all addresses on port 8443. </span>lxc config <span class="token keyword">set</span> core.https_address <span class="token string">"[::]"</span><span class="token comment" spellcheck="true"># set a trust password to be used by new clients.</span>lxc config <span class="token keyword">set</span> core.trust_password some-password</code></pre><p>Add a remote server and use it</p><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># This will prompt you to confirm the remote server fingerprint and then ask you for the password.</span>lxc remote add host-a <span class="token operator">&lt;</span>ip address or DNS<span class="token operator">></span><span class="token comment" spellcheck="true"># use all the same command as above but prefixing the container and images name with the remote host like:</span>lxc <span class="token function">exec</span> host-a:first -- <span class="token function">apt-get</span> update</code></pre>]]></content>
      
      
      <categories>
          
          <category> Tech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LXD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is Kubeflow?</title>
      <link href="2020/04/27/what-is-kubeflow/"/>
      <url>2020/04/27/what-is-kubeflow/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Kubeflow — AI on Kubernetes — anywhere</p></blockquote><blockquote><p>Kubeflow, a standardised machine learning solution for on-premises and on-cloud training. </p></blockquote><blockquote><p>Kubeflow brings together all the most popular tools for machine learning, starting with JupyterHub and Tensorflow, in a standardised workflow running on Kubernetes. </p></blockquote><blockquote><p>Kubeflow is an open source artificial intelligence / machine learning (AI/ML) tool that helps improve deployment, portability and management of AI/ML models.</p></blockquote><blockquote><p>Kubeflow works well with TensorFlow and other modern AI/ML frameworks such as PyTorch, MXNet and Chainer allowing users to enhance their existing code and setup.</p></blockquote><blockquote><p>Kubeflow lets your data scientists focus on the pieces that matter to the business.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> kubeflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is Charmed Kubernetes?</title>
      <link href="2020/04/27/what-is-charmed-kubernetes/"/>
      <url>2020/04/27/what-is-charmed-kubernetes/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Containers as a Service</p></blockquote><blockquote><p>Charmed Kubernetes provides a well integrated, turn-key Kubernetes® platform that is open, extensible, and secure.</p></blockquote><blockquote><p>Kubernetes uses Container Network Interface (CNI) as an interface between network providers and Kubernetes networking. </p></blockquote><blockquote><p>Canonical delivers a standardised set of open source log aggregation and systems monitoring dashboards with every cloud, using Prometheus, the Elasticsearch and Kibana stack (ELK), and Nagios.</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Welcome Ubuntu 20.04</title>
      <link href="2020/04/27/welcome-ubuntu-20-04/"/>
      <url>2020/04/27/welcome-ubuntu-20-04/</url>
      
        <content type="html"><![CDATA[<p>16 hours ago - Uploaded by OMG! UBUNTU!</p><blockquote><p>Ubuntu 20.04 LTS is released April 23, 2020, succeeding Ubuntu 19.10 as the latest stable release of this …</p></blockquote><blockquote><p>Ubuntu is the platform to power your Artificial Intelligence ambitions — from developer workstations, to racks, to clouds and to the edge with smart connected IoT.</p></blockquote><blockquote><p>With a strong focus on AI/ML and providing a cloud-native platform for the enterprise, Ubuntu is the platform of choice for K8s.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is CaaS ?</title>
      <link href="2020/04/27/what-is-caas/"/>
      <url>2020/04/27/what-is-caas/</url>
      
        <content type="html"><![CDATA[<p>From <a href="https://www.webopedia.com/TERM/C/caas-containers-as-a-service.html" target="_blank" rel="noopener">https://www.webopedia.com/TERM/C/caas-containers-as-a-service.html</a> By Forrest Stroud</p><blockquote><p>Containers-as-a-Service (CaaS) is an emerging services offering for container-based virtualization in which providers offer a complete framework to customers for deploying and managing containers, applications and clusters.</p></blockquote><blockquote><p>The Containers-as-a-Service model is designed to help both developers and IT departments develop, run and manage containerized applications. In a CaaS model, containers and clusters are provided as a service that can be deployed in on-premises data centers or over the cloud.</p></blockquote><p>Why is CaaS important?</p><p>From <a href="https://www.ibm.com/services/cloud/containers-as-a-service" target="_blank" rel="noopener">https://www.ibm.com/services/cloud/containers-as-a-service</a></p><blockquote><p>A model with broad application, CaaS helps developers streamline the process of constructing a fully scaled container and applications deployment. The model is a boon for IT departments, providing an enabled container deployment service that has governance control in a security-rich environment. The CaaS model helps enterprises simplify container management within their software-defined infrastructures.</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> CaaS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TFX Overview</title>
      <link href="2020/04/25/tfx-overview/"/>
      <url>2020/04/25/tfx-overview/</url>
      
        <content type="html"><![CDATA[<h3 id="TensorFlow-Extended-是什么？"><a href="#TensorFlow-Extended-是什么？" class="headerlink" title="TensorFlow Extended 是什么？"></a>TensorFlow Extended 是什么？</h3><p>Google 开源的端到端平台，用于部署生产型机器学习流水线。</p><h3 id="TFX-Core-Concepts"><a href="#TFX-Core-Concepts" class="headerlink" title="TFX Core Concepts"></a>TFX Core Concepts</h3><h4 id="TFX-Pipelines"><a href="#TFX-Pipelines" class="headerlink" title="TFX Pipelines"></a>TFX Pipelines</h4><blockquote><p>A TFX pipeline defines a data flow through several components, with the goal of implementing a specific ML task (e.g., building and deploying a regression model for specific data). Pipeline components are built upon TFX libraries. The result of a pipeline is a TFX deployment target and/or service of an inference request.</p></blockquote><blockquote><p>A TFX pipeline is a sequence of components that implement an ML pipeline which is specifically designed for scalable, high-performance machine learning tasks. </p></blockquote><h4 id="Artifacts"><a href="#Artifacts" class="headerlink" title="Artifacts"></a>Artifacts</h4><blockquote><p>In a pipeline, an artifact is a unit of data that is passed between components. Generally, components have at least one input artifact and one output artifact. All artifacts must have associated metadata, which defines the type and properties of the artifact. Artifacts must be strongly typed with an artifact type registered in the ML Metadata store.</p></blockquote><h4 id="Model-vs-SavedModel"><a href="#Model-vs-SavedModel" class="headerlink" title="Model vs. SavedModel"></a>Model vs. SavedModel</h4><p><strong>Model</strong></p><blockquote><p>A model is the output of the training process. It is the serialized record of the weights that have been learned during the training process. These weights can be subsequently used to compute predictions for new input examples. For TFX and TensorFlow, ‘model’ refers to the checkpoints containing the weights learned up to that point.</p></blockquote><p><strong>SavedModel</strong></p><blockquote><p>What is a SavedModel: a universal, language-neutral, hermetic, recoverable serialization of a TensorFlow model.</p></blockquote><blockquote><p>Why is it important: It enables higher-level systems to produce, transform, and consume TensorFlow models using a single abstraction.</p></blockquote><blockquote><p>SavedModel is the recommended serialization format for serving a TensorFlow model in production, or exporting a trained model for a native mobile or JavaScript application</p></blockquote><h4 id="Schema"><a href="#Schema" class="headerlink" title="Schema"></a>Schema</h4><blockquote><p>Some TFX components use a description of your input data called a schema. The schema is an instance of schema.proto.</p></blockquote><blockquote><p>The schema can specify data types for feature values, whether a feature has to be present in all examples, allowed value ranges, and other properties. </p></blockquote><blockquote><p>One of the benefits of using TensorFlow Data Validation (TFDV) is that it will automatically generate a schema by inferring types, categories, and ranges from the training data.</p></blockquote><blockquote><p>In a typical TFX pipeline TensorFlow Data Validation generates a schema, which is consumed by the other components.</p></blockquote><h3 id="TFX-Pipeline-Components"><a href="#TFX-Pipeline-Components" class="headerlink" title="TFX Pipeline Components"></a>TFX Pipeline Components</h3><p>A TFX pipeline typically includes the following components:</p><ul><li>ExampleGen: initial input component of a pipeline that ingests and optionally splits the input dataset.</li><li>StatisticsGen: calculates statistics for the dataset.</li><li>SchemaGen: examines the statistics and creates a data schema.</li><li>ExampleValidator: looks for anomalies and missing values in the dataset.</li><li>Transform: performs feature engineering on the dataset.</li><li>Trainer: trains the model.</li><li>Evaluator: performs deep analysis of the training results and helps you validate your exported models, ensuring that they are “good enough” to be pushed to production.</li><li>Pusher: deploys the model on a serving infrastructure.</li></ul><p><img src="/images/261a09290f89497cb892a3f5dd2f926e.png" alt="bc3b97ba3f6311cddccb27f985126b5d.png"></p><h3 id="Anatomy-of-a-Component"><a href="#Anatomy-of-a-Component" class="headerlink" title="Anatomy of a Component"></a>Anatomy of a Component</h3><p><img src="/images/8e0ad6e85fa1414bb40c1416b31acb71.png" alt="745a33fdc35be6b6f85ef302250b6c85.png"><br>TFX components consist of three main pieces:</p><ul><li><p>Driver and Publisher</p><blockquote><p>The driver supplies metadata to the executor by querying the metadata store, while the publisher accepts the results of the executor and stores them in metadata. As a developer you will typically not need to interact with the driver and publisher directly, but messages logged by the driver and publisher may be useful during debugging.</p></blockquote></li><li><p>Executor</p><blockquote><p>The executor is where a component performs its processing. As a developer you write code which runs in the executor, based on the requirements of the classes which implement the type of component that you’re working with. For example, when you’re working on a Transform component you will need to develop a preprocessing_fn.</p></blockquote><h3 id="TFX-Libraries"><a href="#TFX-Libraries" class="headerlink" title="TFX Libraries"></a>TFX Libraries</h3><p><img src="/images/5e1c7ae593664ae2b435a4128344d1f0.png" alt="8fb962ef79a20aca4a7870fadc1e403c.png"><br>TFX includes both libraries and pipeline components.This diagram illustrates the relationships between TFX libraries and pipeline components:</p></li></ul><p>TFX libraries include:</p><ul><li>TensorFlow Data Validation (TFDV) is a library for analyzing and validating machine learning data. </li><li>TensorFlow Transform (TFT) is a library for preprocessing data with TensorFlow.</li><li>TensorFlow is used for training models with TFX.</li><li>TensorFlow Model Analysis (TFMA) is a library for evaluating TensorFlow models. </li><li>TensorFlow Metadata (TFMD) provides standard representations for metadata that are useful when training machine learning models with TensorFlow.</li><li>ML Metadata (MLMD) is a library for recording and retrieving metadata associated with ML developer and data scientist workflows.</li></ul><h3 id="Technologies-Behind-TFX"><a href="#Technologies-Behind-TFX" class="headerlink" title="Technologies Behind TFX"></a>Technologies Behind TFX</h3><blockquote><p>TFX is a Google-production-scale machine learning platform based on TensorFlow.</p></blockquote><ul><li>based on TensorFlow</li></ul><blockquote><p>Apache Beam is an open source, unified model for defining both batch and streaming data-parallel processing pipelines. TFX uses Apache Beam to implement data-parallel pipelines. </p></blockquote><ul><li>Apache Beam</li></ul><h3 id="Developing-with-TFX"><a href="#Developing-with-TFX" class="headerlink" title="Developing with TFX"></a>Developing with TFX</h3><blockquote><p>TFX provides a powerful platform for every phase of a machine learning project, from research, experimentation, and development on your local machine, through deployment.</p></blockquote><p><strong>Data Exploration, Visualization, and Cleaning</strong></p><blockquote><p>TFX pipelines typically begin with an ExampleGen component, which accepts input data and formats it as tf.Examples.Often this is done after the data has been split into training and evaluation datasets so that there are actually two copies of ExampleGen components, one each for training and evaluation. </p></blockquote><p>ExampleGen component accepts input data and formats it as tf.Examples; Split data into training and evaluation dataset. </p><blockquote><p>a StatisticsGen component and a SchemaGen component, which will examine your data and infer a data schema and statistics.</p></blockquote><p>examine data and infer a data schema and statistics.</p><blockquote><p>ExampleValidator component consumes schema and statistics will look for anomalies, missing values, and incorrect data types in your data.</p></blockquote><p>look for anomalies, missing values, and incorrect data types.</p><blockquote><p>TensorFlow Data Validation (TFDV) is a valuable tool when doing initial exploration, visualization, and cleaning of your dataset. </p></blockquote><blockquote><p>TFDV examines your data and infers the data types, categories, and ranges, and then automatically helps identify anomalies and missing values. It also provides visualization tools that can help you examine and understand your dataset. </p></blockquote><blockquote><p>Following your initial model training and deployment, TFDV can be used to monitor new data from inference requests to your deployed models, and look for anomalies and/or drift. This is especially useful for time series data that changes over time as a result of trend or seasonality, and can help inform when there are data problems or when models need to be retrained on new data.</p></blockquote><p>TFDV can be used to monitor new data;especially useful for time series data ;</p><p><strong>Data Visualization</strong></p><blockquote><p>You will first query ML Metadata (MLMD) to locate the results of these executions of these components, and then use the visualization support API in TFDV to create the visualizations in your notebook.</p></blockquote><p><strong>Developing and Training Models</strong><br><img src="/images/103a6be21a1540e28da78494f314ef6d.png" alt="fe79b9f7453f1d79756e027c1a24d7be.png"></p><blockquote><p>A typical TFX pipeline will include a Transform component, which will perform feature engineering by leveraging the capabilities of the TensorFlow Transform (TFT) library. </p></blockquote><p>feature engineering;</p><blockquote><p>A Transform component consumes the schema created by a SchemaGen component, and applies data transformations to create, combine, and transform the features that will be used to train your model.</p></blockquote><p>consumes the schema;data transformations</p><blockquote><p>Cleanup of missing values and conversion of types should also be done in the Transform component if there is ever a possibility that these will also be present in data sent for inference requests. </p></blockquote><p>Cleanup of missing values and conversion of types;</p><p><img src="/images/272c53d2ee354ab9b55cf47e5b46d040.png" alt="470e7ca2bed9fb90aa1e4345b6ab2e33.png"></p><blockquote><p>The result of a Transform component is a SavedModel which will be imported and used in your modeling code in TensorFlow, during a Trainer component. </p></blockquote><p>The result of a Transform component is a SavedModel ????</p><blockquote><p>This SavedModel includes all of the data engineering transformations that were created in the Transform component, so that the identical transforms are performed using the exact same code during both training and inference.</p></blockquote><p>SavedModel includes all of the data engineering transformations.</p><blockquote><p>Using the modeling code, including the SavedModel from the Transform component, you can consume your training and evaluation data and train your model.</p></blockquote><blockquote><p>During the last section of your modeling code you should save your model as both a SavedModel and an EvalSavedModel. </p></blockquote><blockquote><p>Saving as an EvalSavedModel will require you to import and apply TensorFlow Model Analysis (TFMA) library in your Trainer component.</p></blockquote><p><strong>Analyzing and Understanding Model Performance</strong></p><blockquote><p>Following initial model development and training it’s important to analyze and really understand you model’s performance. </p></blockquote><blockquote><p>A typical TFX pipeline will include an Evaluator component, which leverages the capabilities of the TensorFlow Model Analysis (TFMA) library, which provides a power toolset for this phase of development.<br>An Evaluator component consumes the EvalSavedModel that you exported above, and allows you to specify a list of SliceSpecs that you can use when visualizing and analyzing your model’s performance.</p></blockquote><p>Evaluator component consumes the EvalSavedModel;specify a list of SliceSpecs that you can use when visualizing and analyzing your model’s performance.</p><blockquote><p>Each SliceSpec defines a slice of your training data that you want to examine, such as particular categories for categorical features, or particular ranges for numerical features.</p></blockquote><p>SliceSpec defines a slice of your training data that you want to examine;</p><p><strong>Model Analysis and Visualization</strong></p><blockquote><p>you can visualize the results in a Jupyter style notebook. For additional runs you can compare these results as you make adjustments, until your results are optimal for your model and application.</p></blockquote><p>visualize the results in a Jupyter style notebook;compare these results as you make adjustments</p><h3 id="Deployment-Targets"><a href="#Deployment-Targets" class="headerlink" title="Deployment Targets"></a>Deployment Targets</h3><blockquote><p>Once you have developed and trained a model that you’re happy with, it’s now time to deploy it to one or more deployment target(s) </p></blockquote><blockquote><p>TFX supports deployment to three classes of deployment targets. </p></blockquote><p>TensorFlow Serving; TensorFlow Lite;TensorFlow JS</p><blockquote><p>Trained models which have been exported as SavedModels can be deployed to any or all of these deployment targets.</p></blockquote><p>SavedModels can be deployed to deployment targets.</p><p><img src="/images/698ebd673f04494b8f491ab28d381425.png" alt="895ca2e6ee5ad7dfbacc090328455059.png"></p><p><strong>Inference: TensorFlow Serving</strong></p><blockquote><p>TensorFlow Serving (TFS) is a flexible, high-performance serving system for machine learning models, designed for production environments. </p></blockquote><p>designed for production environments.</p><blockquote><p>It consumes a SavedModel and will accept inference requests over either REST or gRPC interfaces. </p></blockquote><p>consumes a SavedModel ;accept inference requests</p><blockquote><p>It runs as a set of processes on one or more network servers, using one of several advanced architectures to handle synchronization and distributed computation. </p></blockquote><p>one or more network servers;synchronization and distributed computation.</p><blockquote><p>In a typical pipeline a Pusher component will consume SavedModels which have been trained in a Trainer component and deploy them to your TFS infrastructure. This includes handling multiple versions and model updates.</p></blockquote><p>Pusher component will consume SavedModels,and deploy them to your TFS infrastructure;handling multiple versions of models</p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TFX </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我为什么不喜欢Microsoft</title>
      <link href="2020/04/13/wo-wei-shi-me-bu-xi-huan-microsoft/"/>
      <url>2020/04/13/wo-wei-shi-me-bu-xi-huan-microsoft/</url>
      
        <content type="html"><![CDATA[<p>我第一台电脑是 windows 98，直到大学一直不知道世界上还有别的操作系统。</p><p>大三开始接触 Ubuntu，从此喜欢上 linux based os；更准确的说，越来越不喜欢 windows。</p><p>随着时间的推移，越来越发现了微软生态的一些共性，隐隐觉得其中一些共性就是我不喜欢windows的原因，如：</p><ul><li><p>把简单的概念讲的晦涩难懂。看看微软产品的文档，再看看 Linux based 产品的文档。</p></li><li><p>搞不清楚产品用户群体。总倾向于设计出功能上包罗万象的产品，试图掩盖用户群体的问题。如 sqlserver 非要设计超级难用的图形界面工具，就是因为搞不清楚用户群是开发人员而不是小白。</p></li></ul><p>…</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 思考 </tag>
            
            <tag> 杂记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我为什么离开上一家</title>
      <link href="2018/04/16/wo-wei-shi-me-chi-kai-shang-yi-jia/"/>
      <url>2018/04/16/wo-wei-shi-me-chi-kai-shang-yi-jia/</url>
      
        <content type="html"><![CDATA[<p>我想静下心来做成一些事情，但有些因素让我分心、浮躁：</p><ol><li>领导层没有兑现当初的承诺，无法判断以后该相信什么不该相信什么。</li><li>领导层过分贪于眼前的利益，不够重视长远的规划。也许很快就能上市，但会牺牲掉被迫短跑冲刺的一线员工。</li><li>团队短板明显，全力以赴完成专注的事情，可能会被拖后腿。</li><li>寒了太多踏踏实实做事的一线员工，总让阿谀奉承、混日子的人得势；看不到任何迹象表明领导层看到了这个问题、并愿意纠正。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人工智能项目流程</title>
      <link href="2018/04/15/ren-gong-zhi-neng-xiang-mu-liu-cheng/"/>
      <url>2018/04/15/ren-gong-zhi-neng-xiang-mu-liu-cheng/</url>
      
        <content type="html"><![CDATA[<p>最近在思考，公司里的项目为什么会做的这么糟糕？有哪些提升点？人工智能项目最佳的流程可能是什么样的？</p><p>涉及的**可能包含如下，我先列出后续慢慢整理补充：</p><ol><li><p>面向数据。目标：更多项目的签约率；工作效率；更灵活、更恰当的分工。<br>数据理解、梳理<br>数据处理<br>模型实验</p></li><li><p>横向、纵向框架的沉淀。目标：项目落地效率越来越高；形成规模效应<br>框架开发<br>工程化</p></li><li><p>敏捷开发。目标：客户、开发人员良好的体验；降低事故率。</p></li><li><p>从客户角度：准确率是核心。系统稳定性高、产品用户体验好，是基本要求。</p></li><li><p>从开发人员角度：公司文化、工作环境、团队能力、舒服高效的协调流程、有挑战性的工作内容是重要因素。</p></li></ol><p>团队组成可能是这样的，</p><ol><li>数据团队</li><li>特种工程团队</li><li>算法实验团队<br>提升算法的各项指标。关注准确率、学术&amp;行业前沿。</li><li>工程实践团队<br>按照软件工程的思想将整个推荐系统涉及到的各个环节、各个组件、各个模块构建成模块化、可复用、接口简洁易用的工程体系，方便业务的迭代、优化、问题定位与排查。关注：系统需求、系统稳定性、算法的快速落地能力。</li><li>基础组件团队<br>比如数据质量管理组件、任务调度、执行组件、监控组件、CI&amp;CD组件、错误恢复组件、AB测试组件、在线评估组件、数据转运组件、缓存组件、数据存储组件、Web服务接口组件。关注：团队运作效率。</li></ol>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 思考 </tag>
            
            <tag> 杂记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树工作原理示例</title>
      <link href="2018/04/12/jue-ce-shu-gong-zuo-yuan-li-shi-li/"/>
      <url>2018/04/12/jue-ce-shu-gong-zuo-yuan-li-shi-li/</url>
      
        <content type="html"><![CDATA[<h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>我们从 Zoo 数据集中抽出10条用于展示决策树工作原理。其中 <code>class</code>是预测目标特征，属于分类问题。</p><blockquote><p>Zoo DataSet <a href="http://archive.ics.uci.edu/ml/machine-learning-databases/zoo/" target="_blank" rel="noopener">http://archive.ics.uci.edu/ml/machine-learning-databases/zoo/</a><br>Data Set Information: A simple database containing 17 Boolean-valued attributes. The “type” attribute appears to be the class attribute.</p></blockquote><p>训练数据：</p><p><img src="/images/dc297d45db6e4832a617654be26defce.png" alt="8ef0c19a815e3bbc419d3c4accd8d2c8.png"></p><p>测试数据：</p><p><img src="/images/173e521777ac454a941dd303d8b33c14.png" alt="cd2ec2fb4aea1fe7e25a8ddbd60d1274.png"></p><p><em>特征数据（如 <code>legs</code>）中，1表示有，0表示无；目标特征（<code>class</code>）中1到7代表不同的动物。</em></p><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p><img src="/images/entropy.png" alt=""><br><img src="/images/information-gain.png" alt=""></p><ul><li>寻找根节点的决策问题</li></ul><pre><code># 计算用每一个特征、每一个值切分数据后的信息增益。挑出信息增益最大的作为最佳决策问题。legs == 0 ? 0.1562499999999999legs == 2 ? 0.049107142857142905legs == 4 ? 0.2062500000000001legs == 6 ? 0.2895833333333334tail == 0 ? 0.15625tail == 1 ? 0.15625domestic == 0 ? 0.1205357142857143domestic == 1 ? 0.1205357142857143catsize == 0 ? 0.2895833333333334catsize == 1 ? 0.2895833333333334#最大信息增益是 0.289，对应的决策问题是 catsize == 1 ?。# 这个问题作为根节点的决策问题</code></pre><ul><li>找到 <code>catsize == 1 ?</code> 是最佳决策问题；根据<code>catsize == 1 ?</code>切分根节点数据集。</li></ul><p><img src="/images/1fc667ccc6cd49cdb33209ea36854672.png" alt="b380fc1d10b9bb1607b740184151ec29.png"></p><ul><li>寻找左1节点的决策问题</li></ul><pre><code># 计算用每一个特征、每一个值切分数据（剩下）后的信息增益。挑出最大的作为最佳决策问题。legs == 0 ? 0.31999999999999984legs == 2 ? 0.019999999999999796legs == 4 ? 0.11999999999999983tail == 0 ? 0.019999999999999796tail == 1 ? 0.019999999999999796domestic == 0 ? 0.0catsize == 1 ? 0.0#最大信息增益是 0.319，对应的决策问题是 legs == 0 ?。# 这个问题作为左1节点的决策问题</code></pre><ul><li>找到 <code>legs == 0 ?</code> 是最佳决策问题；根据 <code>legs == 0 ?</code> 切分左1节点数据集。</li></ul><p><img src="/images/6890d83054ad4dbd88c4baabe4b90749.png" alt="532f43a0204a621d263a8550d2f9bfe1.png"></p><ul><li>寻找左2节点的决策问题</li></ul><pre><code># 计算用每一个特征、每一个值切分数据（剩下）后的信息增益。挑出最大的作为最佳决策问题。legs == 0 ? 0.0tail == 1 ? 0.0domestic == 0 ? 0.0catsize == 1 ? 0.0#信息增益都是 0，本节点作为叶子节点。</code></pre><p><img src="/images/585ad37dd6044fa89836202613ba3e45.png" alt="38f0452db09ea415f9b9dc3d31a377d0.png"></p><ul><li>寻找左2右节点的决策问题</li></ul><pre><code># 计算用每一个特征、每一个值切分数据（剩下）后的信息增益。挑出最大的作为最佳决策问题。legs == 2 ? 0.0legs == 4 ? 0.0tail == 0 ? 0.0tail == 1 ? 0.0domestic == 0 ? 0.0catsize == 1 ? 0.0#信息增益都是 0，本节点作为叶子节点。</code></pre><p><img src="/images/1691afbc5b8545ee86cfe6def22b3ceb.png" alt="2c3074175823acc5eb004e9b4b8dbe70.png"></p><ul><li>寻找右1节点的决策问题</li></ul><pre><code># 计算用每一个特征、每一个值切分数据（剩下）后的信息增益。挑出最大的作为最佳决策问题。legs == 6 ? 0.0tail == 0 ? 0.0domestic == 0 ? 0.1111111111111111domestic == 1 ? 0.11111111111111105catsize == 0 ? 0.0#最大信息增益是 0.11，对应的决策问题是 domestic == 0 ? 或 domestic == 1 ? 。# 任选1个问题作为右1节点的决策问题</code></pre><ul><li>根据 <code>domestic == 0 ?</code> 切分左1节点数据集。</li></ul><p><img src="/images/d65eb5a7051e42159792445a6f8c428e.png" alt="073b6a5bc6dc3fb4cd8ce8b506d09ae2.png"></p><ul><li>寻找右2左节点的决策问题</li></ul><pre><code># 计算用每一个特征、每一个值切分数据（剩下）后的信息增益。挑出最大的作为最佳决策问题。legs == 6 ? 0.0tail == 0 ? 0.0domestic == 0 ? 0.0catsize == 0 ? 0.0#信息增益都是 0，本节点作为叶子节点。</code></pre><p><img src="/images/94bd57c33f7b4be78ed709c7fc9f49a6.png" alt="283c197506ee8a48e3dfc2c449b9afac.png"></p><ul><li>寻找右2右节点的决策问题</li></ul><pre><code># 计算用每一个特征、每一个值切分数据（剩下）后的信息增益。挑出最大的作为最佳决策问题。legs == 6 ? 0.0tail == 0 ? 0.0domestic == 1 ? 0.0catsize == 0 ? 0.0#信息增益都是 0，本节点作为叶子节点。</code></pre><p><img src="/images/dce9cb6e43564f4d9a531311d651da91.png" alt="d07c392dcf12e87351de196fc06d0f46.png"></p><p>最终训练好的决策树长这样</p><pre><code>catsize == 1 ?--&gt; True:  legs == 0 ?  --&gt; True:    Predict {4: 1}  --&gt; False:    Predict {1: 4}--&gt; False:  domestic == 0 ?  --&gt; True:    Predict {7: 1, 6: 1}  --&gt; False:    Predict {6: 1}</code></pre><p><img src="/images/f0b135fc10b241b1aa2d6d47fc49d2ac.png" alt="85646ae39dc34e6d122a7dc54eeb7593.png"></p><h3 id="预测过程"><a href="#预测过程" class="headerlink" title="预测过程"></a>预测过程</h3><h5 id="测试数据示例1"><a href="#测试数据示例1" class="headerlink" title="测试数据示例1"></a>测试数据示例1</h5><p><img src="/images/1fd1a69d2dd943b0a575ac0c658c45f9.png" alt="8e2e2419edfb5becf1de3692c70ba53c.png"></p><p>分流的决策问题序列如下</p><pre><code>catsize == 1 ?--&gt; True:  legs == 0 ?  --&gt; False:    Predict {1: 4}</code></pre><p>预测结果为1，实际分类是2，因此分类错误。</p><h5 id="测试数据示例2"><a href="#测试数据示例2" class="headerlink" title="测试数据示例2"></a>测试数据示例2</h5><p><img src="/images/c2bd6bc81c50411fa9fd1138f71773c8.png" alt="4bc04a6394313e0cb5b742a0d04ff761.png"></p><p>分流的决策问题序列如下</p><pre><code>catsize == 1 ?--&gt; True:  legs == 0 ?  --&gt; False:    Predict {1: 4}</code></pre><p>预测结果为1，实际分类也是1，因此分类正确。</p><p>…<br>…</p><p>测试了 20条测试数据示例，$分类正确数/20=0.76$</p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
            <tag> 树模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树介绍</title>
      <link href="2018/04/12/jue-ce-shu-jie-shao/"/>
      <url>2018/04/12/jue-ce-shu-jie-shao/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是决策树"><a href="#什么是决策树" class="headerlink" title="什么是决策树"></a>什么是决策树</h3><p>决策树是一种用树结构进行学习、预测的监督学习方法。英文 Decision Tree。</p><p>树结构包含节点和边。节点可分为根节点、内部节点、叶子节点。如下图</p><p><img src="/images/83559e0ada20ea6fada523135ccb9e18.png" alt="83559e0ada20ea6fada523135ccb9e18.png"></p><p><strong>学习（训练）过程：</strong></p><ol><li>数据从根节点流入</li><li>寻找<strong>最佳决策问题</strong>，作为切分数据的依据，数据分流入各个边。</li><li>如果某一节点上找不到最佳决策问题（或满足<strong>停止生长要求</strong>），本节点被标为叶子节点并返回。</li></ol><ul><li>数据不纯度下降最多，作为<strong>最佳决策问题</strong>的衡量标准。常用基尼系数(gini impurity)和信息熵(entropy)衡量数据不纯度。</li><li><strong>停止生长要求</strong>是为了防止过拟合。</li></ul><blockquote><p>决策树算法遵循“分而治之”的思路。</p></blockquote><p>寻找最佳决策问题序列并生成树的过程就是学习过程。</p><p><strong>预测过程：</strong></p><ol><li><p><strong>测试数据</strong>流入根节点，依据决策问题分流入边，直至叶子节点。</p></li><li><p>返回叶子节点的<strong>训练数据</strong>所属类别作为分类结果。（回归树中，返回叶子节点的<strong>训练数据</strong>标签均值作为预测结果。</p></li></ol><h3 id="决策树特点"><a href="#决策树特点" class="headerlink" title="决策树特点"></a>决策树特点</h3><p>决策数是一种<strong>监督学习</strong>方法。既可用于分类问题，也可用于回归问题。</p><blockquote><p>决策树的数据（特征和目标数据）可以是离散型，也可以是连续型。</p></blockquote><p><img src="/images/3780d8c9748ea2e6c33547fa6a02d38f.png" alt="3780d8c9748ea2e6c33547fa6a02d38f.png"></p><blockquote><p>根据目标数据是离散的还是连续，分为<strong>分类树</strong>和<strong>回归树</strong>。</p></blockquote><p>决策树属于基于信息的学习算法。</p><blockquote><p>Decision trees are assigned to the information based learning algorithms which use different measures of information gain for learning.</p></blockquote><h3 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h3><p>ID3算法原型见于J.R Quinlan的博士论文，是基础理论较为完善，使用较为广泛的决策树模型。<br>在ID3基础上 J.RQuinlan 进行优化后，陆续推出了C4.5和C5.0决策树算法，后二者现已称为当前最流行的决策树算法。</p><p>CART树本质其实和C4.5区别不大，只不过CART树所有的层都是二叉树，<br>也就是每层只有两个分枝。</p><p>决策树算法一般使用递归实现，伪代码如下</p><blockquote><p>参考 <a href=":/9caaf2f5afea4cd0b698a774b1286d9d">决策树工作原理示例</a></p></blockquote><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Node</span><span class="token punctuation">(</span>dta<span class="token punctuation">,</span> l_branch<span class="token operator">=</span>None<span class="token punctuation">,</span> r_branch<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>data <span class="token operator">=</span> dta    self<span class="token punctuation">.</span>left_branch <span class="token operator">=</span> l_branch     self<span class="token punctuation">.</span>right_branch <span class="token operator">=</span> r_branchbst_tree<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>    bst_question<span class="token punctuation">,</span> left_data<span class="token punctuation">,</span> right_data <span class="token operator">=</span> find_bst_question<span class="token punctuation">(</span>data<span class="token punctuation">)</span>    <span class="token keyword">if</span> bst_question 达不到条件<span class="token punctuation">:</span>        <span class="token keyword">return</span> Node<span class="token punctuation">(</span>data<span class="token punctuation">)</span>    left_branch <span class="token operator">=</span> bst_tree<span class="token punctuation">(</span>left_data<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># &lt;== 递归</span>    right_branch <span class="token operator">=</span> bst_tree<span class="token punctuation">(</span>right_data<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># &lt;== 递归</span>    tree <span class="token operator">=</span> Node<span class="token punctuation">(</span>data<span class="token punctuation">,</span> left_branch<span class="token punctuation">,</span> right_branch<span class="token punctuation">)</span>    <span class="token keyword">return</span> tree</code></pre><p>基于决策树的集成学习（ensemble learning）方法效果很好，如<br>装袋法（Bagging）突出代表 Random Forests；提升法（Boosting）突出代表 Adaboost,Gradient Boost,XGBoost。</p><h3 id="决策树的优势"><a href="#决策树的优势" class="headerlink" title="决策树的优势"></a>决策树的优势</h3><ol><li><p>白盒，可解释性强</p><blockquote><p>从根节点到叶子节点的路径就是决策依据</p><p>树形结构容易可视化。</p><p>参考 <a href="https://blog.csdn.net/llh_1178/article/details/78516774" target="_blank" rel="noopener">可视化决策树之Python实现</a></p></blockquote></li><li><p>特征不需要标准化（Normalization）</p><blockquote><p>特征标准化是为了使不同特征值处于相同的量级范围，不至于出现部分特征主导输出结果的情况。</p><p>简单的决策树来说，计算基尼指数、信息增益等量基于概率值，因此特征值的量级对决策依据没有影响。</p></blockquote></li><li><p>既能用于分类问题，也能用于回归问题。</p><blockquote><p>根据目标数据是离散还是连续的，分为<strong>分类树</strong>和<strong>回归树</strong>。</p></blockquote></li><li><p>可以建模（捕捉）特征间的非线性(non-linear)关系</p><blockquote><p>线性(linear)，指量与量之间按比例、成直线的关系，在数学上可以理解为一阶导数为常数的函数；如 $k_1X_1+k_2X_2+k_3X_3 + … + k_nX_n=0$，其中$k_n$ 是常数</p><p>非线性(non-linear)则指不按比例、不成直线的关系，一阶导数不为常数。</p></blockquote></li><li><p>可以建模（捕捉）不同可解释特征间的相互作用。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
            <tag> 树模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Algorithms 4 Note 1.4</title>
      <link href="2013/05/23/algorithms-4-note-1-4/"/>
      <url>2013/05/23/algorithms-4-note-1-4/</url>
      
        <content type="html"><![CDATA[<h1 id="1-4-算法分析"><a href="#1-4-算法分析" class="headerlink" title="1.4　算法分析"></a>1.4　算法分析</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>我的程序会运行多长时间？为什么我的程序耗尽了所有内存？</p><h2 id="1-4-1-科学方法"><a href="#1-4-1-科学方法" class="headerlink" title="1.4.1　科学方法"></a>1.4.1　科学方法</h2><p>科学家用来理解自然世界的方法对于研究计算机程序的运行时间同样有效：</p><ul><li>细致地观察真实世界的特点，通常还要有精确的测量；</li><li>根据观察结果提出假设模型；</li><li>根据模型预测未来的事件；</li><li>继续观察并核实预测的准确性；</li><li>如此反复直到确认预测和观察一致。</li></ul><p>科学方法的一条关键原则是我们所设计的实验必须是<strong>可重现</strong>的，这样他人也可以自己验证假设的真实性。<br>所有的假设也必须是<strong>可证伪</strong>的，这样我们才能确认某个假设是错误的（并需要修正）。</p><blockquote><p>再多的实验也不一定能够证明我是对的，但只需要一个实验就能证明我是错的。—- 爱因斯坦</p></blockquote><h2 id="1-4-2-观察"><a href="#1-4-2-观察" class="headerlink" title="1.4.2　观察"></a>1.4.2　观察</h2><p>如何定量测量程序的运行时间？</p><ul><li>第一个定量观察就是，计算性任务的困难程度可以用输入的规模来衡量。根据直觉，程序的运行时间应该随着输入规模的增长而变长，但我们每次在开发和运行一个程序时想问的问题都是<strong>运行时间的增长有多快</strong>。</li><li>另一个定量观察是运行时间和输入本身相对无关，它主要取决于问题规模。因此我们现在来重点研究如何更好地将<strong>问题规模和运行时间的关系</strong>量化。</li></ul><h3 id="1-4-2-1-举例"><a href="#1-4-2-1-举例" class="headerlink" title="1.4.2.1　举例"></a>1.4.2.1　举例</h3><p>ThreeSum 程序。</p><h3 id="1-4-2-2-计时器"><a href="#1-4-2-2-计时器" class="headerlink" title="1.4.2.2　计时器"></a>1.4.2.2　计时器</h3><p>准确测量给定程序的确切运行时间是很困难的。不过幸运的是我们一般只需要近似值就可以了。</p><ul><li>我们希望能够把需要几秒钟或者几分钟就能完成的程序和需要几天、几个月甚至更长时间才能完成的程序区别开来，</li><li>而且我们希望知道对于同一个任务某个程序是不是比另一个程序快一倍。</li></ul><p>因此，我们仍然需要准确的测量手段来生成实验数据，并根据它们得出并验证关于程序的运行时间和问题规模的假设。</p><h3 id="1-4-2-3-实验数据的分析"><a href="#1-4-2-3-实验数据的分析" class="headerlink" title="1.4.2.3　实验数据的分析"></a>1.4.2.3　实验数据的分析</h3><p>程序在不同的计算机上的运行时间之比通常是一个常数。<br>尽管如此，你还是会提出更详细的问题：作为问题规模的一个函数，我的程序的运行时间是多久？</p><h2 id="1-4-3-数学模型"><a href="#1-4-3-数学模型" class="headerlink" title="1.4.3　数学模型"></a>1.4.3　数学模型</h2><p>D. E. Knuth 认为，尽管有许多复杂的因素影响着我们对程序的运行时间的理解，原则上我们仍然可能构造出一个数学模型来描述任意程序的运行时间。<br>Knuth 的基本见地很简单——一个程序运行的总时间主要和两点有关：</p><ul><li>执行每条语句的耗时；(取决于计算机、Java 编译器和操作系统)</li><li>执行每条语句的频率。(取决于程序本身和输入)</li></ul><p>如果对于程序的所有部分我们都知道了这些性质，可以将它们相乘并将程序中所有指令的成本相加得到总运行时间。<br>第一个挑战是判定语句的执行频率。</p><h3 id="1-4-3-1-近似"><a href="#1-4-3-1-近似" class="headerlink" title="1.4.3.1　近似"></a>1.4.3.1　近似</h3><p>这种频率分析可能会产生复杂冗长的数学表达式。用近似的方式忽略公式中那些非常复杂但幂次较低，且对最终结果的贡献无关紧要的项。<br>定义。我们用 $<del>f(N)$ 表示所有随着 N 的增大除以 $f(N)$ 的结果趋近于 1 的函数。我们用 $g(N)\</del>f(N)$ 表示 $g(N)/f(N)$ 随着 N 的增大趋近于 1。</p><h3 id="1-4-3-2-近似运行时间"><a href="#1-4-3-2-近似运行时间" class="headerlink" title="1.4.3.2　近似运行时间"></a>1.4.3.2　近似运行时间</h3><p>我们观察到的一个关键现象是执行最频繁的指令决定了程序执行的总时间——我们将这些指令称为程序的内循环。这种情况是很典型的：许多程序的运行时间都只取决于其中的一小部分指令。</p><p><img src="/images/image-20200523174610103.png" alt="image-20200523174610103"></p><h3 id="1-4-3-3-对增长数量级的猜想"><a href="#1-4-3-3-对增长数量级的猜想" class="headerlink" title="1.4.3.3　对增长数量级的猜想"></a>1.4.3.3　对增长数量级的猜想</h3><p>1.4.2.3 节中的实验和表 1.4.4 中的数学模型都支持以下猜想：<br>性质 A。ThreeSum（在 N 个数中找出三个和为 0 的整数元组的数量）的运行时间的增长数量级为 $N^3$。（在本书中，我们使用性质表示需要用实验验证的猜想。）</p><p><img src="/images/image-20200523174716575.png" alt="image-20200523174716575"></p><h3 id="1-4-3-4-算法的分析"><a href="#1-4-3-4-算法的分析" class="headerlink" title="1.4.3.4　算法的分析"></a>1.4.3.4　算法的分析</h3><p>类似于性质 A 的猜想的意义很重要，因为它们将抽象世界中的一个 Java 程序和真实世界中运行它的一台计算机联系了起来。<br>增长数量级概念的应用使我们能够继续向前迈进一步：<strong>将程序和它实现的算法隔离开来。</strong>（ThreeSum 的运行时间的增长数量级是 $N^3$，这与它是由 Java 实现或是它运行在你的笔记本电脑上或是某人的手机上或是一台超级计算机上无关。决定这一点的主要因素是它需要检查输入中任意三个整数的所有可能组合。）<br>你所使用的算法（有时还要算上输入模型）决定了增长的数量级。(将算法和某台计算机上的具体实现分离开来是一个强大的概念，因为这使我们对算法性能的知识可以应用于任何计算机。实际上，经典算法的性能理论大部分都发表于数十年前，但它们仍然适用于今天的计算机。)</p><h3 id="1-4-3-5-成本模型"><a href="#1-4-3-5-成本模型" class="headerlink" title="1.4.3.5　成本模型"></a>1.4.3.5　成本模型</h3><p>我们使用了一个成本模型来评估算法的性质。这个模型定义了我们所研究的算法中的基本操作。例如，适合于 3-sum 问题的成本模型是我们访问数组元素的次数。3-sum 的暴力算法使用了 $~N^3/2$ 次数组访问来计算 N 个整数中和为 0 的整数三元组的数量。<br>在全书中我们都会使用某个确定的成本模型研究所讨论的算法。</p><h3 id="1-4-3-6-总结"><a href="#1-4-3-6-总结" class="headerlink" title="1.4.3.6　总结"></a>1.4.3.6　总结</h3><p>对于大多数程序，得到其运行时间的数学模型所需的步骤如下：</p><ul><li>确定输入模型，定义问题的规模；</li><li>识别内循环；</li><li>根据内循环中的操作确定成本模型；</li><li>对于给定的输入，判断这些操作的执行频率。这可能需要进行数学分析——我们在本书中会在学习具体的算法时给出一些例子。</li></ul><p>如果一个程序含有多个方法，我们一般会分别讨论它们，<br>例如，二分查找( BinarySearch)的输入模型是大小为 N 的数组 a[]，内循环是一个 while 循环中的所有语句，成本模型是比较操作（比较两个数组元素的值）。（3.1 节中的命题 B 详细完整地给出了 1.1 节中讨论的内容，该命题说明它所需的比较次数最多为 $lgN + 1$。）<br>白名单的输入模型是白名单的大小 N 和由标准输入得到的 M 个整数，且我们假设 $M &gt;&gt; N$，内循环是一个 while 循环中的所有语句，成本模型是比较操作（承自二分查找）。由二分查找的分析我们可以立即得到对白名单问题的分析——比较次数最多为 $M(lgN + 1)$。</p><p>在算法分析中进行数学建模是一个多产的研究领域，但它多少超出了本书的范畴。通过二分查找、归并排序和其他许多算法你仍会看到，理解特定的数学模型对于理解基础算法的运行效率是很关键的，因此我们常常会详细地证明它们或是引用经典研究中的结论。在其中，我们会遇到各种数学分析中广泛使用的函数和近似函数。</p><h2 id="1-4-4-增长数量级的分类"><a href="#1-4-4-增长数量级的分类" class="headerlink" title="1.4.4 增长数量级的分类"></a>1.4.4 增长数量级的分类</h2><p>我们在实现算法时使用了几种结构性的原语（普通语句、条件语句、循环、嵌套语句和方法调用），所以成本增长的数量级一般都是问题规模 N 的若干函数之一。</p><h3 id="1-4-4-1-常数级别"><a href="#1-4-4-1-常数级别" class="headerlink" title="1.4.4.1　常数级别"></a>1.4.4.1　常数级别</h3><p>运行时间的增长数量级为常数的程序完成它的任务所需的操作次数一定，因此它的运行时间不依赖于 N。</p><h3 id="1-4-4-2-对数级别"><a href="#1-4-4-2-对数级别" class="headerlink" title="1.4.4.2 对数级别"></a>1.4.4.2 对数级别</h3><p>运行时间的增长数量级为对数的程序仅比常数时间的程序稍慢。运行时间和问题规模成对数关系的程序的经典例子就是<strong>二分查找</strong>。对数的底数和增长的数量级无关，所以我们在说明对数级别时一般使用 $logN$。</p><h3 id="1-4-4-3-线性级别"><a href="#1-4-4-3-线性级别" class="headerlink" title="1.4.4.3 线性级别"></a>1.4.4.3 线性级别</h3><p>使用常数时间处理输入数据中的所有元素或是基于单个 for 循环的程序是十分常见的。此类程序的增长数量级是线性的——它的运行时间和 N 成正比。</p><h3 id="1-4-4-4-线性对数级别"><a href="#1-4-4-4-线性对数级别" class="headerlink" title="1.4.4.4 线性对数级别"></a>1.4.4.4 线性对数级别</h3><p>我们用线性对数描述运行时间和问题规模 N 的关系为 $NlogN$ 的程序(和之前一样，对数的底数和增长的数量级无关)。<br>线性对数算法的典型例子是 <strong>Merge.sort和Quick.sort</strong></p><h3 id="1-4-4-5-平方级别"><a href="#1-4-4-5-平方级别" class="headerlink" title="1.4.4.5　平方级别"></a>1.4.4.5　平方级别</h3><p>一个运行时间的增长数量级为 N^2 的程序一般都含有两个嵌套的 for 循环，对由 N 个元素得到的所有元素对进行计算。初级排序算法 <strong>Selection.sort() 和 Insertion.sort()</strong> 都是这种类型的典型程序。</p><h3 id="1-4-4-6-立方级别"><a href="#1-4-4-6-立方级别" class="headerlink" title="1.4.4.6 立方级别"></a>1.4.4.6 立方级别</h3><p>一个运行时间的增长数量级为 $N^3$ 的程序一般都含有三个嵌套的 for 循环，对由 N 个元素得到的所有三元组进行计算。<strong>ThreeSum</strong> 就是一个典型的例子。</p><h3 id="1-4-4-7-指数级别"><a href="#1-4-4-7-指数级别" class="headerlink" title="1.4.4.7 指数级别"></a>1.4.4.7 指数级别</h3><p>在第 6 章中（也只会在第 6 章）我们将会遇到运行时间和 $2^N$ 或者更高级别的函数成正比的程序。一般我们会使用指数级别来描述增长数量级为 $b^N$ 的算法(其中 $b &gt; 1$ 且为常数，尽管不同的 b 值得到的运行时间可能完全不同)。<br>指数级别的算法非常慢——不可能用它们解决大规模的问题。但指数级别的算法仍然在算法理论中有着重要的地位，因为它们看起来仍然是解决许多问题的最佳方案。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>以上是最常见分类，但肯定不是最全面的。算法的增长数量级可能是 $N^2logN$ 或者 $N^(3/2)$ 或者是其他类似的函数。实际上，详细的算法分析可能会用到若干个世纪以来发明的各种数学工具。<br>我们所学习的一大部分算法的性能特点都很简单，可以使用我们所讨论过的某种增长数量级函数精确地描述。如，归并排序所需的运行时间的增长数量级是线性对数的。简单起见，我们将这句话简写为归并排序是线性对数的。</p><h2 id="1-4-5-设计更快的算法"><a href="#1-4-5-设计更快的算法" class="headerlink" title="1.4.5 设计更快的算法"></a>1.4.5 设计更快的算法</h2><p>学习程序的增长数量级的一个重要动力是为了帮助我们为同一个问题设计更快的算法。<br>怎么知道如何设计一个更快的算法呢？这个问题的答案是，我们已经讨论并使用过两个经典的算法，即归并排序和二分查找。也知道归并排序是线性对数级别的，二分查找是对数级别的。如何利用它们解决 3-sum 问题呢？</p><h2 id="1-4-6-倍率实验"><a href="#1-4-6-倍率实验" class="headerlink" title="1.4.6 倍率实验"></a>1.4.6 倍率实验</h2><p>下面这种方法可以简单有效地<strong>预测任意程序的性能并判断它们的运行时间大致的增长数量级</strong>。</p><p>在有性能压力的情况下应该考虑对编写过的所有程序进行<strong>倍率实验</strong>——这是一种估计运行时间的增长数量级的简单方法，或许它能够发现一些性能问题，比如你的程序并没有想象的那样高效。</p><p>一般来说，我们可以用以下方式对程序的运行时间的增长数量级作出假设并预测它的性能。</p><h3 id="1-4-6-1-评估它解决大型问题的可行性"><a href="#1-4-6-1-评估它解决大型问题的可行性" class="headerlink" title="1.4.6.1 评估它解决大型问题的可行性"></a>1.4.6.1 评估它解决大型问题的可行性</h3><p>你需要回答这个基本问题：“该程序能在可接受的时间内处理这些数据吗？”<br>了解程序的运行时间的增长数量级能够为你提供精确的信息，从而理解你能够解决的问题规模的上限。<br>理解诸如此类的问题，是研究性能的首要原因。没有这些知识，你将对一个程序所需的时间一无所知；而如果你有了它们，一张信封的背面就足够你计算出运行所需的时间并采取相应的行动。</p><h3 id="1-4-6-2-评估使用更快的计算机所产生的价值"><a href="#1-4-6-2-评估使用更快的计算机所产生的价值" class="headerlink" title="1.4.6.2 评估使用更快的计算机所产生的价值"></a>1.4.6.2 评估使用更快的计算机所产生的价值</h3><p>另一个基本问题是：“如果我能够得到一台更快的计算机，解决问题的速度能够加快多少？”<br>如果新计算机比老的快 x 倍，运行时间也将变为原来的 x 分之一。但你一般都会用新计算机来处理更大规模的问题，这将会如何影响所需的运行时间呢？同样，增长的数量级信息也正是你回答这个问题所需要的。</p><p>著名的摩尔定律告诉我们，18 个月后计算机的速度和内存容量都会翻一番，5 年后计算机的速度和内存容量都会变为现在的 10 倍。如果你使用的是平方或者立方级别的算法，摩尔定律就不适用了。进行倍率测试并检查随着输入规模的倍增前后运行时间之比是趋向于 2 而非 4 或者 8 即可验证这种情况。</p><h2 id="1-4-7-注意事项"><a href="#1-4-7-注意事项" class="headerlink" title="1.4.7　注意事项"></a>1.4.7　注意事项</h2><h3 id="1-4-7-1-大常数"><a href="#1-4-7-1-大常数" class="headerlink" title="1.4.7.1　大常数"></a>1.4.7.1　大常数</h3><h3 id="1-4-7-2-非决定性的内循环"><a href="#1-4-7-2-非决定性的内循环" class="headerlink" title="1.4.7.2　非决定性的内循环"></a>1.4.7.2　非决定性的内循环</h3><h3 id="1-4-7-3-指令时间"><a href="#1-4-7-3-指令时间" class="headerlink" title="1.4.7.3　指令时间"></a>1.4.7.3　指令时间</h3><h3 id="1-4-7-4-系统因素"><a href="#1-4-7-4-系统因素" class="headerlink" title="1.4.7.4　系统因素"></a>1.4.7.4　系统因素</h3><h3 id="1-4-7-5-不分伯仲"><a href="#1-4-7-5-不分伯仲" class="headerlink" title="1.4.7.5　不分伯仲"></a>1.4.7.5　不分伯仲</h3><h3 id="1-4-7-6-对输入的强烈依赖"><a href="#1-4-7-6-对输入的强烈依赖" class="headerlink" title="1.4.7.6　对输入的强烈依赖"></a>1.4.7.6　对输入的强烈依赖</h3><h3 id="1-4-7-7-多个问题参量"><a href="#1-4-7-7-多个问题参量" class="headerlink" title="1.4.7.7　多个问题参量"></a>1.4.7.7　多个问题参量</h3><h2 id="1-4-8-处理对于输入的依赖"><a href="#1-4-8-处理对于输入的依赖" class="headerlink" title="1.4.8　处理对于输入的依赖"></a>1.4.8　处理对于输入的依赖</h2><p>对于许多问题，刚才所提到的注意事项中最突出的一个就是<strong>对于输入的依赖</strong>，因为在这种情况下程序的运行时间的变化范围可能非常大。</p><p>ThreeSum 的修改版本的运行时间的范围根据输入的不同可能在常数级别到立方级别之间，因此如果我们想要预测它的性能，就需要对它进行更加细致的分析。</p><h3 id="1-4-8-1-输入模型"><a href="#1-4-8-1-输入模型" class="headerlink" title="1.4.8.1　输入模型"></a>1.4.8.1　输入模型</h3><p>一种方法是更加小心地对我们所要解决的问题所处理的输入建模。</p><h3 id="1-4-8-2-对最坏情况下的性能的保证"><a href="#1-4-8-2-对最坏情况下的性能的保证" class="headerlink" title="1.4.8.2　对最坏情况下的性能的保证"></a>1.4.8.2　对最坏情况下的性能的保证</h3><p>有些应用程序要求程序对于任意输入的运行时间均小于某个指定的上限。<br>为了提供这种性能保证，理论研究者们要从极度悲观的角度来估计算法的性能：<strong>在最坏情况下程序的运行时间是多少？</strong>（这种保守的做法对于运行在核反应堆、心脏起搏器或者刹车控制器之中的软件可能是十分必要的。）</p><h3 id="1-4-8-3-随机化算法"><a href="#1-4-8-3-随机化算法" class="headerlink" title="1.4.8.3　随机化算法"></a>1.4.8.3　随机化算法</h3><p>为性能提供保证的一种重要方法是引入随机性。<br>例如，我们将在2.3 节中学习的<strong>快速排序算法</strong>（可能是使用最广泛的排序算法）在最坏情况下的性能是平方级别的，但通过随机打乱输入，<strong>根据概率我们能够保证它的性能是线性对数</strong>的。每次运行该算法，它所需的时间均不相同，但它的运行时间超过线性对数级别的可能性小到可以忽略。<br>例如， 3.4 节中学习的用于符号表的<strong>散列算法</strong>（同样也可能是使用最广泛的同类算法），在最坏情况下的性能是线性级别的，但<strong>根据概率我们可以保证它的运行时间是常数级别的</strong>。<br>这些保证并不是绝对的，但它们失效的可能性甚至小于你的电脑被闪电击中的可能性。因此，这种保证在实际中也可以用来作为最坏情况下的性能保证。</p><h3 id="1-4-8-4-操作序列"><a href="#1-4-8-4-操作序列" class="headerlink" title="1.4.8.4　操作序列"></a>1.4.8.4　操作序列</h3><p>对于许多应用来说，算法的“输入”可能并不只是数据，还包括用例所进行的一系列操作的顺序。<br>例如，对于一个下压栈来说，用例先压入 N 个值然后再将它们全部弹出的所得到的性能，和 N 次压入弹出的混合操作序列所得到的性能可能大不相同。</p><h3 id="1-4-8-5-均摊分析"><a href="#1-4-8-5-均摊分析" class="headerlink" title="1.4.8.5　均摊分析"></a>1.4.8.5　均摊分析</h3><p>相应地，提供性能保证的另一种方法是通过记录<strong>所有操作的总成本并除以操作总数来将成本均摊</strong>。在这里，我们可以允许执行一些昂贵的操作，但保持所有操作的平均成本较低。这种类型分析的典型例子是我们在 1.3 节中对基于动态调整数组大小的Stack 数据结构（请见 1.3.2.5 节的算法 1.1）的研究。</p><h2 id="1-4-9-内存"><a href="#1-4-9-内存" class="headerlink" title="1.4.9　内存"></a>1.4.9　内存</h2><p>和运行时间一样，一个程序对内存的使用也和物理世界直接相关：计算机中的电路很大一部分的作用就是帮助程序保存一些值并在稍后取出它们。<br>在任意时刻需要保存的值越多，需要的电路也就越多。你可能知道计算机能够使用的内存上限（知道这一点的人应该比知道运行时间限制的人要多）因为你很可能已经在内存上花了不少额外的支出。<br><strong>内存的使用是和实现相关的。</strong><br><strong>Java 最重要的特性之一就是它的内存分配系统。</strong>它的任务是把你从对内存的操作之中解脱出来。（显然，你肯定已经知道应该在适当的时候利用这个功能，但是你也应该（至少是大概）知道程序对内存的需求在何时会成为解决问题的障碍。）<br><strong>分析内存的使用比分析程序所需的运行时间要简单得多，主要原因是它所涉及的程序语句较少（只有声明语句）且在分析中我们会将复杂的对象简化为原始数据类型，而原始数据类型的内存使用是预先定义好的</strong>，而且非常容易理解：只需将变量的数量和它们的类型所对应的字节数分别相乘并汇总即可。（例如，因为 Java 的int数据类型是$-2 147 483 648$ 到$2 147 483 647$ 之间的整数值的集合，即总数为 $2^32$ 个不同的值，典型的 Java 实现使用 32 位来表示 int 值。）<br>根据可用内存的总量就能够计算出保存这些值的极限数量。</p><h3 id="1-4-9-1-对象"><a href="#1-4-9-1-对象" class="headerlink" title="1.4.9.1　对象"></a>1.4.9.1　对象</h3><p>要知道一个对象所使用的内存量，需要将所有实例变量使用的内存与对象本身的开销（一般是 16 字节,这些开销包括一个指向对象的类的引用、垃圾收集信息以及同步信息。）相加。</p><p>另外，一般内存的使用都会被填充为8 字节（64 位计算机中的机器字）的倍数。<br>当我们说明一个引用所占的内存时，我们会单独说明它所指向的对象所占用的内存，因此这个内存使用总量并没有包含 String 值所使用的内存。</p><h3 id="1-4-9-2-链表"><a href="#1-4-9-2-链表" class="headerlink" title="1.4.9.2　链表"></a>1.4.9.2　链表</h3><p>嵌套的非静态（内部）类，例如我们的 Node 类（请见 1.3.3.1 节），还需要额外的 8 字节（用于一个指向外部类的引用）。因此，一个 Node 对象需要使用 40 字节（16 字节的对象开销，指向 Item 和 Node 对象的引用各需 8 字节，另外还有 8 字节的额外开销）。<br>因为 Integer 对象需要使用 24 字节，一个含有 N 个整数的基于链表的栈（请见算法 1.2）需要使用（32+64N）字节，包括 Stack 对象的 16 字节的开销，引用类型实例变量8 字节，int 型实例变量4 字节，4 个填充字节，每个元素需要 64 字节，一个Node 对象的 40 字节和一个Integer 对象的 24 字节。</p><h3 id="1-4-9-3-数组"><a href="#1-4-9-3-数组" class="headerlink" title="1.4.9.3　数组"></a>1.4.9.3　数组</h3><p>图 1.4.9 总结了 Java 中的各种类型的数组对内存的典型需求。<br>Java 中数组被实现为对象，它们一般都会因为记录长度而需要额外的内存。(一个原始数据类型的数组一般需要 24 字节的头信息（16 字节的对象开销，4 字节用于保存长度以及 4 填充字节）再加上保存值所需的内存。)<br>一个对象的数组就是一个对象的引用的数组，所以我们应该在对象所需的内存之外加上引用所需的内存。<br>二维数组是一个数组的数组（每个数组都是一个对象）。</p><h3 id="1-4-9-4-字符串对象"><a href="#1-4-9-4-字符串对象" class="headerlink" title="1.4.9.4　字符串对象"></a>1.4.9.4　字符串对象</h3><p>我们可以用相同的方式说明 Java 的 String 类型对象所需的内存，只是对于字符串来说别名是非常常见的。<br>String 的标准实现含有 4 个实例变量：一个指向字符数组的引用（8 字节）和三个 int 值（各 4 字节）。</p><h3 id="1-4-9-5-字符串的值和子字符串"><a href="#1-4-9-5-字符串的值和子字符串" class="headerlink" title="1.4.9.5　字符串的值和子字符串"></a>1.4.9.5　字符串的值和子字符串</h3><h2 id="1-4-10-展望"><a href="#1-4-10-展望" class="headerlink" title="1.4.10　展望"></a>1.4.10　展望</h2><p>良好的性能是非常重要的。速度极慢的程序和不正确的程序一样无用，因此显然有必要在一开始就关注程序的运行成本，这能够让你大致估计出所要解决的问题的规模，而聪明的做法是时刻关注程序中的内循环代码的组成。</p><p>但在<strong>编程领域中，最常见的错误或许就是过于关注程序的性能。你的首要任务应该是写出清晰正确的代码。</strong>C.A.R. Hoare（快速排序的发明人，也是一位推动编写清晰而正确的代码的领军人物）曾将这种想法总结为：</p><blockquote><p>不成熟的优化是所有罪恶之源</p></blockquote><p>在编程领域中，第二常见的错误或许是完全忽略了程序的性能。较快的算法一般都比暴力算法更复杂，所以很多人宁可使用较慢的算法也不愿应付复杂的代码。但是，几行优秀的代码有时能够给你带来巨大的收益。</p>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法与数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Algorithms 4 1.3 背包、队列和栈</title>
      <link href="2013/05/04/algorithms-4-note-1-3/"/>
      <url>2013/05/04/algorithms-4-note-1-3/</url>
      
        <content type="html"><![CDATA[<h2 id="1-3-背包、队列和栈"><a href="#1-3-背包、队列和栈" class="headerlink" title="1.3　背包、队列和栈"></a>1.3　背包、队列和栈</h2><p><strong>许多基础数据类型</strong>都和对象的<strong>集合</strong>有关。具体来说，数据类型的值就是一组对象的集合，所有操作都是关于添加、删除或是访问集合中的对象。</p><p>在本节中，我们将学习三种这样的数据类型，分别是<strong>背包</strong>（Bag）、<strong>队列</strong>（Queue）和<strong>栈</strong>（Stack）。它们的不同之处在于<strong>删除或者访问对象的顺序不同</strong>。</p><h2 id="1-3-1-API"><a href="#1-3-1-API" class="headerlink" title="1.3.1　API"></a>1.3.1　API</h2><p>我们对集合型的抽象数据类型的讨论从定义它们的API开始.集合类的抽象数据类型的一个关键特性是<strong>我们应该可以用它们存储任意类型的数据</strong>，泛型能够做到这一点。</p><h3 id="1-3-1-1-泛型"><a href="#1-3-1-1-泛型" class="headerlink" title="1.3.1.1　泛型"></a>1.3.1.1　泛型</h3><p>泛型，也叫做参数化类型。有了泛型，我们只需要一份API（和一次实现）就能够处理所有类型的数据，甚至是在未来定义的数据类型。</p><h3 id="1-3-1-2-自动装箱"><a href="#1-3-1-2-自动装箱" class="headerlink" title="1.3.1.2　自动装箱"></a>1.3.1.2　自动装箱</h3><p>自动将一个原始数据类型转换为一个封装类型；自动将一个封装类型转换为一个原始数据类型被称为自动拆箱。</p><h3 id="1-3-1-3-可迭代的集合类型"><a href="#1-3-1-3-可迭代的集合类型" class="headerlink" title="1.3.1.3　可迭代的集合类型"></a>1.3.1.3　可迭代的集合类型</h3><p>对于许多应用场景，用例的要求只是用某种方式处理集合中的每个元素，或者叫做<strong>迭代访问</strong>集合中的所有元素。</p><p>这种模式非常重要，在 Java 和其他许多语言中它都是一级语言特性。</p><h3 id="1-3-1-4-背包（Bag）"><a href="#1-3-1-4-背包（Bag）" class="headerlink" title="1.3.1.4　背包（Bag）"></a>1.3.1.4　背包（Bag）</h3><p>背包是一种不支持从中删除元素的集合数据类型。它的目的就是帮助用例收集元素并迭代遍历所有收集到的元素。</p><h3 id="1-3-1-5-先进先出队列（Queue）"><a href="#1-3-1-5-先进先出队列（Queue）" class="headerlink" title="1.3.1.5　先进先出队列（Queue）"></a>1.3.1.5　先进先出队列（Queue）</h3><p>先进先出队列（或简称队列）是一种基于先进先出（FIFO）策略的集合类型。使用队列的主要原因是在用集合保存元素的同时保存它们的相对顺序：使它们入列顺序和出列顺序相同。</p><h3 id="1-3-1-6-下压栈（Stack）"><a href="#1-3-1-6-下压栈（Stack）" class="headerlink" title="1.3.1.6　下压栈（Stack）"></a>1.3.1.6　下压栈（Stack）</h3><p>下压栈（或简称栈）是一种基于后进先出（LIFO）策略的集合类型。</p><h3 id="1-3-1-7-算术表达式求值"><a href="#1-3-1-7-算术表达式求值" class="headerlink" title="1.3.1.7　算术表达式求值"></a>1.3.1.7　算术表达式求值</h3><p>E.W.Dijkstra 在 20 世纪 60 年代发明了一个非常简单的算法，用两个栈（一个用于保存运算符，一个用于保存操作数）完成了这个任务。（<strong>Dijkstra 的双栈算术表达式求值算法</strong>）</p><h2 id="1-3-2-集合类数据类型的实现"><a href="#1-3-2-集合类数据类型的实现" class="headerlink" title="1.3.2　集合类数据类型的实现"></a>1.3.2　集合类数据类型的实现</h2><h3 id="1-3-2-1-定容栈"><a href="#1-3-2-1-定容栈" class="headerlink" title="1.3.2.1　定容栈"></a>1.3.2.1　定容栈</h3><p>作为热身，我们先来看一种表示容量固定的字符串栈的抽象数据类型。</p><h3 id="1-3-2-2-泛型"><a href="#1-3-2-2-泛型" class="headerlink" title="1.3.2.2　泛型"></a>1.3.2.2　泛型</h3><p>如何才能实现一个泛型的栈呢？把所有的 String 都替换为 Item（类型参数），用于表示用例将会使用的某种具体类型的象征性的占位符。</p><h3 id="1-3-2-3-调整数组大小"><a href="#1-3-2-3-调整数组大小" class="headerlink" title="1.3.2.3　调整数组大小"></a>1.3.2.3　调整数组大小</h3><p>选择用数组表示栈内容意味着用例必须预先估计栈的最大容量。方法核心是将栈移动到另一个大小不同的数组中。</p><h3 id="1-3-2-4-对象游离"><a href="#1-3-2-4-对象游离" class="headerlink" title="1.3.2.4　对象游离"></a>1.3.2.4　对象游离</h3><p>我们对 pop() 的实现中，被弹出的元素的引用仍然存在于数组中。这个元素实际上已经是一个孤儿了——它永远也不会再被访问了，但 Java 的垃圾收集器没法知道这一点，除非该引用被覆盖。即使用例已经不再需要这个元素了，数组中的引用仍然可以让它继续存在。这种情况（保存一个不需要的对象的引用）称为游离。</p><h3 id="1-3-2-5-迭代"><a href="#1-3-2-5-迭代" class="headerlink" title="1.3.2.5　迭代"></a>1.3.2.5　迭代</h3><p>任意可迭代的集合数据类型中我们都需要实现：1. 集合数据类型必须实现一个 iterator() 方法并返回一个 Iterator 对象；Iterator 类必须包含两个方法：hasNext()（返回一个布尔值）和next()（返回集合中的一个泛型元素）。迭代器都是泛型的，因此我们可以使用参数类型 Item 来帮助用例遍历它们指定的任意类型的对象。</p><h2 id="1-3-3-链表"><a href="#1-3-3-链表" class="headerlink" title="1.3.3　链表"></a>1.3.3　链表</h2><p>定义。链表是一种递归的数据结构，它或者为空（null），或者是指向一个结点（node）的引用，该结点含有一个泛型的元素和一个指向另一条链表的引用。在结构化存储数据集时，链表是数组的一种重要的替代方式。</p><h3 id="1-3-3-1-结点记录"><a href="#1-3-3-1-结点记录" class="headerlink" title="1.3.3.1　结点记录"></a>1.3.3.1　结点记录</h3><p>一个 Node 对象含有两个实例变量，类型分别为 Item（参数类型）和 Node。Item 是一个占位符，表示我们希望用链表处理的任意数据类型（我们将会使用 Java 的泛型使之表示任意引用类型）；Node 类型的实例变量显示了这种数据结构的链式本质。</p><h3 id="1-3-3-2-构造链表"><a href="#1-3-3-2-构造链表" class="headerlink" title="1.3.3.2　构造链表"></a>1.3.3.2　构造链表</h3><p>链表表示的是一列元素。可以用数组表示同一列元素，不同之处在于，在链表中向序列插入元素或是从序列中删除元素都更方便。</p><h3 id="1-3-3-3-在表头插入结点"><a href="#1-3-3-3-在表头插入结点" class="headerlink" title="1.3.3.3　在表头插入结点"></a>1.3.3.3　在表头插入结点</h3><p>例如，要在首结点为first 的给定链表开头插入字符串not，我们先将first 保存在oldfirst 中，然后将一个新结点赋予 first，并将它的 item 域设为 not，next 域设为 oldfirst。</p><h3 id="1-3-3-4-从表头删除结点"><a href="#1-3-3-4-从表头删除结点" class="headerlink" title="1.3.3.4　从表头删除结点"></a>1.3.3.4　从表头删除结点</h3><p>只需将 first 指向 first.next 即可。</p><h3 id="1-3-3-5-在表尾插入结点"><a href="#1-3-3-5-在表尾插入结点" class="headerlink" title="1.3.3.5　在表尾插入结点"></a>1.3.3.5　在表尾插入结点</h3><h3 id="1-3-3-6-其他位置的插入和删除操作"><a href="#1-3-3-6-其他位置的插入和删除操作" class="headerlink" title="1.3.3.6　其他位置的插入和删除操作"></a>1.3.3.6　其他位置的插入和删除操作</h3><p>已经展示了在链表中如何通过若干指令实现以下操作：在表头插入结点；从表头删除结点；在表尾插入结点。其他操作，例如以下几种，就不那么容易实现了：删除指定的结点；在指定结点前插入一个新结点。实现任意插入和删除操作的标准解决方案是使用双向链表，其中每个结点都含有两个链接，分别指向不同的方向。</p><h3 id="1-3-3-7-链表的遍历"><a href="#1-3-3-7-链表的遍历" class="headerlink" title="1.3.3.7　链表的遍历"></a>1.3.3.7　链表的遍历</h3><p>访问链表中的所有元素也有一个对应的方式：将循环的索引变量 x 初始化为链表的首结点，然后通过 x.item访问和 x 相关联的元素，并将 x 设为 x.next 来访问链表中的下一个结点，如此反复直到 x 为 null 为止。</p><h3 id="1-3-3-8-栈的实现"><a href="#1-3-3-8-栈的实现" class="headerlink" title="1.3.3.8　栈的实现"></a>1.3.3.8　栈的实现</h3><p>它将栈保存为一条链表，栈的顶部即为表头，实例变量 first 指向栈顶。这样，当使用 push() 压入一个元素时，我们会按照 1.3.3.3 节所讨论的代码将该元素添加在表头；当使用pop() 删除一个元素时，我们会按照 1.3.3.4 节讨论的代码将该元素从表头删除。要实现 size() 方法，我们用实例变量 N 保存元素的个数，在压入元素时将 N 加 1，在弹出元素时将 N 减 1。要实现 isEmpty() 方法，只需检查 first 是否为 null。链表的使用达到了我们的最优设计目标：它可以处理任意类型的数据；所需的空间总是和集合的大小成正比；操作所需的时间总是和集合的大小无关。</p><h3 id="1-3-3-9-队列的实现"><a href="#1-3-3-9-队列的实现" class="headerlink" title="1.3.3.9　队列的实现"></a>1.3.3.9　队列的实现</h3><p>基于链表数据结构实现 Queue API 也很简单，如算法 1.3 所示。它将队列表示为一条从最早插入的元素到最近插入的元素的链表，实例变量 first 指向队列的开头，实例变量last 指向队列的结尾。这样，要将一个元素入列（enqueue()），我们就将它添加到表尾（请见图 1.3.8 中讨论的代码，但是在链表为空时需要将 first和 last 都指向新结点）；要将一个元素出列（dequeue()），我们就删除表头的结点（代码和Stack 的pop() 方法相同，只是当链表为空时需要更新last 的值）。size() 和 isEmpty() 方法的实现和 Stack 相同。和 Stack 一样，Queue 的实现也使用了泛型参数 Item。Queue 的实现使用的数据结构和 Stack 相同——链表，但它实现了不同的添加和删除元素的算法，这也是用例所看到的后进先出和先进后出的区别所在。</p><h3 id="1-3-3-10-背包的实现"><a href="#1-3-3-10-背包的实现" class="headerlink" title="1.3.3.10　背包的实现"></a>1.3.3.10　背包的实现</h3><p>用链表数据结构实现我们的 Bag API 只需要将 Stack中的 push() 改名为 add()，并去掉 pop() 的实现即可。Bag 的实现维护了一条链表，用于保存所有通过 add() 添加的元素。size() 和 isEmpty() 方法的代码和 Stack 中的完全相同。</p><h2 id="1-3-4-综述"><a href="#1-3-4-综述" class="headerlink" title="1.3.4　综述"></a>1.3.4　综述</h2><p>本节中学习的支持泛型和迭代的背包、队列和栈的实现所提供的抽象使我们能够编写简洁的用例程序来操作对象的集合。</p><p>深入理解这些抽象数据类型非常重要，原因有三：</p><ul><li>第一，我们将以这些数据类型为基石构造本书中的其他更高级的数据结构；</li><li>第二，它们展示了数据结构和算法的关系以及同时满足多个有可能相互冲突的性能目标时所要面对的挑战；</li><li>第三，我们将要学习的若干算法的实现重点就是需要其中的抽象数据类型能够支持对对象集合的强大操作，这些实现正是我们的起点。</li></ul><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><p>我们现在拥有两种表示对象集合的方式，即数组和链表。Java 内置了数组，链表也很容易使用 Java 的标准方法实现。两者都非常基础，常常被称为顺序存储和链式存储。</p><p>在本书后面部分，我们会在各种抽象数据类型的实现中将多种方式结归并扩展这些基本的数据结构。其中一种重要的扩展就是各种含有多个链接的数据结构，如二叉树（由含有两个链接的结点组成）；另一个重要的扩展是复合型的数据结构（我们可以使用背包存储栈，用队列存储数组，等等），如，图（用数组的背包表示它）。用这种方式很容易定义任意复杂的数据结构，而我们重点研究抽象数据类型的一个重要原因就是试图控制这种复杂度。（我们在本节中研究背包、队列和栈时描述数据结构和算法的方式是全书的原型。）</p><h3 id="识别目标并使用数据抽象解决问题"><a href="#识别目标并使用数据抽象解决问题" class="headerlink" title="识别目标并使用数据抽象解决问题"></a>识别目标并使用数据抽象解决问题</h3><p>在研究一个新的应用领域时，我们将会按照以下步骤识别目标并使用数据抽象解决问题：</p><ul><li><ol><li>定义 API；</li></ol></li><li><ol start="2"><li>根据特定的应用场景开发用例代码；</li></ol></li><li><ol start="3"><li>描述一种数据结构（一组值的表示），并在 API 所对应的抽象数据类型的实现中根据它定义类的实例变量；</li></ol></li><li><ol start="4"><li>描述算法（实现一组操作的方式），并根据它实现类中的实例方法；</li></ol></li><li><ol start="5"><li>分析算法的性能特点。</li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法与数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
