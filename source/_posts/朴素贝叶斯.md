---
title: 朴素贝叶斯
category:
  - MachineLearning
tags:
  - 贝叶斯
  - 概率
mathjax: true
date: 2021-02-24 19:15:02
img: /images/naive.jpg
---

朴素贝叶斯方法是基于贝叶斯定于和特征条件独立假设的分类方法。
<!--more-->

对于给定的训练数据集
1. 首先基于条件独立假设学习输入输出的联合概率分布；
2. 然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出 y。

朴素贝叶斯法优点
* 实现简单
* 学习与预测效率都高

### 基本方法
朴素贝叶斯法国训练数据学习联合概率分布$P(X = x, Y = y)$。

$$P(X=x, Y=c_k)=P(X=x) * P(Y=c_k|X=x)=P(Y=c_k) * P(=X=x|Y=c_k)$$

具体的计算先验概率分布$P(Y=c_k)$ 和 条件概率分布$P(X=x|Y=c_k)$。

计算条件概率分布 $P(X=x|Y=c_k)$ 是有困难（有指数级数量的参数），如：
* 假设 X 是[m, n]维数据，即训练集有n个特征，每一个是离散型变量，特征$X^{(n)}$ 有$S_n$ 种取值；
* Y是[m, 1]维数据，Y 是离散型变量，取值为c_k，有 K 种取值；
* 那么条件概率分布 $P(X^{(1)}=x^{(1)}, X^{(2)}=x^{(2)},... X^{(n)}=x^{(n)}|Y=c_k)$ 中参数个数为 $K * \prod_{i=0}^n{S_n}$

于是，朴素贝叶斯对条件概率分布做了条件独立性假设，使用离散型独立变量的联合分布计算公式
$$P(A, B)=P(A) * P(B)$$
$$P(A, B|C)=P(A|C) * P(B|C)$$
具体的条件独立性假设如下：

$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)}, X^{(2)}=x^{(2)},... X^{(n)}=x^{(n)}|Y=c_k)=\prod_{j=1}^jP(X^{(j)}=x^{(j)}|Y=c_k)$

特征之间相互独立，意味着每个特征独立的对结果产生影响。条件独立性假设在实际应用中是不现实的，因此得名“朴素“贝叶斯。

对于给定的 x，使用学习到的模型计算后验概率分布 $P(Y=c_k|X=x)$，将后验概率最大的类 C作为x的分类结果输出。后验概率使用贝叶斯公式计算：
$$P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k) * P(Y=c_k)}{P(X=x)}$$

其中，转化一下$P(X=x)$
$$P(X=x) = \sum_{k=1}^{k}P(X=x, Y=c_k)=\sum_{k=1}^{k}P(X=x|Y=c_k) * P(Y=c_k)$$
于是后验概率计算公式变为：
$$P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k) * P(Y=c_k)}{\sum_{k=1}^{k}P(X=x|Y=c_k) * P(Y=c_k)}$$

其中，已计算得先验概率$P(Y=c_k)$、 条件概率分布 $P(X=x|Y=c_k)=\prod_{j=1}^jP(X^{(j)}=x^{(j)}|Y=c_k)$

于是朴素贝叶斯分类器可表示为：
$$y=f(x)=\underset{c_k}{\operatorname{argmax}}\frac{P(Y=c_k) * P(X=x|Y=c_k)}{\sum_{k=1}^{k}P(Y=c_k) * P(X=x|Y=c_k)}$$
$$=\underset{c_k}{\operatorname{argmax}}\frac{P(Y=c_k) * \prod_{j=1}^jP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_{k=1}^{k}P(Y=c_k) * \prod_{j=1}^jP(X^{(j)}=x^{(j)}|Y=c_k)}$$

因为，对于所有$Y=c_k$分母都是相同的，所以可简写为：

$$y=f(x)=\underset{c_k}{\operatorname{argmax}}P(Y=c_k) * \prod_{j=1}^jP(X^{(j)}=x^{(j)}|Y=c_k)$$

### 参考文献
* 《统计学习方法》-李航
